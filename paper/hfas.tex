%% 
%% Copyright 2007-2019 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 168 2019-02-25 07:15:41Z apu.v $
%%
%%
%\documentclass[preprint, review, 12pt]{elsarticle}
\documentclass[preprint, 3p, times, twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{acronym}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}
\usepackage[inline]{enumitem}
\usepackage{bbm}
\usepackage[skip=2pt]{caption}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{rotating}

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}
\acrodef{RMSE}{root mean squared error}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}
\journal{International Journal of Forecasting}

\begin{document}

\begin{frontmatter}

\title{Hierarchical Forecasting at Scale}

\author{}
% \author{%
%   Olivier~Sprangers \\
%   AIRLab, NL \\
%   University of Amsterdam, NL\\
%   \texttt{o.r.sprangers@uva.nl} \\
%   \And
%   Sebastian~Schelter \\
%   University of Amsterdam, NL \\
%   \texttt{s.schelter@uva.nl} \\
%   \And
%   Maarten~de~Rijke \\
%   University of Amsterdam, NL \\
%   \texttt{m.derijke@uva.nl} \\
% }



\begin{abstract}
  Hierarchical forecasting techniques allow creation of forecasts that are coherent with respect to a pre-specified hierarchy of the underlying time series. This targets a key problem in e-commerce, where we often find millions of products across many product hierarchies, and forecasts need to be made for both individual products and product aggregations. However, existing hierarchical forecasting techniques scale poorly when the number of time series increases, which limits their potential for application at a scale of millions of products. 
  
  In this paper, we investigate the use of learning a coherent forecast for millions of products with a single bottom-level forecast model by using a loss function that directly optimizes the hierarchical product structure. We implement our loss function using sparse linear algebra, such that the number of operations in our loss function scales linearly rather than quadratically with the number of products and levels in the hierarchical structure. The benefit of our sparse hierarchical loss function is that it provides practitioners a method of producing bottom-level forecasts that are coherent to any chosen (weighted, temporal) hierarchy. In addition, removing the need for a post-processing step as used in traditional hierarchical forecasting techniques reduces the computational cost of the prediction phase in the forecasting pipeline. 
  
  In our tests on the public M5 dataset, our sparse hierarchical loss function performs on par with the baseline loss function. Next, we implemented our sparse hierarchical loss function within an existing gradient boosting-based forecasting model at bol.com, a large European e-commerce platform. Unfortunately, in this setting our sparse hierarchical loss resulted in a slightly worse forecasting performance as measured by RMSE of about 1\% at the product level, as compared to the the baseline model. 
  
  % We achieve this by introduce a sparse hierarchical loss function that allows us to create hierarchical forecasts in a computationally efficient way such that orders of magnitude more time series can be considered compared to existing hierarchical forecasting techniques.   
  
  % In this paper, we introduce a sparse hierarchical loss function that allows us to create hierarchical forecasts in a computationally efficient way such that orders of magnitude more time series can be considered compared to existing hierarchical forecasting techniques. We implement our sparse hierarchical loss function within an existing forecasting system at bol.com, a large European e-commerce platform. 
  
  % Hierarchical forecasting techniques allow creation of forecasts that are coherent with respect to a pre-specified hierarchy of the underlying time series. This targets a key problem in e-commerce, where we often find millions of products across many product hierarchies and forecasts need to be made for both individual products and product aggregations. Applying existing hierarchical forecasting techniques across millions of products is computationally demanding, as these techniques scale poorly with respect to the number of products or product groups.
  
  % For one of the largest ecommerce companies in the Netherlands, we investigated the use of learning a coherent forecast for millions of products with a single bottom-level forecast model by using a loss function that directly optimizes the hierarchical product structure. We implemented this using sparse linear algebra, such that the number of operations in our loss function scales linearly rather than quadratically with the number of products and levels in the hierarchical structure. We chose a gradient boosting model for our forecasts, as this is also the baseline forecasting model in the company and these models have shown strong performance in recent forecasting competitions with similar data characteristics (e.g. M5 competition). The benefit of our sparse hierarchical loss function is that it provides practitioners a method of producing bottom-level forecasts that are coherent to any chosen (weighted) hierarchy. In addition, removing the need for a post-processing step as used in traditional hierachical forecasting techniques reduces the computational cost of the prediction phase in the forecasting pipeline. 

  % In our offline tests on the M5 dataset, our sparse hierarchical loss function performed on par with the baseline loss function. Unfortunately, in our production setting, our sparse hierarchical loss resulted in a slightly worse forecasting performance as measured by RMSE of about 1\% at the product level, as compared to the the baseline model that used a tweedie loss function. 

  % are typically applied as a post-processing step that requires a matrix inversion that scales quadratically with the number of products or product groups.

\end{abstract}

\begin{keyword}
  Hierarchical forecasting, Large-scale forecasting, Efficiency in forecasting methods
\end{keyword}

\end{frontmatter}

% Intro alternative snippets
% time series, where individual time series may be aggregated into increasingly larger aggregations that are meaningful from the perspective of the forecasting application, such as individual product demand that may be aggregated into department demand, to store demand, to regional demand. 

\section{Introduction}
In e-commerce, we may need forecasts for millions of products concurrently \cite{bose_probabilistic_2017}, and we are often faced with two forecasting challenges. First, forecasts at the lowest granularity - often the individual product level - are required but we also need forecasts for higher granularities, for example at the category, department or regional level. Second, forecasts at different time granularities are required, for example daily or weekly forecasts. It is common that separate forecast models are made for each separate (temporal) granularity, and as such these forecasts may not be coherent with each other. Hierarchical forecasting \cite{hyndman_optimal_2011} and temporal hierarchical forecasting techniques \cite{athanasopoulos_forecasting_2017,rangapuram_coherent_2023,theodosiou_forecasting_2021} aim to solve the problem of creating forecasts that are coherent with respect to a pre-specified (temporal) hierarchy of the underlying time series. 

\paragraph{Challenges with existing (temporal) hierarchical forecasting techniques} Reconciliation methods \cite{hyndman_optimal_2011,athanasopoulos_forecasting_2017,wickramasuriya_optimal_2019} apply a post-processing step that requires a matrix inversion that scales quadratically with the number of products or product hierarchies. In settings with millions of products such as in e-commerce, this becomes computationally very expensive at prediction time. Neural network methods \cite{rangapuram_endtoend_2021,rangapuram_coherent_2023} directly optimize for the (temporal) hierarchy in an end-to-end manner. However, these multivariate methods require a forward pass over all training samples at every training epoch, which becomes computionally very expensive at training time in settings with millions of products. 
  
\paragraph{Sparse loss function} In order to overcome these scaling issues, we implement a sparse hierarchical loss function that directly optimizes the (temporal) hierarchical structure. Our sparse implementation ensures that the number of operations in our loss function scales linearly rather than quadratically with the number of products and levels in the hierarchical structure, enabling computationally efficient training. The benefit of our sparse hierarchical loss function is that it provides practitioners a method of producing bottom-level forecasts that are coherent to any chosen (weighted, temporal) hierarchy. In addition, removing the need for a post-processing step as used in traditional hierarchical forecasting techniques reduces the computational cost of the prediction phase in the forecasting pipeline.

\paragraph{Evaluation} We evaluate our sparse hierarchical loss function on a gradient-boosted forecasting system on public (M5) and private (bol.com) datasets. For the M5 dataset, we demonstrate that our implementation provides similar or better forecasting performance compared with reconciliation methods and neural network methods. For the private dataset, we present the results of an offline test on the product-level forecast system of bol.com, a European e-commerce company selling millions of products. Unfortunately, results from our offline test showed slightly worse performance as compared to the existing forecasting system at bol.com. 

\paragraph{Contributions} In summary, the main contributions of this paper are:
\begin{enumerate}
  \item We present a sparse hierarchical loss function that enables direct end-to-end training of (temporal) hierarchical forecasts in large-scale settings.
  \item We empirically demonstrate that a sparse hierarchical loss function outperforms existing (temporal) hierarchical forecasting methods by up to [x\%].
  \item We provide a pip-installable Python package that allows practitioners to easily use our sparse hierarchical aggregated loss framework in large-scale settings.
\end{enumerate}

\section{Related work} \label{sec:relwork}
% In this setting, there is a fixed hierarchical organization between time series, and one is interested in finding an optimal reconciliation between forecasts of different levels within the hierarchy. Such reconciliation steps are often performed as post-processing steps, i.e. a forecast is made for every time series and its hierarchical aggregations, and after training these forecasts are reconciled to satisfy the required hierarchical structure

\paragraph{Forecasting for large-scale settings} Contemporary large-scale forecasting applications require forecasting many time series concurrently \cite{bose_probabilistic_2017}. In academia, there has been a surge in the use of neural network-based forecasting methods, which are methods that commonly learn a single forecast model that can produce forecasts for many time series. We refer the interested reader to the recent survey of \citet{benidis_deep_2023} for an overview of these methods. However, tree-based methods topped the M5 forecasting competition \cite{makridakis_m5_2022}, which is believed to be due to the strong implementations available of these algorithms \cite{januschowski_forecasting_2022}, such as the packages LightGBM \cite{ke_lightgbm_2017} or XGBoost \cite{chen_xgboost_2016}. Our own experience within bol.com confirms this view: the ease of use, execution speed and strong default performance are key reasons a tree-based method is often the default choice when creating a new forecast model.

\paragraph{Hierarchical forecasting} Hierarchical forecasting \cite{hyndman_optimal_2011, hyndman_fast_2016, taieb_coherent_2017, bentaieb_regularized_2019, wickramasuriya_optimal_2019} and temporal hierarchical forecasting techniques \cite{taieb_sparse_2017,athanasopoulos_forecasting_2017,rangapuram_coherent_2023,theodosiou_forecasting_2021} aim to solve the problem of creating forecasts that are coherent with respect to a pre-specified (temporal) hierarchy of the underlying time series. We divide hierarchical forecasting methods into reconciliation methods and other methods.

\subparagraph{Reconciliation methods} These methods solve the hierarchical forecasting problem as a post-processing step by reconciling the forecasts to a pre-specified (temporal) hierarchy \cite{hyndman_optimal_2011, hyndman_fast_2016, taieb_coherent_2017, bentaieb_regularized_2019, wickramasuriya_optimal_2019, panagiotelis_forecast_2021}. Limitations of these approaches are (i) that they require a post-processing step, (ii) computing the reconciliation may be computationally expensive, as we show in Section~\ref{sec:ourwork}, and (iii) approaches that are computationally less expensive tend to perform worse, as we show in Section~\ref{sec:experiments}. Recent work by \citet{taieb_sparse_2017, bentaieb_regularized_2019} has improved forecasting performance of previous reconciliation approaches but at the expense of even higher computational cost, as we explain in Section~\ref{sec:background}. \citet{zhang_optimal_2023} study the problem of reconciling hierarchical forecasts where some of the base forecasts are immutable, i.e. these are not allowed to be modified by the reconciliation method. However, this work also does not address the scalability issues we identify in Section~\ref{sec:ourwork}.

\subparagraph{Other methods} In \cite{rangapuram_endtoend_2021,rangapuram_coherent_2023} neural network-based end-to-end hierarchical probabilistic forecasting method are proposed to solve the hierarchical forecasting problem. More recently and most closely related to our work, \citet{han_simultaneously_2021} introduced SHARQ, a method that reconciles probabilistic hierarchical forecasts during training by employing a regularized loss function that enforces hierarchical consistency of bottom-up forecasts through regularization.


\section{Background} \label{sec:background}
Suppose we have a time series \(\vec{y}_t\), where \(t\) denotes the time stamp. We are interested in estimating future values \(\hat{y}_{t}\) of the time series by employing a model \(f\) based on past values \(y_{t-1}, \dots, y_{t-T}\) of the time series and additional attributes \(X\):
\begin{equation}
  \hat{y}_{t} = f(y_{t-1}, \dots, y_{t-T}, X_{t}, X_{t-1}, \dots, X_{t-T}).
\end{equation}
In our hierarchical forecasting setting, we aim to create forecasts for many time series concurrently, whilst adherring to pre-specified hierarchical relationships that exist between the time series. This can be formalized as \cite{hyndman_forecasting_2021}:
\begin{equation} \label{eq:hfp}
  \tilde{\textbf{y}}_{t} = SP\hat{\textbf{y}}_{t} \;,
\end{equation}
where \(\hat{\textbf{y}}_{t} \in \mathbb{R}^{m} \) denotes the vector of forecasts for all \(m\) time series in the hierarchy, \(S \in \{0, 1\}^{m \times n}\) is a matrix that defines the hierarchical relationship between the \(n\) bottom-level time series and the \(m^* = m - n\) aggregations, and \(P \in \mathbb{R}^{n \times m}\) is a matrix that encapsulates the contribution of each forecast to the final estimate. We can use the matrix \(P\) to define various forecast contribution scenarios. Note that we can straightforwardly extend Equation~\eqref{eq:hfp} to the setting of \textit{temporal hierarchies} \cite{athanasopoulos_forecasting_2017,rangapuram_coherent_2023} by considering forecasts of different time granularities in our vector of base forecasts \(\hat{\textbf{y}}_{t}\) and using an appropriate choice of \(S\) to aggregate series of a different time granularity.

The optimal solution to the problem in Equation~\eqref{eq:hfp} can be found using \textit{Reconciliation methods} and \textit{Other methods}.

\paragraph{Reconciliation methods}

\textit{MinTShrink} \cite{wickramasuriya_optimal_2019} and variants find the optimal \(P\) matrix by solving the following minimization problem for a particular choice of \(W\):
\begin{align}
  \min_P &(\hat{\textbf{y}}_{t} - SP\hat{\textbf{y}}_{t})^T W (\hat{\textbf{y}}_{t} - SP\hat{\textbf{y}}_{t}) \nonumber \\
  & \text{s.t.} \quad PS=I. \label{eq:minp}
\end{align}
Assuming \(W\) is positive definite, this has the following solution (ref. Theorem 1 of \cite{wickramasuriya_optimal_2019}):
\begin{align} 
  P &= (S^TW^{-1}S)^{-1}S^TW^{-1} \nonumber \\
    &= (J - JWU(U^TWU)^{-1}U^T) \;, \label{eq:p1}
\end{align}
in which \(S\) is partitioned as \(S^T = [C^T \; I_n]\), \(J = [0_{n \times m^*} \; I_n]\), \(U^T = [I_{m^*} \; -C]\). In \textit{MinTShrink}, \(W\) is estimated using the shrunk empirical covariance estimate of \cite{schafer_shrinkage_2005}. Simpler choices for \(W\), such as the identity matrix, reduce the solution to the \textit{Ordinary Least Squares (OLS)} solution of \cite{hyndman_optimal_2011}. In \textit{ERM}, \citet{bentaieb_regularized_2019} note than \textit{MinTShrink} and variants rely on the assumption of unbiasedness of the base forecasts. Therefore, they relax this assumption by formulating the hierarchical reconciliation problem as an \textit{Empirical Risk Minimization} problem, introducing the \textit{ERM} method. In addition, they propose two regularized variants of \textit{ERM} aimed at reducing forecast variance.

\paragraph{Other methods} \textit{Hier-E2E} \cite{rangapuram_endtoend_2021} solves the problem of Equation~\eqref{eq:hfp} by learning a neural network model that combines the forecasting and reconciliation step in a single model, resulting in an end-to-end solution removing the need for a post-processing step. Similarly, \textit{COPDeepVAR} \cite{rangapuram_coherent_2023} is an end-to-end neural network method that enforces temporal hierarchies, however this is a univariate method that is not able to enforce structural hierarchies (i.e. cross-sectional hierarchies) simultaneously, and therefore not attractive for our task. \textit{SHARQ} \cite{han_simultaneously_2021} also moves the reconciliation step into the training phase and achieves reconciliation using a regularized loss function, where the regularization enforces the coherency. However, this method does not enforce absolute coherency to the hierarchy.

\section{Hierarchical Forecasting in Large-scale Settings}   \label{sec:ourwork}
Our main motivations of this paper are the limitations of prior work for problem settings with many time series.

\paragraph{Scaling issues with reconciliation methods} \label{sec:scalingissuesreconmethods}
In reconciliation methods, we see the following issues when scaling to many time series:
\begin{itemize}
  \item The reconciliation is performed as a \textit{post-processing} step, and thus has to be performed as an additional step after generating the base forecasts. Even though \(P\) in Eq.~\ref{eq:hfp} can be computed once using Eq.~\eqref{eq:p1}, the reconciliation still needs to be performed after each base forecast is produced. Also, \(P\) ideally is sparse \cite{bentaieb_regularized_2019}, but no reconciliation method guarantees this so computing Eq.~\ref{eq:hfp} will generally be a dense matrix-vector product that scales with the number of timeseries.
  \item For \textit{MinTShrink} \cite{wickramasuriya_optimal_2019}, estimating \(W\) according to the method of \cite{schafer_shrinkage_2005} is computationally expensive, as computing this estimate has a computational complexity of \(O(Nn^2)\), with \(N\) denoting the number of training samples used to compute the shrunk covariance estimate. In addition, the shrunk covariance estimate of \cite{schafer_shrinkage_2005} is not guaranteed to give consistent results in high-dimensional settings \cite{touloumis_nonparametric_2015}, making it less applicable for problem settings with many time series. Finally, the estimate for \(W\) will generally be a dense matrix, so we cannot make use of efficient sparse algorithms to solve Eq.~\eqref{eq:p1}. However, even for simpler, sparse choices of \(W\) (such as the identity matrix of \textit{OLS} \cite{hyndman_optimal_2011}), we still need to invert a matrix of size \(m^* \times m^*\) in order to solve Eq.~\eqref{eq:p1}, which becomes computationally costly for problems with many aggregations, which naturally arise in retail forecasting scenarios. For example, for the M5 retail forecasting competition \cite{makridakis_m5_2021}, \(m^*=12,350\), even though there are only 3,049 unique products in this dataset.  
  \item For \textit{ERM} and its regularized variants \cite{bentaieb_regularized_2019}, we need to either invert multiple dense matrices that scale quadratically with the number of time series, or we need to compute a Kronecker product that scales quadratically with the number of time series, followed by an expensive lasso search procedure. Improving the computational complexity of the \textit{ERM} methods is also mentioned in \cite{bentaieb_regularized_2019} as an avenue for future work.
\end{itemize}

\paragraph{Scaling issues with other methods} \label{sec:scalingissuesneuralmethods} \textit{Hier-E2E} \cite{rangapuram_endtoend_2021} is a multivariate method, which means both input and output of the neural network scale with the number of time series. For neural networks, this significantly adds to the training cost and parameter cost as a large amount of parameters are required to handle all the separate time series. This in turn requires GPUs with more memory to train these models, which increases cost to operate them. 

\section{Sparse Hierarchical Loss}
We now present our main technical contribution. To find the best forecasts for the hierarchical forecasting problem \eqref{eq:hfp}, we will optimize the following loss function:
\begin{align} \label{eq:whloss}
  L &= \sum \left[ \frac{\frac{1}{2}(\textbf{y}_{t} - \hat{\textbf{y}}_{t})^2}{l \sum_{j=1}^n S_{ij}} \right] \; ,
\end{align}
in which \(l\) denotes the number of levels in the hierarchy and \(ij\) are the (row, column) indices of the elements of the \(S\)-matrix. Note that \eqref{eq:whloss} resembles the \textit{Weighted Root Mean Squared Error} from the M5 competition \cite{makridakis_m5_2022}.

We are interested in finding bottom-level forecasts that can be aggregated according to a pre-specified hierarchy \(S\), thus \(P = [0_{n \times m^*} \; I_n] \) in ~\ref{eq:hfp} and:
\begin{equation} \label{eq:hfpbu}
  \tilde{\textbf{y}}_{t} = S \hat{\textbf{y}}^n_{t} \;,
\end{equation}
where \(\hat{\textbf{y}}^n_{t}\) denotes the vector of bottom-level forecasts of length \(n\). 
Considering only bottom-level forecasts has a number of benefits: (i) each forecast is coherent to any hierarchy by design, and (ii) we reduce the number of required forecasts from \(m\) to \(n\), which can be a significant reduction (there is no need for a forecast for \(m^*\) aggregations in the hierarchy). 
Now, we can derive the gradient and hessian of \eqref{eq:whloss} with respect to the bottom-level forecasts \(\hat{\textbf{y}}^n_{t}\) : 
\begin{align} 
  \frac{\partial L}{\partial \hat{\textbf{y}}_{t}} &=  \left[ \frac{(\hat{\textbf{y}}_{t} - \textbf{y}_{t})}{l \sum_{j=1}^n S_{j}} \right] \;, \label{eq:hfp_grad} \\
  \frac{\partial L}{\partial \hat{\textbf{y}}^n_{t}} &= \left(\frac{\partial L}{d \hat{\textbf{y}}_{t}}\right)^\intercal S \;, \label{eq:hfpbu_grad}  \\
  \frac{\partial^2 L}{\partial \hat{\textbf{y}}_{t}^{n, 2}} &= \left[ \sum_{j=1}^n \frac{S}{l \sum_{j=1}^n S_{ij}} \right]\;. \label{eq:hfpbu_hess}                                               
\end{align}

\paragraph{Analysis} The best possible forecast is achieved when the loss \eqref{eq:whloss} is minimized, or equivalently when the gradient \eqref{eq:hfp_grad} is zero:
\begin{align} 
  \frac{\partial L}{\partial \hat{\textbf{y}}_{t}} &=  \left[ \frac{(\hat{\textbf{y}}_{t} - \textbf{y}_{t})}{l \sum_{j=1}^n S_{j}} \right] \;, \nonumber \\
                                                   &=  \left[ \frac{(S \hat{\textbf{y}}^n_{t} - S \textbf{y}^n_{t})}{l \sum_{j=1}^n S_{j}} \right] \;, \nonumber \\
                                                   &=  \left[ \frac{S(\hat{\textbf{y}}^n_{t} - \textbf{y}^n_{t})}{l \sum_{j=1}^n S_{j}} \right] \;, \nonumber
\end{align}
which becomes zero when \(\hat{\textbf{y}}^n_{t} = \textbf{y}^n_{t}\). Thus, the best forecast model is found when each bottom-level forecast equals the ground truth. This is equivalent to the standard (i.e., non-hierarchical) squared error loss often used in forecasting problems. We argue that our hierarchical loss gradient can be seen as a \textit{smoothed} gradient compared to the standard squared error loss gradient (i.e. \((\hat{\textbf{y}}^n_{t} - \textbf{y}^n_{t})\)). For example, consider the canonical case where we have two bottom-level time series (\(n=2\)) and a single aggregation (the sum of the two time series), thus \(m^* = 1\) and \(m = m^* + n = 3\). Finally, there are two levels in our hierarchy, thus \(l = 2\). Now, we have:
\begin{align}
  \tilde{\textbf{y}}_{t} = \underbrace{
    \begin{bmatrix}
    1 &1 \\
    1 &0 \\
    0 &1
    \end{bmatrix}}_S 
    \underbrace{    
    \begin{bmatrix}
      \hat{\textbf{y}}^1_{t} \\
      \hat{\textbf{y}}^2_{t}
    \end{bmatrix}}_{\hat{\textbf{y}}^n_{t}} \;,
\end{align}
and the gradient of the loss with respect to the bottom level time series \eqref{eq:hfpbu_grad} reads:
\begin{align} 
  \begin{bmatrix}
  \frac{\partial L}{\partial \hat{\textbf{y}}^1_{t}} \\
  \frac{\partial L}{\partial \hat{\textbf{y}}^2_{t}}
  \end{bmatrix}^\intercal 
  &= \left[ \frac{S(\hat{\textbf{y}}^n_{t} - \textbf{y}^n_{t})}{l \sum_{j=1}^n S_{j}} \right]^\intercal S  \;, \nonumber \\
  &=  \left[ \frac{
    \begin{bmatrix}
      1 &1 \\
      1 &0 \\
      0 &1
    \end{bmatrix}
    \left(  
    \begin{bmatrix}
      \hat{\textbf{y}}^1_{t} \\
      \hat{\textbf{y}}^2_{t}
    \end{bmatrix}  
    - 
    \begin{bmatrix}
      \textbf{y}^1_{t} \\
      \textbf{y}^2_{t}
    \end{bmatrix}  
    \right)
    }{2 
    \begin{bmatrix}
      2 &1 &1
    \end{bmatrix}^\intercal  
    } \right]^\intercal
    \begin{bmatrix}
      1 &1 \\
      1 &0 \\
      0 &1
    \end{bmatrix}  \;, \nonumber \\
  &=  \left[ \frac{
    \begin{bmatrix}
      \hat{\textbf{y}}^1_{t} + \hat{\textbf{y}}^2_{t} - \textbf{y}^1_{t} - \textbf{y}^2_{t} \\
      \hat{\textbf{y}}^1_{t} - \textbf{y}^1_{t} \\
      \hat{\textbf{y}}^2_{t} - \textbf{y}^2_{t} \\
    \end{bmatrix}
    }{ 
    \begin{bmatrix}
      4 &2 &2
    \end{bmatrix}^\intercal  
    } \right]^\intercal
    \begin{bmatrix}
      1 &1 \\
      1 &0 \\
      0 &1
    \end{bmatrix}  \;, \nonumber \\
  &=          
   \begin{bmatrix}
     \frac{1}{4}(\hat{\textbf{y}}^1_{t} + \hat{\textbf{y}}^2_{t} - \textbf{y}^1_{t} - \textbf{y}^2_{t}) \\
     \frac{1}{2}(\hat{\textbf{y}}^1_{t} - \textbf{y}^1_{t}) \\
     \frac{1}{2}(\hat{\textbf{y}}^2_{t} - \textbf{y}^2_{t}) \\
   \end{bmatrix}^\intercal
   \begin{bmatrix}
     1 &1 \\
     1 &0 \\
     0 &1
   \end{bmatrix}  \;, \nonumber \\
  &=
  \begin{bmatrix}
    \frac{3}{4}(\hat{\textbf{y}}^1_{t} - \textbf{y}^1_{t}) + \frac{1}{4}(\hat{\textbf{y}}^2_{t} - \textbf{y}^2_{t}) \\
    \frac{1}{4}(\hat{\textbf{y}}^1_{t} - \textbf{y}^1_{t}) + \frac{3}{4}(\hat{\textbf{y}}^2_{t} - \textbf{y}^2_{t}) \\
  \end{bmatrix}^\intercal  \; \nonumber.      
\end{align}  
Hence, we \textit{smooth} the bottom-level gradient by adding to it portions of the gradients of all aggregations the bottom-level series belongs to. This derivation also shows the motivation of adding the denominator part \(l \sum_{j=1}^n S_{ij} \) to the loss function \eqref{eq:whloss}: it is neccessary to scale the aggregation gradients by the number of elements in the aggregation, otherwise the magnitude of the gradient grows with the number of time series and the number of levels in the hierarchy, which is undesirable when trying to facilitate stable learning.
  
\paragraph{Sparsity} \(S\) is highly sparse, as it has only \(nl\) non-zero elements: the number of bottom-level time series multiplied by the number of aggregations in the hierarchy. Hence, the overall sparsity of \(S\) is given by \( 1 - \frac{nl}{mn} \). For example, for the M5 dataset \cite{makridakis_m5_2021}, \(n = 3,049\), \(l = 12\), \(m=42,840\), corresponding to a sparsity of 99.97\%. This motivates the use of sparse linear algebra when computing Equations~\eqref{eq:whloss}-\eqref{eq:hfpbu_hess}.

% it can be rewritten as:
% \begin{align}
%   S &= \left[I_n \; | \; S^a_{(m^*-1) \times n} \; | \; I_{n \times n}\right]^\intercal \; ,
% \end{align}
% with \(I_n\) denoting the identity vector corresponding to the top level aggregation, \(S^a\) the matrix containing the lower level aggregation components of the \(S\) matrix, which contains \(l - 2\) aggregations of size \(n\) each, and \(I_{n \times n}\) denoting the identity matrix corresponding to the bottom-level. Hence, the overall sparsity ratio of \(S\) - the size of the array divided by the number of non-zero elements - is \( \frac{n + n(l - 2) + n}{mn} = \frac{nl}{mn}\). For example, for the M5 dataset \cite{makridakis_m5_2022}, \(n = 3,049\), \(l = 12\), \(m=42,840\), corresponding to a sparsity of 99.97\%.
% \begin{align}
%   s &= \frac{n + nl + n}{mn} \\
%     &= \frac{n(2 + l)}{mn} \\
%     &= \frac{(2 + l)}{m}
% \end{align}
% - the size of the array divided by the number of non-zero elements - 

\paragraph{Implementation} We implement the hierachical loss \eqref{eq:whloss}, the bottom-level gradient \eqref{eq:hfpbu_grad} and hessian \eqref{eq:hfpbu_hess} in Python using the sparse library from \textit{SciPy} \cite{virtanen_scipy_2020}. Our implementation, including the code to reproduce the experiments from Section~\ref{sec:experiments}, is available on GitHub.\footnote{https://github.com/elephaint/hfas}

\section{Experiments}
  \label{sec:experiments}
  In this section we empirically verify the usefulness of our sparse hierarchical loss. First, we evaluate forecasting accuracy using a set of small experiments on the public M5 dataset \cite{makridakis_m5_2021}. Then, we evaluate our sparse hierarchical loss in an offline experiment on a proprietary dataset from our e-commerce partner.

  \subsection{Public datasets} \label{subsec:publicdatasets}
  \paragraph{Task \& Dataset} Our task is to forecast product demand one-day ahead. We use the M5 dataset \cite{makridakis_m5_2021} for our offline, public dataset experiments. The M5 dataset contains product-level sales from Walmart for 3,049 products across 10 stores in the USA. Furthermore, the dataset contains 12 cross-sectional product aggregations (e.g. department, region), which allow us to test hierarchical forecasting performance. We preprocess the dataset resulting in a set of features as described in \ref{app:m5dataset}.
  
  \paragraph{Baseline models} For our baseline forecasting model, we use LightGBM \cite{ke_lightgbm_2017}. Tree-based models dominated the M5 forecasting competition due to their strong performance and ease of use \cite{makridakis_m5_2022,januschowski_forecasting_2022}. Moreover, our e-commerce partner's primary product forecasting is a LightGBM-based model, so we expect results from offline experiments on public datasets to transfer to our proprietary setting when using the same base forecasting model. We note that deep learning based approaches are becoming more prevalent in e-commerce \cite{kunz_deep_2023}, especially with the rise of the Transformer-architecture in forecasting models \cite{lim_temporal_2021,li_enhancing_2019}. We consider this for future work, and did not consider this for our study as (i) the cloud cost to operate these models is 10x higher for our e-commerce partner as compared to a tree-based model and (ii) none of the neural-network based methods are able to scale to the size of our e-commerce partner, as explained in Section~\ref{sec:scalingissuesneuralmethods}.

  \paragraph{Experimental setup} To test our hierarchical sparse loss function against baseline forecasting systems, we consider the following scenarios:
  \begin{enumerate}
    \item \textbf{Bottom-up} We train a single global model on only the bottom-level timeseries.
    \item \textbf{Separate aggregations} We train separate models for every aggregation in the hierarchy, resulting in 12 models for the entire M5 dataset.
    \item \textbf{Global} We train a single global model on all timeseries in the dataset, including all the aggregations.
  \end{enumerate}
  For the first scenario in our experiments (\textit{Bottom-up}), we vary both the \textit{objective} (i.e. loss function that is optimized by LightGBM) and the \textit{evaluation metric} (i.e. the loss function that governs early-stopping during hyperparameter optimization). For the \textit{objective}, we consider the \textit{squared error loss} (SL), the \textit{Tweedie loss} (TL) and our sparse Hierarchical Loss (HL). The Tweedie loss is a loss function that assumes that the timeseries follow a distribution somewhere in between a Poisson and a Gamma distribution, which is useful in zero-inflated settings such as retail demand forecasting. It is a loss favored by contestants in the M5 forecasting competition \cite{januschowski_forecasting_2022}, and it is the loss also used in the primary forecasting system of our e-commerce partner.

  For the latter two settings, we will obtain non-coherent forecasts. Thus, these methods require a reconciliation post-processing step to reconcile the forecasts to the hierarchy. We employ the following reconciliation methods:
  \begin{itemize}
    \item \textbf{Base} No reconciliation is performed.
    \item \textbf{OLS} Ordinary Least Squares (OLS) \cite{hyndman_optimal_2011}, where \(W\) in Equation~\eqref{eq:minp} is the identity matrix.
    \item \textbf{WLS-struct} and \textbf{WLS-var} Weighted Least Squares (WLS) \cite{wickramasuriya_optimal_2019}, where \(W\) in Equation~\eqref{eq:minp} is a diagonal matrix containing respectively the sum of the rows of \(S\) (\textit{WLS-struct}) or the in-sample forecast errors (\textit{WLS-var}).
    \item \textbf{MinT-cov} and \textbf{MinT-shrink}. \textit{Trace Minimization} \cite{wickramasuriya_optimal_2019}, where \(W\) in Equation~\eqref{eq:minp} is respectively the covariance matrix of in-sample forecast errors (\textit{MinT-cov}) or the shrunk covariance matrix of in-sample forecast errors (\textit{MinT-shrink}).
    \item \textbf{ERM} The Empirical Risk Minimization (ERM) method \cite{bentaieb_regularized_2019}. Due to computational issues explained in Section~\ref{sec:scalingissuesreconmethods}, we were not able to apply the regularized ERM variants to our experiments.
  \end{itemize}
  
  We optimize the hyperparameters of each of the LightGBM models by Bayesian hyperparameter optimization using Optuna \cite{akiba_optuna_2019}. The settings for the hyperparameter optimization can be found in \ref{app:hyperparam}. Each model is trained for 10 different random seeds, and our results are based on the mean of those 10 rollouts. 

  \paragraph{Evaluation} We evaluate our results for every aggregation in the hierarchy using the Root Mean Squared Error (RMSE) \cite{hyndman_forecasting_2021}. We present the RMSE relative to the \textit{Bottom-up} scenario using the squared-loss objective with the squared-loss metric. For absolute values and standard deviation of the results, see \ref{app:experiments}.

  \paragraph{Results - single store} We first run our experiment only on a single store of the M5 dataset to establish that our method works correctly. We present the results in Table~\ref{tab:singlestore_rel}. The best method is the \textit{bottom-up} method that uses the \textit{Tweedie Loss} in combination with our hierarchical loss as metric. However, differences are small and compared to the baseline, the result is not significantly better. We also conclude that our hierachical loss works, but not as good as the baseline method and some of its variants. Finally, we find that from the reconciliation methods, \textit{MinT-shrink} and \textit{WLS-var} perform best when reconciling forecasts from multiple models, whereas \textit{MinT-Cov} performs best when reconciling forecasts from a single, global model.

  \begin{table*}[t]
    \caption{Forecasting results for a single store on the M5 dataset. We report relative RMSE as compared to the baseline (shown in italic). Bold indicates best method for the aggregation. For absolute values and standard deviation of the results, see \ref{app:experiments}.}
    \label{tab:singlestore_rel}
    \begin{center}
    \begin{tabular}{l c  cccccc}
    \toprule 
     & &  &\multicolumn{4}{ c }{Aggregation level}   &  \\
     \cmidrule(r){4-7}
    Scenario/Objective & Metric  & Reconciliation & Product & Department & Category & Total & All series \\
    \midrule									
    \texttt{Bottom-up}									\\
    \hspace{0.1cm} 	\textit{SL}	&\textit{SL}	&\textit{None}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	\\
    \hspace{0.1cm} 	SL	&HL	&None	&1.00	&1.03	&1.05	&1.07	&1.05	\\
    \hspace{0.1cm} 	HL	&HL	&None	&1.00	&1.04	&1.06	&1.07	&1.05	\\
    \hspace{0.1cm} 	HL	&SL	&None	&1.00	&1.03	&1.05	&1.02	&1.03	\\
    \hspace{0.1cm} 	TL	&HL	&None	&\textbf{1.00}	&\textbf{0.99}	&\textbf{0.99}	&\textbf{0.99}	&\textbf{0.99}	\\
    \hspace{0.1cm} 	TL	&SL	&None	&\textbf{1.00}	&1.05	&1.06	&1.09	&1.06	\\
    \hspace{0.1cm} 	TL	&TL	&None	&1.03	&1.64	&1.72	&1.73	&1.66	\\
    \midrule									
    \texttt{Sep. agg.}									\\
    \hspace{0.1cm} 	SL	&SL	&Base	&1.00	&1.20	&1.31	&1.46	&1.33	\\
    \hspace{0.1cm} 	SL	&SL	&OLS	&1.00	&1.27	&1.37	&1.32	&1.30	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-struct	&1.00	&1.11	&1.16	&1.10	&1.11	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-var	&1.00	&1.03	&1.05	&1.00	&1.02	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-cov	&1.01	&1.13	&1.21	&1.17	&1.16	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&1.00	&1.03	&1.05	&1.01	&1.02	\\
    \hspace{0.1cm} 	SL	&SL	&ERM	&2.40	&2.36	&2.34	&2.04	&2.24	\\
    \midrule									
    \texttt{Global}									\\
    \hspace{0.1cm} 	SL	&SL	&Base	&1.02	&1.40	&1.60	&1.53	&1.49	\\
    \hspace{0.1cm} 	SL	&SL	&OLS	&1.02	&1.45	&1.54	&1.44	&1.45	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-struct	&1.02	&1.23	&1.30	&1.25	&1.25	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-var	&1.02	&1.06	&1.09	&1.07	&1.07	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-cov	&1.02	&1.03	&1.02	&1.02	&1.02	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&1.02	&1.06	&1.09	&1.07	&1.07	\\
    \hspace{0.1cm} 	SL	&SL	&ERM	&1.86	&1.61	&1.49	&1.41	&1.52	\\
    
    \bottomrule
    \end{tabular}
    \end{center}
    \end{table*}

  \paragraph{Results - all stores} We then run our experiment on all stores of the M5 dataset. We present the results in Table~\ref{tab:allstores_rel}.

  \begin{sidewaystable*}[t]
    \caption{Forecasting results for all stores on the M5 dataset. We report relative RMSE as compared to the baseline (shown in italic). Bold indicates best method for the aggregation. For absolute values and standard deviation of the results, see \ref{app:experiments}.}
    \label{tab:allstores_rel}
    \begin{center}
    {\small\setlength{\tabcolsep}{2pt} 
    \begin{tabular}{l c  cccccccccccccc}
    \toprule 
     &&&& &  &\multicolumn{3}{ c }{Store}   &\multicolumn{2}{ c }{Product} &\multicolumn{3}{ c }{State} \\
     \cmidrule(r){7-9} \cmidrule(r){10-11} \cmidrule(r){12-14}
    Scenario/Objective & Metric  & Reconciliation &Product	&Department	&Category &Department	&Category	&Total &Store	&State &Department &Category &Total	&Total	&All series \\
    \midrule																	
    \texttt{Bottom-up}																	\\
    \hspace{0.1cm} 	\textit{SL}	&\textit{SL}	&\textit{None}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	\\
    \hspace{0.1cm} 	SL	&HL	&None	&1.00	&0.96	&0.96	&0.97	&0.96	&0.97	&1.00	&1.00	&0.97	&0.96	&0.97	&0.96	&0.97	\\
    \hspace{0.1cm} 	HL	&HL	&None	&1.00	&0.88	&0.86	&0.94	&0.92	&0.93	&0.98	&0.99	&0.90	&0.89	&0.90	&0.88	&0.89	\\
    \hspace{0.1cm} 	HL	&SL	&None	&\textbf{0.99}	&\textbf{0.86}	&\textbf{0.84}	&0.92	&\textbf{0.91}	&\textbf{0.92}	&\textbf{0.98}	&\textbf{0.99}	&\textbf{0.89}	&\textbf{0.87}	&\textbf{0.89}	&\textbf{0.86}	&\textbf{0.88}	\\
    \hspace{0.1cm} 	TL	&HL	&None	&1.00	&0.98	&0.99	&1.00	&1.00	&1.00	&1.00	&1.00	&0.98	&0.99	&1.01	&1.02	&1.00	\\
    \hspace{0.1cm} 	TL	&SL	&None	&1.00	&0.97	&0.96	&1.02	&1.02	&1.02	&1.01	&1.01	&1.00	&1.00	&1.00	&0.97	&0.99	\\
    \hspace{0.1cm} 	TL	&TL	&None	&1.06	&1.73	&1.72	&1.34	&1.36	&1.35	&1.27	&1.13	&1.48	&1.48	&1.46	&1.68	&1.57	\\
    \midrule																	
    \texttt{Sep. agg.}																\\
    \hspace{0.1cm} 	SL	&SL	&Base	&1.01	&1.59	&1.38	&1.15	&1.15	&1.66	&1.17	&1.02	&1.35	&2.05	&1.28	&1.41	&1.48	\\
    \hspace{0.1cm} 	SL	&SL	&OLS	&1.02	&1.56	&1.51	&1.29	&1.30	&1.27	&1.12	&1.05	&1.43	&1.45	&1.32	&1.31	&1.39	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-struct	&1.01	&1.39	&1.39	&1.15	&1.16	&1.11	&1.07	&1.03	&1.27	&1.28	&1.23	&1.27	&1.28	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-var	&1.01	&1.27	&1.26	&1.09	&1.10	&1.07	&1.06	&1.03	&1.17	&1.17	&1.14	&1.21	&1.18	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-cov	&79.23	&23.00	&17.8	&34.08	&27.87	&23.05	&75.12	&77.53	&28.71	&23.34	&20.25	&16.19	&23.95	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&1.01	&1.33	&1.31	&1.08	&1.08	&1.03	&1.06	&1.03	&1.19	&1.18	&1.13	&1.23	&1.2	\\
    \hspace{0.1cm} 	SL	&SL	&ERM	&1.74	&1.56	&1.56	&1.43	&1.33	&1.34	&2.00	&1.85	&1.43	&1.4	&1.46	&1.68	&1.53	\\
    \midrule																	
    \texttt{Global}																	\\
    \hspace{0.1cm} 	SL	&SL	&Base	&1.01	&1.24	&1.32	&0.92	&0.93	&0.95	&1.00	&1.01	&0.99	&1.05	&1.20	&1.75	&1.32	\\
    \hspace{0.1cm} 	SL	&SL	&OLS	&1.01	&1.23	&1.26	&1.04	&1.03	&1.07	&1.01	&1.01	&1.12	&1.13	&1.20	&1.35	&1.22	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-struct	&1.01	&1.01	&1.02	&0.95	&0.93	&0.93	&1.01	&1.01	&0.97	&0.97	&0.99	&1.05	&1.00	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-var	&1.01	&0.97	&0.97	&0.95	&0.94	&0.93	&1.02	&1.01	&0.96	&0.95	&0.96	&0.98	&0.97	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-cov	&188.31	&41.03	&26.28	&93.75	&78.41	&70.08	&184.82	&183.42	&77.28	&62.97	&53.63	&23.73	&57.13	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&1.01	&0.99	&1.00	&\textbf{0.91}	&0.90	&0.89	&1.01	&1.01	&0.95	&0.95	&0.96	&1.01	&0.97	\\
    \hspace{0.1cm} 	SL	&SL	&ERM	&1.74	&1.26	&1.30	&1.34	&1.26	&1.24	&2.04	&1.87	&1.29	&1.25	&1.26	&1.32	&1.30	\\
    
    
    \bottomrule
    \end{tabular}}
    \end{center}
    \end{sidewaystable*}


  \paragraph{Analysis: compute time}
  
  \paragraph{Analysis: impact of hierarchy}

  \paragraph{Analysis: impact of temporal aggregations}

  
  \subsection{Proprietary datasets}
    \label{subsec:proprietarydatasets}

\section{Discussion}
  \label{sec:discussion}


\section{Conclusion}
  \label{sec:conclusion}

\section*{Acknowledgments}
  This research was (partially) funded by the Hybrid Intelligence Center, a 10-year program funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research.\footnote{\url{https://www.hybrid-intelligence-centre.nl/}}

  All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
  
  We thank the reviewers for their constructive feedback and help on improving our work. 


\bibliographystyle{elsarticle-num-names} 
\bibliography{lib}

\clearpage

\appendix

\section{M5 Dataset} \label{app:m5dataset}

\section{Hyperparameter optimization} \label{app:hyperparam}

\section{Experiments} \label{app:experiments}

\begin{table*}[t]
  \caption{Forecasting results for a single store on the M5 dataset. We report RMSE scores with standard deviation in brackets. Bold indicates best method for the aggregation.}
  \label{tab:singlestore_abs}
  \begin{center}
  \begin{tabular}{l c  cccccc}
  \toprule 
   &    &  &\multicolumn{4}{ c }{Aggregation level}   &  \\
   \cmidrule(r){4-7}
  Scenario/Obj. & Metric  & Reconciliation & Product & Department & Category & Total & All series \\
  \midrule									
  \texttt{Bottom-up}									\\
  \hspace{0.1cm} 	SL	&SL	&None	&2.01 (0.00)	&74.81 (1.17)	&124.27 (2.27)	&263.94 (7.33)	&7.40 (0.15)	\\
  \hspace{0.1cm} 	SL	&HL	&None	&2.01 (0.00)	&77.27 (1.33)	&130.41 (2.78)	&281.41 (6.79)	&7.76 (0.15)	\\
  \hspace{0.1cm} 	HL	&HL	&None	&2.01 (0.00)	&77.75 (2.35)	&131.83 (4.85)	&281.94 (15.26)	&7.80 (0.31)	\\
  \hspace{0.1cm} 	HL	&SL	&None	&2.01 (0.00)	&77.03 (2.32)	&129.87 (5.33)	&267.97 (14.67)	&7.59 (0.30)	\\
  \hspace{0.1cm} 	TL	&HL	&None	&\textbf{2.00 (0.00)}	&\textbf{74.06 (2.29)}	&\textbf{122.47 (5.02)}	&\textbf{262.30 (10.78)}	&\textbf{7.33 (0.26)}	\\
  \hspace{0.1cm} 	TL	&SL	&None	&\textbf{2.00 (0.00)}	&78.53 (1.98)	&131.48 (4.65)	&286.75 (12.54)	&7.87 (0.27)	\\
  \hspace{0.1cm} 	TL	&TL	&None	&2.07 (0.00)	&122.98 (1.85)	&213.13 (3.10)	&455.70 (8.94)	&12.30 (0.20)	\\
  \midrule				
  \texttt{Sep.~agg.}									\\					
  \hspace{0.1cm} 	SL	&SL	&Base	&2.01 (0.00)	&89.65 (1.78)	&162.52 (3.97)	&386.67 (21.94)	&9.86 (0.28)	\\
  \hspace{0.1cm} 	SL	&SL	&OLS	&2.01 (0.00)	&94.88 (1.84)	&169.86 (4.27)	&348.07 (13.68)	&9.62 (0.25)	\\
  \hspace{0.1cm} 	SL	&SL	&WLS-struct	&2.01 (0.00)	&83.31 (0.90)	&144.7 (1.78)	&289.46 (5.69)	&8.23 (0.10)	\\
  \hspace{0.1cm} 	SL	&SL	&WLS-var	&2.01 (0.00)	&77.21 (0.75)	&130.46 (1.76)	&264.99 (6.08)	&7.57 (0.11)	\\
  \hspace{0.1cm} 	SL	&SL	&MinT-cov	&2.02 (0.00)	&84.51 (3.90)	&150.38 (9.21)	&308.69 (27.17)	&8.59 (0.56)	\\
  \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&2.01 (0.00)	&77.16 (0.76)	&130.66 (1.92)	&265.27 (6.68)	&7.58 (0.12)	\\
  \hspace{0.1cm} 	SL	&SL	&ERM	&4.81 (0.16)	&176.56 (20)	&290.94 (42.84)	&539.3 (67.07)	&16.55 (1.62)	\\
  \midrule									
  \texttt{Global}									\\
  \hspace{0.1cm} 	SL	&SL	&Base	&2.04 (0.01)	&104.82 (4.49)	&198.45 (7.22)	&404.37 (22.42)	&11.02 (0.36)	\\
  \hspace{0.1cm} 	SL	&SL	&OLS	&2.05 (0.01)	&108.6 (3.47)	&191.75 (6.44)	&381.04 (17.67)	&10.71 (0.36)	\\
  \hspace{0.1cm} 	SL	&SL	&WLS-struct	&2.04 (0.01)	&92.11 (2.08)	&162.11 (3.36)	&330.51 (7.54)	&9.22 (0.18)	\\
  \hspace{0.1cm} 	SL	&SL	&WLS-var	&2.04 (0.01)	&79.23 (2.26)	&135.78 (4.23)	&282.67 (8.81)	&7.92 (0.22)	\\
  \hspace{0.1cm} 	SL	&SL	&MinT-cov	&2.05 (0.01)	&77.27 (3.32)	&127.32 (7.35)	&269.02 (16.04)	&7.58 (0.38)	\\
  \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&2.04 (0.01)	&78.93 (2.31)	&135.13 (4.36)	&281.22 (9.11)	&7.88 (0.23)	\\
  \hspace{0.1cm} 	SL	&SL	&ERM	&3.73 (0.08)	&120.31 (14.55)	&184.77 (30.82)	&371.55 (70.75)	&11.24 (1.49)	\\  
  \bottomrule
  \end{tabular}
  \end{center}
  \end{table*}



\end{document}
\endinput
