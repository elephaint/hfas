%% 
%% Copyright 2007-2019 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 168 2019-02-25 07:15:41Z apu.v $
%%
%%
%\documentclass[preprint, review, 12pt]{elsarticle}
\documentclass[preprint, 3p, times, twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{acronym}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{makecell}
\usepackage[inline]{enumitem}
\usepackage{bbm}
\usepackage[skip=2pt]{caption}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{rotating}
\usepackage{float}
\usepackage{multirow}
\usepackage{subfigure}

\newcommand{\ssc}[1]{\textcolor{magenta}{SSC: #1.}}

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}
\acrodef{RMSE}{root mean squared error}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}
\journal{International Journal of Forecasting}

\begin{document}

\begin{frontmatter}

\title{Hierarchical Forecasting at Scale}

% \author{}
\author{
  % Olivier~Sprangers \\
  % AIRLab, NL \\
  % University of Amsterdam, NL\\
  % \texttt{o.r.sprangers@uva.nl} \\
  % % \And
  % Wander~Wadman \\
  % bol.com, NL \\
  % \texttt{wwadman@bol.com} \\
%   \And
%   Sebastian~Schelter \\
%   University of Amsterdam, NL \\
%   \texttt{s.schelter@uva.nl} \\
%   \And
%   Maarten~de~Rijke \\
%   University of Amsterdam, NL \\
%   \texttt{m.derijke@uva.nl} \\
}



\begin{abstract}
  Hierarchical forecasting techniques allow the creation of forecasts that are coherent with respect to a pre-specified hierarchy of the underlying time series. This targets a key problem in e-commerce, where we often find millions of products across many product hierarchies, and forecasts need to be made for both individual products and product aggregations. However, existing hierarchical forecasting techniques scale poorly when the number of time series increases, which limits their applicability at a scale of millions of products. 
  
  In this paper, we propose to learn a coherent forecast for millions of products with a single bottom-level forecast model by using a loss function that directly optimizes the hierarchical product structure. We implement our loss function using sparse linear algebra, such that the number of operations in our loss function scales quadratically rather than cubically with the number of products and levels in the hierarchical structure. The benefit of our sparse hierarchical loss function is that it provides practitioners a method of producing bottom-level forecasts that are coherent to any chosen cross-sectional or temporal hierarchy. In addition, removing the need for a post-processing step as required in traditional hierarchical forecasting techniques reduces the computational cost of the prediction phase in the forecasting pipeline, as will as its deployment complexity.  
  
  In our tests on the public M5 dataset, our sparse hierarchical loss function performs up to 10\% better compared to the baseline loss function. Next, we implement our sparse hierarchical loss function within an existing gradient boosting-based forecasting model at bol.com, a large European e-commerce platform. At bol.com, each day a forecast for the weekly demand of every product for the next twelve weeks is required. In this setting our sparse hierarchical loss resulted in an improved forecasting performance as measured by RMSE of about 2\% at the product level, as compared to the the baseline model, and an improvement of about 10\% at the product level as measured by MAE. Finally, we found an increase in forecasting performance of about 5-10\% (both RMSE and MAE) when evaluating the forecasting performance across the cross-sectional hierarchies that we defined. These results demonstrate the usefulness of our sparse HL applied to a production forecasting system at a major e-commerce platform.

\end{abstract}

\begin{keyword}
  Hierarchical forecasting, Large-scale forecasting, Efficiency in forecasting methods
\end{keyword}

\end{frontmatter}

\section{Introduction} \label{sec:intro}
In e-commerce, we are often faced with two forecasting challenges. First, forecasts at the lowest granularity - often the individual product level - are required but we also need forecasts for higher granularities, for example at the category, department or regional level, as higher level forecasts are often needed in logistics and financial planning. Second, forecasts at different time granularities are required, for example daily or weekly forecasts. It is common that separate forecast models are made for each separate (temporal) granularity, and as such these forecasts may not be coherent with each other. Hierarchical forecasting \cite{hyndman_optimal_2011} and temporal hierarchical forecasting techniques \cite{athanasopoulos_forecasting_2017,rangapuram_coherent_2023,theodosiou_forecasting_2021} aim to solve the problem of creating forecasts that are coherent with respect to a pre-specified cross-sectional and/or temporal hierarchy of the underlying time series. 

% It is common that separate forecast models are made for each separate granularity, and as such these forecasts may not be coherent with each other. Hierarchical forecasting techniques \cite{hyndman_optimal_2011} aim to solve the problem of creating forecasts that are coherent with respect to a pre-specified cross-sectional hierarchy of the underlying time series. Second, contemporary large-scale forecasting applications require forecasting many time series concurrently \cite{bose_probabilistic_2017} and at increasingly lower time intervals, thus requiring faster training and prediction times, which in turn requires forecasting methods that scale better when the number of time series increases. In this work, we are interested in solving both these forecasting challenges.

\paragraph{Challenges with existing cross-sectional and temporal hierarchical forecasting techniques} Reconciliation methods \cite{hyndman_optimal_2011,athanasopoulos_forecasting_2017,wickramasuriya_optimal_2019} adjust the forecasts for each level in the hierarchy by minimizing the errors at each forecast level. These methods are applied as a post-processing step that requires a matrix inversion that scales cubically with the number of products or product hierarchies. In settings with millions of products such as in e-commerce, this becomes computationally expensive at prediction time. Neural network methods can optimize for the hierarchy in an end-to-end manner, however these are either multivariate methods that scale poorly to millions of time series \cite{rangapuram_endtoend_2021} or they can only optimize for the temporal hierarchy \cite{rangapuram_coherent_2023}.
  
\paragraph{Sparse loss function} In order to overcome these scaling issues, we design a sparse hierarchical loss function that directly optimizes both cross-sectional and temporal hierarchical structures. Our corresponding sparsity-aware implementation ensures that the number of operations in our loss function scales quadratically rather than cubically with the number of products and levels in the hierarchical structure, enabling computationally efficient training. The benefit of our sparse hierarchical loss function is that it provides practitioners a method of producing bottom-level forecasts that are coherent to any chosen cross-sectional and temporal hierarchy. In addition, removing the need for a post-processing step as used in traditional hierarchical forecasting techniques reduces the computational cost of the prediction phase in the forecasting pipeline. Furthermore, this also reduces the deployment complexity of the forecasting pipeline.

\paragraph{Evaluation} We evaluate our sparse hierarchical loss function on a gradient-boosted forecasting system on the public M5 dataset \cite{makridakis_m5_2022} and a private dataset from our e-commerce partner. For the M5 dataset, we demonstrate that our implementation provides up to 10\% better forecasting performance as measured by both RMSE and MAE compared with (i) reconciliation methods and (ii) baseline bottom-level forecasting methods that use a standard loss function. For the private dataset, we present the results of an offline test on the product-level forecast system of bol.com, a European e-commerce company with a catalog of millions of unique products. We find our sparse HL improves the forecasting performance by about 2\% on RMSE and 10\% on MAE as compared to the baseline forecasting system. This demonstrates the usefulness of our sparse HL in a large-scale setting.

\paragraph{Contributions} In summary, the main contributions of this paper are:
\begin{enumerate}
  \item We design a sparse hierarchical loss function that enables direct end-to-end training of cross-sectional and temporal hierarchical forecasts in large-scale settings in Section~\ref{sec:sparsehloss}.
  \item We empirically demonstrate that our sparse hierarchical loss function can outperform existing hierarchical forecasting reconciliation methods by up to 10\% in Section~\ref{subsec:publicdatasets}.
  \item We show how our sparse hierarchical loss function scales to large-scale settings and demonstrate a reduction of both training and prediction time of up to an order of magnitude compared to the best hierarchical forecasting reconciliation methods (Section~\ref{par:timecomplexity}).
  \item We present the results of an offline test of our method for the primary product demand forecasting model at bol.com, a European e-commerce company with a catalogue of millions of unique products, demonstrating an improvement of 2\% on RMSE and 10\% on MAE as compared to the baseline forecasting system, in Section~\ref{subsec:proprietarydatasets}.
\end{enumerate}

\section{Related work} \label{sec:relwork}

\paragraph{Forecasting for large-scale settings} Contemporary large-scale forecasting applications require forecasting many time series concurrently \cite{bose_probabilistic_2017}. In academia, there has been a surge in the use of neural network-based forecasting methods, which are methods that commonly learn a single forecast model that can produce forecasts for many time series. We refer the interested reader to the recent survey of \citet{benidis_deep_2023} for an overview of these methods. However, tree-based methods topped the M5 forecasting competition \cite{makridakis_m5_2022}, which is believed to be due to the strong implementations available of these algorithms \cite{januschowski_forecasting_2022}, such as the packages LightGBM \cite{ke_lightgbm_2017} or XGBoost \cite{chen_xgboost_2016}. Our own experience within bol.com confirms this view: the ease of use, execution speed and strong default performance are key reasons a tree-based method is often the default choice when creating a new forecast model.

\paragraph{Hierarchical forecasting} Hierarchical forecasting \cite{hyndman_optimal_2011, hyndman_fast_2016, taieb_coherent_2017, bentaieb_regularized_2019, wickramasuriya_optimal_2019} and temporal hierarchical forecasting techniques \cite{taieb_sparse_2017,athanasopoulos_forecasting_2017,rangapuram_coherent_2023,theodosiou_forecasting_2021} aim to solve the problem of creating forecasts that are coherent with respect to a pre-specified cross-sectional and/or temporal hierarchy of the underlying time series. We divide hierarchical forecasting methods into \textit{Reconciliation methods} and \textit{Other methods}.

\textit{Reconciliation methods} These methods solve the hierarchical forecasting problem as a post-processing step by reconciling the forecasts to a pre-specified cross-sectional and/or temporal hierarchy \cite{hyndman_optimal_2011, hyndman_fast_2016, taieb_coherent_2017, bentaieb_regularized_2019, wickramasuriya_optimal_2019, panagiotelis_forecast_2021, girolimetto_point_2023}. Limitations of these approaches are (i) that they require a post-processing step, (ii) computing the reconciliation may be computationally expensive, as we show in Section~\ref{subsec:ourwork}, and (iii) approaches that are computationally less expensive tend to perform worse, as we show in Section~\ref{sec:experiments}. Recent work by \citet{taieb_sparse_2017, bentaieb_regularized_2019} has improved forecasting performance of previous reconciliation approaches but at the expense of even higher computational cost, as we explain in Section~\ref{sec:background}.

\textit{Other methods} In \cite{rangapuram_endtoend_2021,rangapuram_coherent_2023} neural network-based end-to-end hierarchical probabilistic forecasting method are proposed to solve the hierarchical forecasting problem. More recently and most closely related to our work, \citet{han_simultaneously_2021} introduced SHARQ, a method that reconciles probabilistic hierarchical forecasts during training by employing a regularized loss function that aims to improve hierarchical consistency of bottom-up forecasts through regularization. However, the regularization does not strictly enforce the cross-sectional hierarchy in this method.

\section{Background} \label{sec:background}
To understand our problem setting and the issues we identify with existing hierarchical forecasting methods, we introduce the hierarchical forecasting problem and common methods of solving the hierarchical forecasting problem.

Suppose we have a time series \(\vec{y}_t\), where \(t\) denotes the time stamp. We are interested in finding estimates \(\hat{y}_{t}\) of future values \({y}_{t}\) of the time series by employing a model \(f\) based on past values \(y_{t-1}, \dots, y_{t-T}\) of the time series and additional attributes \(X\):
\begin{equation}
  \hat{y}_{t} = f(y_{t-1}, \dots, y_{t-T}, X_{t}, X_{t-1}, \dots, X_{t-T}).
\end{equation}
In our hierarchical forecasting setting, we aim to create forecasts for many time series concurrently, whilst adhering to pre-specified hierarchical relationships that exist between the time series. This can be formalized as \cite{hyndman_forecasting_2021}:
\begin{equation} \label{eq:hfp}
  \tilde{\textbf{y}}_{t} = SP\hat{\textbf{y}}_{t} \;,
\end{equation}
where \(\hat{\textbf{y}}_{t} \in \mathbb{R}^{m} \) denotes the vector of forecasts for all \(m\) time series in the hierarchy, \(S \in \{0, 1\}^{m \times n}\) is a matrix that defines the hierarchical relationship between the \(n\) bottom-level time series and the \(m^* = m - n\) aggregations, \(P \in \mathbb{R}^{n \times m}\) is a matrix that encapsulates the contribution of each forecast to the final estimate, and \(\tilde{\textbf{y}}_{t} \in \mathbb{R}^{m} \) is the vector of forecasts adjusted for the hierarchy. We can use the matrix \(P\) to define various forecast contribution scenarios. Note that we can straightforwardly extend Equation~\eqref{eq:hfp} to the setting of \textit{temporal hierarchies} \cite{athanasopoulos_forecasting_2017,rangapuram_coherent_2023} by considering forecasts of different time granularities in our vector of base forecasts \(\hat{\textbf{y}}_{t}\) and using an appropriate choice of \(S\) to aggregate series of a different time granularity. We will show how cross-sectional and temporal hierarchical forecasting can be combined in Section~\ref{sec:sparsehloss}.

The optimal solution to the problem in Equation~\eqref{eq:hfp} can be found using \textit{Reconciliation methods} and \textit{Other methods}.

\paragraph{Reconciliation methods}

\textit{MinTShrink} \cite{wickramasuriya_optimal_2019} and variants find the optimal \(P\) matrix by solving the following minimization problem for a particular choice of \(W\):
\begin{align}
  \min_P &(\hat{\textbf{y}}_{t} - SP\hat{\textbf{y}}_{t})^T W (\hat{\textbf{y}}_{t} - SP\hat{\textbf{y}}_{t}) \nonumber \\
  & \text{s.t.} \quad PS=I. \label{eq:minp}
\end{align}
Assuming \(W\) is positive definite, this has the following solution (ref. Theorem 1 of \cite{wickramasuriya_optimal_2019}):
\begin{align} 
  P &= (S^TW^{-1}S)^{-1}S^TW^{-1} \nonumber \\
    &= (J - JWU(U^TWU)^{-1}U^T) \;, \label{eq:p1}
\end{align}
in which \(S\) is partitioned as \(S^T = [C^T \; I_n]\), \(J = [0_{n \times m^*} \; I_n]\), \(U^T = [I_{m^*} \; -C]\). In \textit{MinTShrink}, \(W\) is estimated using the shrunk empirical covariance estimate of \cite{schafer_shrinkage_2005}. Simpler choices for \(W\), such as the identity matrix, reduce the solution to the \textit{Ordinary Least Squares (OLS)} solution of \cite{hyndman_optimal_2011}. In \textit{ERM}, \citet{bentaieb_regularized_2019} note than \textit{MinTShrink} and variants rely on the assumption of unbiasedness of the base forecasts. Therefore, they relax this assumption by formulating the hierarchical reconciliation problem as an \textit{Empirical Risk Minimization} problem, introducing the \textit{ERM} method. In addition, they propose two regularized variants of \textit{ERM} aimed at reducing forecast variance. 

\paragraph{Other methods} \textit{Hier-E2E} \cite{rangapuram_endtoend_2021} solves the problem of Equation~\eqref{eq:hfp} by learning a neural network model that combines the forecasting and reconciliation step in a single model, resulting in an end-to-end solution removing the need for a post-processing step. Similarly, \textit{COPDeepVAR} \cite{rangapuram_coherent_2023} is an end-to-end neural network method that enforces temporal hierarchies, however this is a univariate method that is not able to enforce structural hierarchies (i.e. cross-sectional hierarchies) simultaneously, and therefore not suited to our task. \textit{SHARQ} \cite{han_simultaneously_2021} also moves the reconciliation step into the training phase and achieves reconciliation using a regularized loss function, where the regularization enforces the coherency. However, this method does not enforce absolute coherency to the hierarchy.

\subsection{Scaling issues of hierarchical forecasting methods} \label{subsec:ourwork}

Our main motivation for this paper are the limitations of prior work for problem settings with many time series.

\paragraph{Scaling issues with reconciliation methods} \label{sec:scalingissuesreconmethods}
In reconciliation methods, we encounter the following issues when scaling to many time series:
\begin{itemize}
  \item The reconciliation is performed as a \textit{post-processing} step, and thus has to be performed as an additional step after generating the base forecasts. Even though \(P\) in Eq.~\ref{eq:hfp} can be computed once using Eq.~\eqref{eq:p1}, the reconciliation still needs to be performed after each base forecast is produced. Also, \(P\) ideally is sparse \cite{bentaieb_regularized_2019}, but no reconciliation method guarantees this, so computing Eq.~\ref{eq:hfp} will generally be a dense matrix-vector product that scales with the number of time series.
  \item For \textit{MinTShrink} \cite{wickramasuriya_optimal_2019}, estimating \(W\) according to the method of \cite{schafer_shrinkage_2005} is computationally expensive, with a computational complexity of \(O(Nn^2)\), where \(N\) denotes the number of training samples used to compute the shrunk covariance estimate. In addition, the shrunk covariance estimate of \cite{schafer_shrinkage_2005} is not guaranteed to give consistent results in high-dimensional settings \cite{touloumis_nonparametric_2015}, making it less applicable for problem settings with many time series. Finally, the estimate for \(W\) will generally be a dense matrix, so we cannot make use of efficient sparse algorithms to solve Eq.~\eqref{eq:p1}. However, even for simpler, sparse choices of \(W\) (such as the identity matrix of \textit{OLS} \cite{hyndman_optimal_2011}), we still need to invert a matrix of size \(m^* \times m^*\) in order to solve Eq.~\eqref{eq:p1}, which becomes computationally costly for problems with many aggregations, which naturally arise in retail forecasting scenarios. For example, for the M5 retail forecasting competition \cite{makridakis_m5_2021}, \(m^*=12,350\), even though there are only 3,049 unique products in this dataset.  
  \item For \textit{ERM} and its regularized variants \cite{bentaieb_regularized_2019}, we need to either invert multiple dense matrices that scale quadratically with the number of time series, or we need to compute a Kronecker product that scales quadratically with the number of time series, followed by an expensive lasso search procedure. Improving the computational complexity of the \textit{ERM} methods is also mentioned in \cite{bentaieb_regularized_2019} as an avenue for future work.
\end{itemize}

\paragraph{Scaling issues with other methods} \label{sec:scalingissuesneuralmethods} \textit{Hier-E2E} \cite{rangapuram_endtoend_2021} is a multivariate method, which means both input and output of the neural network scale with the number of time series. For neural networks, this significantly adds to the training cost and parameter cost as a large amount of parameters are required to handle all the separate time series. This in turn requires GPUs with more memory to train these models, which increases cost to operate them. 

\section{Sparse Hierarchical Loss} \label{sec:sparsehloss}
In this section we present our main technical contribution, the sparse hierarchical loss. First, we show how cross-sectional and temporal hierarchical forecasting can be combined. Then, we introduce our loss function and demonstrate it via a toy example.

\paragraph{Combining cross-sectional and temporal hierarchical forecasting} We are interested in finding forecasts that can be aggregated according to a pre-specified cross-sectional hierarchy \(S^c \in \{0, 1\}^{m^c \times n^c}\) and temporal hierarchy \(S^t \in \{0, 1\}^{m^t \times n^t}\):
\begin{align} 
  \tilde{\textbf{y}}^c_{t} &= S^c P^c \hat{\textbf{y}}^c_{t} \;, \label{eq:hfpbu_c_full} \\
  \tilde{\textbf{y}}^t &= S^t P^t \hat{\textbf{y}}^t \;. \label{eq:hfpbu_t_full}
\end{align}
These equations can be interpreted as follows:
\begin{itemize}
  \item In Eq. \eqref{eq:hfpbu_c_full}, we aggregate \(n^c\) bottom-level time series from the same timestep \(t\) across a set of \(m^c = n^c + m^{*, c}\) cross-sectional aggregations.
  \item In Eq. \eqref{eq:hfpbu_t_full}, we aggregate each time series consisting of \(n^t\) timesteps across a set of \(m^t = n^t + m^{*, t}\) temporal aggregations, hence we drop the subscript \(t\).
\end{itemize}
We will only create bottom-level forecasts, thus \(P^c = [0_{n^c} \times m^{*, c} \; I_{n^c}] \) and \(P^t = [0_{n^t} \times m^{*, t} \; I_{n^t}] \), yielding:
\begin{align} 
  \tilde{\textbf{y}}^c_{t} &= S^c \hat{\textbf{y}}^{n^c}_{t} \;, \label{eq:hfpbu_c} \\
  \tilde{\textbf{y}}^t_{t} &= S^t \hat{\textbf{y}}^{n^t}_{t} \;. \label{eq:hfpbu_t}
\end{align}
Considering only bottom-level forecasts has a number of benefits: (i) each forecast is coherent to any hierarchy by design, and (ii) we reduce the number of required forecasts from \(m\) to \(n\), which can be a significant reduction (there is no need for a forecast for \(m^*\) aggregations in the hierarchy). 
We now construct a matrix of bottom-level forecasts \(\hat{\textbf{Y}} \in \mathbb{R}^{n^c \times n^t}\), in which the columns represent the forecasts of the bottom-level time series at a timestep \(t\). This allows us to combine \eqref{eq:hfpbu_c}-\eqref{eq:hfpbu_t} as follows:
\begin{equation}
  \tilde{\textbf{Y}} = S^c \hat{\textbf{Y}} (S^t)^\intercal \;, \label{eq:hfp_tc} 
\end{equation}
in which \(\tilde{\textbf{Y}} \in \mathbb{R}^{m^c \times m^t}\) represents the matrix of forecasts aggregated according to both cross-sectional and temporal hierarchies. Equivalently, we can aggregate our bottom-level ground truth values \(\bar{\textbf{Y}} \in \mathbb{R}^{n^c \times n^t}\):
\begin{equation}
  \textbf{Y} = S^c \bar{\textbf{Y}} (S^t)^\intercal \;.
\end{equation}

\paragraph{Sparse hierarchical loss} To find the best forecasts for the hierarchical forecasting problem \eqref{eq:hfp_tc}, we try to find a model \(f(\cdot)\) by gradient-based optimization of the following loss function:
\begin{align} \label{eq:whloss}
  L &= \sum \left[ \vcenter{\hbox{$\displaystyle \frac{ \frac{1}{2}\left(\textbf{Y} - \tilde{\textbf{Y}}\right)^2}{ \underbrace{\left[ l^{c} \sum_{j=1}^{n^c} S^c_{ij} \right]}_{d^c} \otimes \underbrace{\left[ l^{t} \sum_{j=1}^{n^t} S^t_{ij} \right]^\intercal}_{(d^t)^\intercal} } $}} \right] \; , \\
    &= \sum \left[ \vcenter{\hbox{$\displaystyle \frac{ \frac{1}{2}\left(\textbf{Y} - \tilde{\textbf{Y}}\right)^2}{ d^c \otimes (d^t)^\intercal } $}} \right] \; ,
\end{align}
in which respectively \(l^{c}\) and \(l^{t}\) denote the number of levels in hierarchies \(S^c\) and \(S^t\), and \(ij\) are the (row, column) indices of the elements of the \(S\)-matrices, and \(\otimes\) denotes the outer-product between the denominator components in the loss function. Note that \eqref{eq:whloss} shares similarities with the \textit{Weighted Root Mean Squared Error} from the M5 competition \cite{makridakis_m5_2022}.

We can derive the gradient and hessian of \eqref{eq:whloss} with respect to the bottom-level forecasts \(\hat{\textbf{Y}}\): 
\begin{align} 
  \frac{\partial L}{\partial \tilde{\textbf{Y}}} &=  \left[ \frac{\tilde{\textbf{Y}} - \textbf{Y}}{ d^c \otimes (d^t)^\intercal } \right] \;, \label{eq:hfp_grad} \\
  \frac{\partial L}{\partial \hat{\textbf{Y}}} &= (S^c)^\intercal \left(\frac{\partial L}{\partial \tilde{\textbf{Y}}}\right) S^t \;, \label{eq:hfpbu_grad}  \\
  \frac{\partial^2 L}{\partial \hat{\textbf{Y}}^2} &= (S^c)^\intercal \left( \frac{1}{d^c \otimes (d^t)^\intercal} \right) S^t \;. \label{eq:hfpbu_hess}                                         
\end{align}

\paragraph{Analysis} The best possible forecast is achieved when the loss \eqref{eq:whloss} is minimized, or equivalently when the gradient \eqref{eq:hfp_grad} is zero:
\begin{align} 
  \frac{\partial L}{\partial \tilde{\textbf{Y}}} &=  \left[ \frac{\tilde{\textbf{Y}} - \textbf{Y}}{ d^c \otimes (d^t)^\intercal } \right] \;, \nonumber \\
                                                 &=  \left[ \frac{S^c \hat{\textbf{Y}} (S^t)^\intercal - S^c \bar{\textbf{Y}} (S^t)^\intercal}{ d^c \otimes (d^t)^\intercal } \right] \;, \nonumber \\
                                                 &=  \left[ \frac{S^c \left(\hat{\textbf{Y}} - \bar{\textbf{Y}}\right) (S^t)^\intercal}{ d^c \otimes (d^t)^\intercal } \right] \;, \nonumber
\end{align}
which becomes zero when \(\hat{\textbf{Y}} = \bar{\textbf{Y}}\). Thus, the best forecast model is found when each bottom-level forecast equals the ground truth. This is equivalent to the standard (i.e., non-hierarchical) squared error loss often used in forecasting problems. We argue that our hierarchical loss gradient can be seen as a \textit{smoothed} gradient compared to the standard squared error loss gradient (i.e. \(\hat{\textbf{Y}} - \bar{\textbf{Y}}\)). For example, consider the canonical case where we have two bottom-level time series (\(n^c=2\)) consisting of two timesteps (\(n^t=2\)). Furthermore, suppose we have a single cross-sectional aggregation (the sum of the two time series, thus \(m^{*,c} = 1\) and \(m^c = m^{*,c} + n^c = 3\)), and a single temporal aggregation (the sum of the two timesteps, thus \(m^{*,t} = 1\) and \(m^t = m^{*,t} + n^t = 3\)). Finally, there are two levels in our cross-sectional hierarchy and in our temporal hierarchy, thus \(l^c = 2\) and \(l^t = 2\). The standard squared error loss gradient for this problem is:
\begin{align} \label{eq:sqloss_grad}
  \begin{bmatrix}
    \frac{\partial L}{\partial \hat{\textbf{y}}^0_{0}} & \frac{\partial L}{\partial \hat{\textbf{y}}^0_{1}} \\
    \frac{\partial L}{\partial \hat{\textbf{y}}^1_{1}} & \frac{\partial L}{\partial \hat{\textbf{y}}^1_{1}}
    \end{bmatrix} 
    = 
\begin{bmatrix}
  e_0^0 & e_0^1 \\
  e_1^0 & e_1^1 \\
\end{bmatrix}  \;, 
\end{align}
in which \(e^i_j\) denotes the bottom-level error (\(\hat{y}^i_j - \bar{y}^i_j\)).
For our hierarchical loss, Eq.~\eqref{eq:hfp_tc} reads:
\begin{align}
  \tilde{\textbf{Y}} = \underbrace{
    \begin{bmatrix}
    1 &1 \\
    1 &0 \\
    0 &1
    \end{bmatrix}}_{S^c}
    \underbrace{    
    \begin{bmatrix}
      \hat{\textbf{y}}^0_{0} & \hat{\textbf{y}}^0_{1} \\
      \hat{\textbf{y}}^1_{0} & \hat{\textbf{y}}^1_{1} \\
    \end{bmatrix}}_{\hat{\textbf{Y}}}
    \underbrace{
    \begin{bmatrix}
      1 &1 &0\\
      1 &0 &1\\
      \end{bmatrix}}_{(S^t)^\intercal}     
    \;,
\end{align}
and the gradient of the loss with respect to the bottom level time series \eqref{eq:hfpbu_grad} reads (ref. \ref{app:hl_derivation} for the full derivation):
\begin{align*} 
  &\begin{bmatrix}
  \frac{\partial L}{\partial \hat{\textbf{y}}^0_{0}} & \frac{\partial L}{\partial \hat{\textbf{y}}^0_{1}} \\
  \frac{\partial L}{\partial \hat{\textbf{y}}^1_{1}} & \frac{\partial L}{\partial \hat{\textbf{y}}^1_{1}}
  \end{bmatrix} 
  = (S^c)^\intercal \left[ \frac{S^c \left(\hat{\textbf{Y}} - \bar{\textbf{Y}}\right) (S^t)^\intercal}{ d^c \otimes (d^t)^\intercal } \right] S^t \\
  = 
  &\begin{bmatrix}
    \frac{9}{16}e_0^0 + \frac{3}{16}e_1^0 + \frac{3}{16}e_0^1 + \frac{1}{16}e_1^1 & \frac{9}{16}e_0^1 + \frac{3}{16}e_0^1 + \frac{3}{16}e_1^1 + \frac{1}{16}e_1^0 \\
    \frac{9}{16}e_1^0 + \frac{3}{16}e_0^0 + \frac{3}{16}e_1^1 + \frac{1}{16}e_0^1 & \frac{9}{16}e_1^1 + \frac{3}{16}e_1^0 + \frac{3}{16}e_0^1 + \frac{1}{16}e_0^0 \\
  \end{bmatrix}  \;. \nonumber \\         
\end{align*}
When we compare this result to the standard squared error loss gradient \eqref{eq:sqloss_grad}, we find that we \textit{smooth} the bottom-level gradient by adding to it portions of the gradients of all cross-sectional and temporal aggregations the bottom-level series belongs to. This derivation also shows the motivation of adding the denominator parts \(d^c\) and \(d^t\) to the loss function \eqref{eq:whloss}: it is neccessary to scale the aggregation gradients by the number of elements in the aggregation, otherwise the magnitude of the gradient grows with the number of time series and the number of levels in the hierarchy, which is undesirable when trying to facilitate stable learning.
  
\paragraph{Sparsity} \(S^c\) and \(S^t\) are highly sparse. For example, \(S^c\) has only \(n^cl^c\) non-zero elements: the number of bottom-level time series multiplied by the number of aggregations in the hierarchy. Hence, the overall sparsity of \(S^c\) is given by \( 1 - \frac{n^cl^c}{m^cn^c} \). For the M5 dataset \cite{makridakis_m5_2021}, \(n^c = 3,049\), \(l^c = 12\), \(m^c=42,840\), corresponding to a sparsity of 99.97\%. This motivates the use of sparse linear algebra when computing Equations~\eqref{eq:whloss}-\eqref{eq:hfpbu_hess}.

\paragraph{Implementation} We implement the hierachical loss \eqref{eq:whloss}, the bottom-level gradient \eqref{eq:hfpbu_grad} and hessian \eqref{eq:hfpbu_hess} in Python using the sparse library from \textit{SciPy} \cite{virtanen_scipy_2020}. Our implementation, including the code to reproduce the experiments from Section~\ref{sec:experiments}, is available on GitHub.\footnote{[TAKEN OUT FOR ANONYMIZATION]}

\ssc{Maybe we could add some more details here?}

\section{Experiments}
  \label{sec:experiments}
  In this section we empirically verify the usefulness of our sparse hierarchical loss. First, we evaluate forecasting accuracy using a set of experiments on the public M5 dataset \cite{makridakis_m5_2021}. Then, we evaluate our sparse hierarchical loss in an offline experiment on a proprietary dataset from our e-commerce partner.

  \subsection{Public datasets} \label{subsec:publicdatasets}
  \paragraph{Task \& Dataset} Our task is to forecast product demand. We use the M5 dataset \cite{makridakis_m5_2021} for our offline, public dataset experiments. The M5 dataset contains product-level sales from Walmart for 3,049 products across 10 stores in the USA. Furthermore, the dataset contains 12 cross-sectional product aggregations (e.g. department, region), which allow us to test hierarchical forecasting performance. We preprocess the dataset resulting in a set of features as described in \ref{app:m5dataset}. We forecast 28 days into the future.
  
  \paragraph{Baseline models} For our baseline forecasting model, we primarily use LightGBM \cite{ke_lightgbm_2017}, trained to predict one-day ahead. We subsequently recursively generate predictions for 28 days. Tree-based models dominated the M5 forecasting competition due to their strong performance and ease of use \cite{makridakis_m5_2022,januschowski_forecasting_2022}. Moreover, our e-commerce partner's primary product forecasting is a LightGBM-based model, so we expect results from offline experiments on public datasets to transfer to our proprietary setting when using the same base forecasting model. We compare the performance of our LightGBM models against traditional statistical methods ARIMA \cite{box_distribution_1970}, ETS \cite{hyndman_forecasting_2008}, Theta \cite{assimakopoulos_theta_2000}, SeasonalNaive \cite{hyndman_forecasting_2021}, Naive \cite{hyndman_forecasting_2021} and Croston \cite{croston_forecasting_1972}. 
  We note that deep learning based approaches are becoming more prevalent in e-commerce \cite{kunz_deep_2023}, especially with the rise of the Transformer-architecture in forecasting models \cite{lim_temporal_2021,li_enhancing_2019}. We consider this for future work, and did not consider this for our study as (i) the cloud cost to operate these models is 10x higher for our e-commerce partner as compared to a tree-based model and (ii) none of the neural-network based methods are able to scale to the size of our e-commerce partner, as explained in Section~\ref{sec:scalingissuesneuralmethods}. 

  \paragraph{Experimental setup} To test our hierarchical sparse loss function against baseline forecasting systems, we consider the following scenarios:
  \begin{enumerate}
    \item \texttt{Bottom-up}. We train a single global model on only the bottom-level time series.
    \item \texttt{Separate aggregations}. We train separate models for every aggregation in the hierarchy, resulting in 12 models for the entire M5 dataset.
    \item \texttt{Global}. We train a single global model on all time series in the dataset, including all the aggregations.
  \end{enumerate}
  For the first scenario in our experiments (\texttt{Bottom-up}), we vary both the \textit{objective} (i.e. loss function that is optimized by LightGBM) and the \textit{evaluation metric} (i.e. the loss function that governs early-stopping during hyperparameter optimization). For the \textit{objective}, we consider the \textit{squared error loss} (SL), the \textit{Tweedie loss} (TL) and our \textit{sparse Hierarchical Loss} (HL). The Tweedie loss is a loss function that assumes that the time series follow a distribution somewhere in between a Poisson and a Gamma distribution, which is useful in zero-inflated settings such as retail demand forecasting. It is a loss function that was favored by contestants in the M5 forecasting competition \cite{januschowski_forecasting_2022}, and it is the loss also used in the primary forecasting system of our e-commerce partner.

  For the latter two scenarios, we will obtain non-coherent forecasts. Thus, these methods require a reconciliation post-processing step to reconcile the forecasts to the hierarchy. We employ the following cross-sectional reconciliation methods:
  \begin{itemize}
    \item \textbf{Base}. No reconciliation is performed.
    \item \textbf{OLS}. Ordinary Least Squares (OLS) \cite{hyndman_optimal_2011}, where \(W\) in Equation~\eqref{eq:minp} is the identity matrix.
    \item \textbf{WLS-struct}. and \textbf{WLS-var}. Weighted Least Squares (WLS) \cite{wickramasuriya_optimal_2019}, where \(W\) in Equation~\eqref{eq:minp} is a diagonal matrix containing respectively the sum of the rows of \(S\) (\textit{WLS-struct}) or the in-sample forecast errors (\textit{WLS-var}).
    \item \textbf{MinT-shrink}. \textit{Trace Minimization} \cite{wickramasuriya_optimal_2019}, where \(W\) in Equation~\eqref{eq:minp} is the shrunk covariance matrix of in-sample forecast errors. We also experimented with using the non-shrunk covariance matrix of the in-sample forecast errors (\textit{MinT-cov}), but this produced erroneous/high variance results, which we attribute to precisely the motivation to shrink the covariance matrix in \textit{MinT-shrink}: to reduce the variance when the amount of time series considered becomes very large.
    \item \textbf{ERM}. The \textit{Empirical Risk Minimization} (ERM) method \cite{bentaieb_regularized_2019}. Due to computational issues explained in Section~\ref{sec:scalingissuesreconmethods}, we were not able to apply the regularized ERM variants to our experiments, but only the unregularized variant.
  \end{itemize}
  
  We optimize the hyperparameters of each of the LightGBM models by Bayesian hyperparameter optimization using Optuna \cite{akiba_optuna_2019}. The settings for the hyperparameter optimization can be found in \ref{app:hyperparam}. Each model is trained for 10 different random seeds, and our results are based on the mean and standard deviation of those 10 rollouts.

  \paragraph{Evaluation} We evaluate our results for every aggregation in the hierarchy using the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) \cite{hyndman_forecasting_2021}. In the results section, we present the RMSE / MAE relative to the \texttt{Bottom-up} scenario using the squared-loss objective with the squared-loss metric. For absolute values and standard deviation of the results, see \ref{app:experiments}.

  \paragraph{Results - LightGBM as baseline model} For our first experiment, we only consider cross-sectional hierarchies (i.e. \(S^t = I_{n^t}\)). We present our results on relative RMSE using LightGBM as baseline model in Table~\ref{tab:allstores_rel_rmse} and conclude the following:
  \begin{itemize}
    \item The best method is the \texttt{Bottom-up}-scenario combined with our sparse hierarchical loss as objective, outperforming the baseline by 0-20\% across aggregations. This holds for both settings in which we use our sparse hierarchical loss.
    \item Even when we only use our sparse hierarchical loss as an evaluation metric during training whilst optimizing the standard squared loss (the SL/HL scenario), we already see a small improvement of $\pm$5\% across aggregations. 
    \item Even though the Tweedie loss improves over the baseline loss, our sparse hierarchical loss function still outperforms it by $\pm$5\% across aggregations.
    \item From the reconciliation methods, \textit{MinT-shrink} and \textit{WLS-var} perform best in the \texttt{Separate aggregations}-scenario, although the performance delta across aggregations is still $\pm$5-30\% as compared to the best (our) method.
  \end{itemize}
  For relative MAE, we present our results in Table~\ref{tab:allstores_rel_mae}. We find that overall, our sparse hierachical loss still performs best by $\pm$5\% compared to other loss functions and scenarios. However, the results are more nuanced: we find that \textit{MinT-shrink} in the \texttt{Separate aggregations}-scenario performs strong as well. In addition, we also find that the Tweedie loss (TL) performs relatively well. This finding corroborates the usefullness of the TL in intermittent demand settings, such as retail, where zero demand is often observed.

  We next compare our findings against the forecasting results when employing different baseline models in Table~\ref{tab:otherbaselinemodels}. For brevity, we only show the metrics for all time series combined (incl. aggregations). In addition, we only show a single reconciliation method for the other baseline models, as we found little difference in results when employing different reconciliation methods. We then find that on RMSE, our sparse hierachical loss outperforms the other baseline models by at least 50\%, and on MAE by at least 10\%. This verifies that on this dataset and with this type of problem, using a more complex model such as LightGBM greatly improves forecasting performance, as was also shown in the M5 forecasting competition \cite{makridakis_m5_2020}.

  \begin{table}[t]
    \caption{Forecasting results for all time series (incl. aggregations) on the M5 dataset, using different baseline models. We show absolute and relative RMSE and MAE. Lower is better, and bold indicates best method.}
    \label{tab:otherbaselinemodels}
    \begin{center}
    {\small\setlength{\tabcolsep}{2pt} 
    \begin{tabular}{l c cccc}
    \toprule 
     & &\multicolumn{2}{ c }{RMSE}   &\multicolumn{2}{ c }{MAE} \\
     \cmidrule(r){3-4} \cmidrule(r){5-6}
    Model &Reconciliation &Abs. &Rel. &Abs. &Rel. \\
    \midrule																	
    LightGBM (SL/SL)	&None	&\textit{22.39}	&\textit{2.20}	&\textit{1.00}	&\textit{1.00}	\\
    LightGBM (HL/HL)	&None	&\textbf{19.54}	&\textbf{2.10}	&\textbf{0.87}	&\textbf{0.95}	\\
    LightGBM (HL/SL)	&None	&\textbf{19.59}	&\textbf{2.10}	&\textbf{0.88}	&\textbf{0.95}	\\
    ARIMA	&MinT-shrink	&39.88	&2.43	&1.78	&1.10	\\
    ETS	&MinT-shrink	&36.48	&2.35	&1.63	&1.07	\\
    Theta	&MinT-shrink	&36.66	&2.39	&1.64	&1.08	\\
    Croston	&None	&39.40	&2.76	&1.76	&1.25	\\
    Naive	&None	&74.91	&3.95	&3.35	&1.80	\\
    Seasonal Naive	&None	&39.40	&2.76	&1.76	&1.25	\\        
    \bottomrule
    \end{tabular}}
    \end{center}
    \end{table}

    \begin{table}[t]
      \caption{Forecasting results for all time series (incl. aggregations) on the M5 dataset, ablating for the use of cross-sectional and temporal hierarchies. We show absolute and relative RMSE and MAE, with standard deviation in brackets. Lower is better, and bold indicates best method. Note that when not using cross-sectional nor temporal aggregations, the hierarchical loss is equal to the standard squared error loss.}
      \label{tab:ablation_hierarchies}
      \begin{center}
      {\small\setlength{\tabcolsep}{2pt} 
      \begin{tabular}{c c  cccc}
      \toprule 
      \multicolumn{2}{ c }{Hierarchies} &\multicolumn{2}{ c }{RMSE}   &\multicolumn{2}{ c }{MAE} \\
      \cmidrule(r){1-2}  \cmidrule(r){3-4} \cmidrule(r){5-6}
      Cross-sectional &Temporal &Abs. &Rel. &Abs. &Rel. \\
      \midrule																	
      No	&No	&\textit{22.39} (0.16)	&\textit{2.20} (0.01)	&\textit{1.00}	&\textit{1.00}	\\
      Yes	&No	&\textbf{19.54}	(0.38) &\textbf{2.10} (0.01)	&\textbf{0.87}	&\textbf{0.95}	\\
      Yes	&Yes	&29.81 (1.52)	&2.47	(0.04)  &1.33	&1.12	\\
      No	&Yes	&26.65 (0.32)	&2.36	(0.01) &1.19	&1.07	\\
      Random	&No	&25.62 (4.75)	&2.31	(0.15) &1.14	&1.05	\\
      \bottomrule
      \end{tabular}}
      \end{center}
      \end{table}

      \begin{table}[t]
        \caption{Forecasting results for all time series (incl. aggregations) on the M5 dataset, ablating for the use of cross-sectional and temporal hierarchies. We show relative RMSE for several forecasting day buckets of the forecast. Lower is better, and bold indicates best method.}
        \label{tab:ablation_hierarchies_timesteps}
        \begin{center}
        {\small\setlength{\tabcolsep}{2pt} 
        \begin{tabular}{c c  cccc}
        \toprule 
        \multicolumn{2}{ c }{Hierarchies} &\multicolumn{4}{ c }{Forecast day}  \\
        \cmidrule(r){1-2}  \cmidrule(r){3-6}
        Cross-sectional &Temporal &1-7 &8-14 &15-21 &22-28 \\
        \midrule																	
        No	&No	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	&\textit{1.00}	\\
        Yes	&No	&\textbf{0.98}	&\textbf{0.99}	&\textbf{0.81}	&\textbf{0.80}	\\
        Yes	&Yes	&1.75	&1.50	&0.96	&1.66	\\
        No	&Yes	&1.37	&1.24	&0.97	&1.41	\\
        Random	&No	&1.25	&1.16	&1.00	&1.32	\\        
        \bottomrule
        \end{tabular}}
        \end{center}
        \end{table}

  \begin{sidewaystable*}[t]
    \caption{Forecasting results for all stores on the M5 dataset, using LightGBM as baseline model. We report relative RMSE as compared to the baseline (shown in italic). Lower is better, and bold indicates best method for the aggregation, taking into account standard deviation of the best method across the 10 seeds. For absolute values and standard deviation of the results, see \ref{app:experiments}.}
    \label{tab:allstores_rel_rmse}
    \begin{center}
    {\small\setlength{\tabcolsep}{2pt} 
    \begin{tabular}{l c  cccccccccccccc}
    \toprule 
     &&&& &  &\multicolumn{3}{ c }{Store}   &\multicolumn{2}{ c }{Product} &\multicolumn{3}{ c }{State} \\
     \cmidrule(r){7-9} \cmidrule(r){10-11} \cmidrule(r){12-14}
    Scenario/Objective & Metric  & Reconciliation &Product	&Department	&Category &Department	&Category	&Total &Store	&State &Department &Category &Total	&Total	&All series \\
    \midrule																	
    \texttt{Bottom-up}																	\\
    \hspace{0.1cm} 	\textit{SL}	&\textit{SL}	&\textit{None}	&\textbf{\textit{1.00}}	&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}	\\
    \hspace{0.1cm} 	SL	&HL	&None	&\textbf{1.00}	&0.97	&0.95	&0.98	&0.97	&0.98	&0.99	&1.00	&0.98	&0.96	&0.99	&1.00	&0.98	\\
    \hspace{0.1cm} 	HL	&HL	&None	&\textbf{1.00}	&\textbf{0.88}	&\textbf{0.80}	&\textbf{0.93}	&\textbf{0.89}	&\textbf{0.93}	&\textbf{0.97}	&\textbf{0.99}	&\textbf{0.89}	&\textbf{0.84}	&\textbf{0.88}	&\textbf{0.87}	&\textbf{0.87}	\\
    \hspace{0.1cm} 	HL	&SL	&None	&\textbf{1.00}	&\textbf{0.88}	&\textbf{0.81}	&\textbf{0.94}	&\textbf{0.90}	&\textbf{0.94}	&\textbf{0.96}	&\textbf{0.98}	&\textbf{0.89}	&\textbf{0.84}	&\textbf{0.88}	&\textbf{0.87}	&\textbf{0.88}	\\
    \hspace{0.1cm} 	TL	&HL	&None	&\textbf{1.00}	&0.95	&0.96	&0.99	&1.00	&1.00	&0.99	&1.00	&0.97	&0.98	&0.98	&0.96	&0.97	\\
    \hspace{0.1cm} 	TL	&SL	&None	&\textbf{1.00}	&0.94	&0.93	&1.00	&1.00	&1.00	&0.99	&1.00	&0.97	&0.98	&0.98	&0.93	&0.96	\\
    \hspace{0.1cm} 	TL	&TL	&None	&1.17	&2.72	&2.81	&1.76	&1.83	&1.73	&1.52	&1.33	&2.15	&2.18	&2.08	&2.71	&2.38	\\
    \midrule																	
    \texttt{Sep. agg.}																	\\
    \hspace{0.1cm} 	SL	&SL	&Base	&\textbf{1.00}	&1.44	&1.29	&1.19	&1.14	&1.14	&1.01	&0.99	&1.23	&1.34	&1.27	&1.60	&1.35	\\
    \hspace{0.1cm} 	SL	&SL	&OLS	&\textbf{1.00}	&1.39	&1.41	&1.10	&1.06	&1.07	&1.00	&1.00	&1.20	&1.19	&1.23	&1.50	&1.30	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-struct	&\textbf{1.00}	&1.26	&1.37	&1.03	&1.05	&1.02	&0.99	&0.99	&1.11	&1.16	&1.16	&1.39	&1.23	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-var	&\textbf{1.00}	&1.12	&1.23	&0.99	&1.02	&0.99	&0.99	&0.99	&1.03	&1.09	&1.07	&1.22	&1.12	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&\textbf{1.00}	&1.15	&1.27	&0.97	&0.99	&0.97	&1.00	&1.00	&1.03	&1.09	&1.09	&1.30	&1.15	\\
    \hspace{0.1cm} 	SL	&SL	&ERM	&1.22	&1.25	&1.29	&1.07	&1.03	&1.07	&1.17	&1.22	&1.17	&1.14	&1.22	&1.49	&1.26	\\
    \midrule																	
    \texttt{Global}																	\\
    \hspace{0.1cm} 	SL	&SL	&Base	&1.02	&1.33	&1.45	&1.09	&1.10	&1.10	&1.03	&1.03	&1.25	&1.27	&1.81	&1.57	&1.46	\\
    \hspace{0.1cm} 	SL	&SL	&OLS	&1.01	&1.32	&1.39	&1.07	&1.09	&1.16	&1.02	&1.02	&1.20	&1.25	&1.38	&1.49	&1.34	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-struct	&1.01	&1.38	&1.54	&1.08	&1.13	&1.11	&1.03	&1.02	&1.19	&1.28	&1.27	&1.55	&1.36	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-var	&1.01	&1.51	&1.70	&1.18	&1.27	&1.22	&1.03	&1.02	&1.31	&1.43	&1.38	&1.66	&1.48	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&1.03	&1.26	&1.41	&1.05	&1.10	&1.15	&1.06	&1.05	&1.11	&1.17	&1.24	&1.54	&1.30	\\
    \hspace{0.1cm} 	SL	&SL	&ERM	&1.21	&1.59	&1.69	&1.26	&1.28	&1.34	&1.20	&1.23	&1.45	&1.49	&1.61	&1.80	&1.59	\\      
    \bottomrule
    \end{tabular}}
    \end{center}
    \end{sidewaystable*}

    \begin{sidewaystable*}[t]
      \caption{Forecasting results for all stores on the M5 dataset, using LightGBM as baseline model. We report relative MAE as compared to the baseline (shown in italic). Lower is better, and bold indicates best method for the aggregation, taking into account standard deviation of the best method across the 10 seeds. For absolute values and standard deviation of the results, see \ref{app:experiments}.}
      \label{tab:allstores_rel_mae}
      \begin{center}
      {\small\setlength{\tabcolsep}{2pt} 
      \begin{tabular}{l c  cccccccccccccc}
      \toprule 
       &&&& &  &\multicolumn{3}{ c }{Store}   &\multicolumn{2}{ c }{Product} &\multicolumn{3}{ c }{State} \\
       \cmidrule(r){7-9} \cmidrule(r){10-11} \cmidrule(r){12-14}
      Scenario/Objective & Metric  & Reconciliation &Product	&Department	&Category &Department	&Category	&Total &Store	&State &Department &Category &Total	&Total	&All series \\
      \midrule																	
      \texttt{Bottom-up}																	\\
      \hspace{0.1cm} 	\textit{SL}	&\textit{SL}	&\textit{None}	&\textit{1.00}	&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}&\textit{1.00}	\\
      \hspace{0.1cm} 	SL	&HL	&None	&1.00	&0.98	&0.96	&0.98	&0.98	&0.99	&1	&1	&0.97	&0.97	&1	&1.02	&0.99	\\
      \hspace{0.1cm} 	HL	&HL	&None	&0.99	&\textbf{0.81}	&\textbf{0.78}	&\textbf{0.90}	&\textbf{0.89}	&0.94	&0.98	&0.99	&\textbf{0.85}	&\textbf{0.83}	&\textbf{0.89}	&\textbf{0.88}	&\textbf{0.95}	\\
      \hspace{0.1cm} 	HL	&SL	&None	&0.99	&\textbf{0.81}	&\textbf{0.77}	&\textbf{0.90}	&\textbf{0.90}	&0.94	&0.98	&0.99	&\textbf{0.85}	&\textbf{0.83}	&\textbf{0.89}	&\textbf{0.87}	&\textbf{0.95}	\\
      \hspace{0.1cm} 	TL	&HL	&None	&0.98	&\textbf{0.83}	&0.82	&0.93	&0.94	&0.96	&0.97	&0.98	&0.88	&0.89	&0.92	&\textbf{0.88}	&\textbf{0.95}	\\
      \hspace{0.1cm} 	TL	&SL	&None	&0.99	&0.85	&0.84	&0.95	&0.95	&0.98	&0.98	&0.99	&0.9	&0.91	&0.96	&\textbf{0.90}	&\textbf{0.96}	\\
      \hspace{0.1cm} 	TL	&TL	&None	&1.02	&2.06	&2.39	&1.47	&1.61	&1.65	&1.14	&1.07	&1.69	&1.88	&1.97	&2.86	&1.29	\\
      \midrule																	
      \texttt{Sep. agg.}																	\\
      \hspace{0.1cm} 	SL	&SL	&Base	&1.00	&1.11	&1.06	&1.02	&1.1	&1.08	&0.96	&0.97	&1.03	&1.15	&1.2	&1.55	&1.02	\\
      \hspace{0.1cm} 	SL	&SL	&OLS	&\textbf{0.96}	&1.08	&1.16	&1.01	&0.99	&1.00	&0.96	&0.97	&1	&1.01	&1.12	&1.49	&0.99	\\
      \hspace{0.1cm} 	SL	&SL	&WLS-struct	&0.97	&0.99	&1.13	&0.92	&0.95	&0.95	&\textbf{0.96}	&\textbf{0.97}	&0.93	&0.98	&1.04	&1.42	&0.98	\\
      \hspace{0.1cm} 	SL	&SL	&WLS-var	&0.98	&0.91	&1.00	&0.91	&0.92	&\textbf{0.92}	&0.96	&0.97	&0.90	&0.93	&0.95	&1.16	&\textbf{0.96}	\\
      \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&0.97	&0.93	&1.05	&\textbf{0.90}	&\textbf{0.90}	&\textbf{0.91}	&0.96	&0.97	&0.89	&0.92	&0.98	&1.30	&\textbf{0.96}	\\
      \hspace{0.1cm} 	SL	&SL	&ERM	&1.18	&1.17	&1.21	&1.09	&1.06	&1.07	&1.19	&1.22	&1.11	&1.12	&1.19	&1.57	&1.18	\\
      \midrule																	
      \texttt{Global}																	\\
      \hspace{0.1cm} 	SL	&SL	&Base	&1.04	&1.04	&1.17	&1.00	&1.02	&1.05	&0.99	&0.99	&1.04	&1.09	&1.62	&1.58	&1.05	\\
      \hspace{0.1cm} 	SL	&SL	&OLS	&0.98	&1.09	&1.17	&1.05	&1.07	&1.13	&0.99	&1.00	&1.09	&1.13	&1.27	&1.5	&1.03	\\
      \hspace{0.1cm} 	SL	&SL	&WLS-struct	&0.99	&1.09	&1.26	&0.98	&1.01	&1.04	&1.00	&0.99	&1	&1.07	&1.15	&1.57	&1.02	\\
      \hspace{0.1cm} 	SL	&SL	&WLS-var	&0.99	&1.24	&1.40	&1.09	&1.13	&1.13	&1.01	&1.00	&1.14	&1.21	&1.25	&1.69	&1.06	\\
      \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&0.99	&1.13	&1.24	&1.06	&1.08	&1.15	&1.02	&1.01	&1.05	&1.07	&1.15	&1.6	&1.04	\\
      \hspace{0.1cm} 	SL	&SL	&ERM	&1.17	&1.49	&1.62	&1.3	&1.34	&1.37	&1.19	&1.2	&1.39	&1.49	&1.58	&1.96	&1.27	\\
      \bottomrule
      \end{tabular}}
      \end{center}
      \end{sidewaystable*}
          
  \paragraph{Analysis: impact of hierarchy} We investigate the impact of the choice of hierarchy. 
  
  \textit{Temporal hierarchies} As we noted before, we only used cross-sectional aggregations in our first experiments. We now also include temporal aggregations by aggregating our bottom-level time series across years, weeks and months. We ablate for every setting and show the results in Table~\ref{tab:ablation_hierarchies}. Interestingly, we find that using temporal hierarchies jointly with cross-sectional hierarchies reduces forecasting performance by $\pm$35\% (RMSE) and $\pm$17\% (MAE). This setting is even worse then only using temporal hierarchies, which performs worse as compared to using only cross-sectional hierarchies by $\pm$26\% (RMSE) and $\pm$12\% (MAE). We further analyze these results by studying the RMSE across the forecast days in Table~\ref{tab:ablation_hierarchies_timesteps}. As noted before, we forecast 28 days ahead, and each forecast is created by recursively applying the one-step ahead LightGBM model. We find that as we forecast further into the future, the setting with only using cross-sectional aggregations starts to perform better by up to 20\% as compared to the baseline where we do not use any aggregations. Again, the setting where we employ temporal hierarchies too shows relatively bad performance across all forecast day buckets.  
  
  \textit{Random hierarchies} In hierarchical forecasting problems, the aggregation matrices \(S^c\) and \(S^t\) are commonly fixed a priori and considered constant during training and prediction. As we are performing the reconciliation in an end-to-end fashion during training, we can modify these matrices at every iteration. This allows us to understand the robustness of our solution to possible misspecification of the hierarchy, and more generally, to what extent the choice of the hierarchy has an effect on forecasting performance. We perform an experiment by randomly sampling an \(S^c\)-matrix at every iteration of the LightGBM training process. At every iteration, we sample uniformly at random (i) a number of levels for the cross-sectional hierarchy and (ii) a number of maximum categories for the level and construct a random \(S^c\)-matrix to be used in the gradient \eqref{eq:hfpbu_grad} and hessian \eqref{eq:hfpbu_hess}. We validate and test on the `true' \(S^c\)-matrix. We present the results in Table~\ref{tab:ablation_hierarchies} and Table~\ref{tab:ablation_hierarchies_timesteps}, under `Random'. We find that performance deteriorates by about 30\% as compared to the baseline, and we also find that our results in this experiment show higher variance compared to the baseline methods. Thus, misspecification of the hierarchy can severely deteriorate forecasting performance.
  
  \paragraph{Analysis: time complexity} \label{par:timecomplexity} We investigate the computational time complexity required to perform training and prediction for each scenario and present the results in Table~\ref{tab:complexity}. The training and prediction time complexity is indicated by how respectively the training time and prediction time scales with respect to the default LightGBM training and prediction time complexity. We first investigate the case where we only consider cross-sectional hierarchies. This case is indicated by `HL' in Table~\ref{tab:complexity}. First, we note that adding our hierarchical loss objective adds a component to the time complexity that scales with \(n^{c,3}\), as we need to compute \eqref{eq:hfp_grad}. However, our sparse implementation of the hierarchical loss reduces this component from \(n^{c,3}\) to \(n^{c,2}l^c\), effectively reducing the scaling from cubic to quadratic in the number of bottom-level time series, as \(l^c\) is generally small. In the reconciliation scenarios, we always need to compute a matrix inversion to solve Eq.~ \eqref{eq:p1} that scales cubically with the number of cross-sectional aggregations \(m^{c,*}\) or with the total number of time series \(m^c\). The first is not problematic as generally \(m^{c,*} \ll n^c\) in large-scale settings, but methods with this time complexity consequently trade in performance, as we observed in Table~\ref{tab:allstores_rel_rmse}. To empirically verify the differences in asymptotic time complexity, we recorded the training and prediction time for each scenario. We show timings for training and prediction for a single store of the M5 dataset (4M training samples) and for the entire M5 dataset (52M training samples), to provide an indication of scaling when the problem size increases by an order of magnitude. First, we note that using our sparse implementation of the HL reduces training time by a factor of \(3\times\) when training for all stores. Second, our sparse HL has a prediction time similar to the baseline (SL). 
  Next, we find that the training time of our sparse hierarchical loss is two orders of magnitude faster than reconciliation methods in the \texttt{Separate aggregations}-scenario. This is mainly due to the many individual models that need to be trained in this scenario and thus shows a clear benefit of having just a single model. We observe an order of magnitude difference in prediction time when comparing the sparse hierarchical loss to the \texttt{Separate aggregations}-scenario when predictinf all stores. Again, this shows a clear benefit of having just a single model for this forecasting task. 
  For the \texttt{Global}-scenario, we see that reconciliation methods require a smaller training time when training for all stores (about twice less), however that scenario also did not give strong forecasting performance as we established in Table~\ref{tab:allstores_rel_rmse}. Also, the prediction time using our sparse HL is an order of magnitude lower. As ML cost in production systems mainly consists of prediction costs, having a lower prediction time is beneficial.\footnote{For example, Google designed its first TPU for inference: https://techcrunch.com/2017/05/17/google-announces-second-generation-of-tensor-processing-unit-chips.}
  Finally, we also show the time complexity of using both cross-sectional and temporal hierarchies jointly, as indicated by `\(HL+\)' in Table~\ref{tab:complexity}. Adding temporal hierarchies adds another matrix multiplication that scales with the number of timesteps to the complexity. In our experiments, we find that adding temporal hierarchies results in a twice higher training time when training for all stores and a 50\% higher prediction time when predicting for all stores. We view it as a potential future work to investigate how to perform this end-to-end learning of both cross-sectional and temporal hierarchies even more efficiently. 

  To conclude, we showed that our sparse HL incurs some additional training overhead but no additional prediction overhead as compared to the base case SL, whereas it doesn't require the additional reconciliation step that reconciliation methods require.  

  \begin{table*}[t]
    \caption{Computational time complexity and observed timings in seconds for all scenarios. The complexity is indicated by how respectively the training time and prediction time scales with respect to the default LightGBM training/prediction time \(T\), where \(n^t\) denotes the number of timesteps per time series, \(n^c\) denotes the number of bottom-level time series in the hierarchy, \(n^c_l\) the number of time series in each level in the hierarchy and \(l^c\) the number of levels in the cross-sectional hierarchy, \(m^c\) (\(m^t\)) the total number of cross-sectional (temporal) aggregations, and \(m^{*} = m - n\).}
    \label{tab:complexity}
    \begin{center}
    {\small\setlength{\tabcolsep}{1pt} 
    \begin{tabular}{l c ccccccc}
    \toprule 
     &&  &\multicolumn{2}{ c }{Complexity}   &\multicolumn{2}{ c }{Training time (s)} &\multicolumn{2}{ c }{Prediction time (s)}  \\
     \cmidrule(r){4-5} \cmidrule(r){6-7} \cmidrule(r){8-9} 
    Scenario/Objective &Metric  & Reconciliation &Training	&Prediction	&1 store &All stores &1 store &All stores \\
    \midrule						
    \texttt{Bottom-up}						\\
    \hspace{0.1cm} SL &SL &None & $O(T(n^tn^c))$ &$O(T(n^tn^c) + n^tn^{c,3})$		&8	&173	&1.1	&11	\\
    \hspace{0.1cm} SL &SL &None & $O(T(n^tn^c))$ &$O(T(n^tn^c) + n^tn^{c,2}l^c)$		&10	&174	&0.1	&10	\\
    \hspace{0.1cm} HL (dense) &HL (dense) &None & $O(T(n^tn^c + n^tn^{c,3}))$ &$O(T(n^tn^c) + n^tn^{c,3})$		&14	&1,185	&1.1	&10	\\
    \hspace{0.1cm} HL (sparse) &HL (sparse) &None & $O(T(n^tn^c + n^tn^{c,2}l^c))$ &$O(T(n^tn^c) + n^tn^{c,2}l^c)$		&12	&318	&0.1	&11	\\
    \hspace{0.1cm} HL+ (sparse) &HL+ (sparse) &None & $O(T(n^tn^c + n^tn^{c,2}l^c n^cn^{t,2}l^t))$ &$O(T(n^tn^c) + n^tn^{c,2}l^c n^cn^{t,2}l^t))$		&	&723	&	&15	\\
    \midrule 						
    \texttt{Sep. agg.} 						\\
    \hspace{0.1cm} SL &SL &Base & \multirow{6}{*}{$O(l \cdot T(n^c_l n^t_l))$} &$O(l^c \cdot T(n^c_ln^t_l))$ 		&\multirow{6}{*}{11}	&\multirow{6}{*}{36,018}	&4.4	&103	\\
    \hspace{0.1cm} SL &SL &OLS &  &$O(l^c \cdot T(n^t_l n_l^c) + m^{c*,3})$  	&		&	&4.5	&149	\\
    \hspace{0.1cm} SL &SL &WLS-struct &  &$O(l^c \cdot T(n^t_ln_l^c) + m^{c*,3})$ &			&	&4.5	&151	\\
    \hspace{0.1cm} SL &SL &WLS-var &  &$O(l^c \cdot T(n^t_ln_l^c) + m^{c*,3})$		&	&	&4.5	&151	\\
    \hspace{0.1cm} SL &SL &MinT-shrink & &$O(l^c \cdot T(n^t_ln_l^c) + m^{c,3})$ &			&	&5.8	&305	\\
    \hspace{0.1cm} SL &SL &ERM &  &$O(l^c \cdot T(n^t_ln_l^c) + m^{c,3})$		&	&	&6.0	&239	\\
    \midrule 						
    \texttt{Global}  						\\
    \hspace{0.1cm} SL &SL &Base & \multirow{6}{*}{$O(T(m^tm^c))$} &$O(T(m^tm^c))$ 		&\multirow{6}{*}{4}	&\multirow{6}{*}{173}	&2.4	&71	\\
    \hspace{0.1cm} SL &SL &OLS &  &$O(T(m^tm^c) + m^{c*,3})$  		&	&	&2.5	&118	\\
    \hspace{0.1cm} SL &SL &WLS-struct &  &$O(T(m^tm^c) + m^{c*,3})$ 		&	&	&2.5	&120	\\
    \hspace{0.1cm} SL &SL &WLS-var &  &$O(T(m^tm^c) + m^{c*,3})$ 		&	&	&2.5	&120	\\
    \hspace{0.1cm} SL &SL &MinT-shrink &  &$O(T(m^tm^c) + m^{c,3})$		&	&	&4.2	&274	\\
    \hspace{0.1cm} SL &SL &ERM &  &$O(T(m^tm^c) + m^{c,3})$		&	&	&4.0	&207	\\
    \bottomrule				        
    \end{tabular}}
    \end{center}
    \end{table*}
  
  \subsection{Proprietary datasets} \label{subsec:proprietarydatasets}
  At our e-commerce partner bol.com, a LightGBM-based forecasting model is used as the primary product forecasting model. The model is used to forecast weekly product demand for 12 weeks. Every day, 12 separate models are trained, each tasked to forecast demand for a single week for every product. The model is used to forecast the majority of the products on sale at any moment, which are approximately 5 million unique items. We investigate the use of our sparse hierarchical loss function as a drop-in replacement for the existing Tweedie Loss that is used within the company. The offline dataset consists of 36M training samples from the period January 2017 to the end of June 2021. We test on 55M samples from the period July 2021 to January 2022. The baseline model for every weekly forecast model is a LightGBM model with a \textit{Tweedie Loss} (TL). The baseline model uses 19 proprietary features. 
  
  \paragraph{Experimental setup} We replace the TL with our HL and investigate forecasting performance on the test set. For the HL, we use the proprietary aggregations \textit{product\_group} and \textit{seasonality\_group}, each containing respectively $\pm$70 and $\pm$6,000 unique values. We have \(n^c = \pm5\)M bottom-level time series and \(m^{c,*} = \pm6,070\) aggregated time series across \(l^c = 4\) levels: \textit{product} (bottom-level), \textit{product\_group}, \textit{seasonality\_group} and \textit{total}.

  \paragraph{Results} On average, we find our sparse HL outperforms the existing TL model by about 1-2\% on RMSE and \(\pm\)10\% on MAE. We further investigate the performance by investigating how the RMSE and MAE varies across the 12 forecasting horizons and sales buckets and present the results in Figure~\ref{fig:bol_results}. We find that our sparse HL performs best on both RMSE and MAE on the lower sales buckets (up to 100 products sold per week), where it outperforms the TL averaged over all the forecasting horizons. The TL is clearly better for higher sales buckets, commonly outperforming the HL and SL by up to 5\%. Next, we investigate forecasting performance across the cross-sectional hierarchies that we defined. We show the results in Figure~\ref{fig:bol_level_results}. We find that for most forecasting horizons, the HL and SL outperform the TL, with an average outperformance of the HL over the TL of \(\pm\)10\% at the  product level, \(\pm\)5\% at product group level and \(\pm\)4-7\% at seasonality group level. Hence, we are able to confirm some of the results we found in the M5 experiment, although the baseline SL performed quite strong in this experiment as well. We believe this is due to the M5 experiment having much more hierarchical levels (12 as compared to the 4 we used for our private dataset experiment), since the HL is equal to the SL in case of no hierarchies, and with fewer hierarchies the HL thus becomes closer to the SL. Hence, we believe our HL is most useful in settings with both many timeseries as well as many hierarchies.
  To conclude, we find that this experiment demonstrates the usefulness of our sparse HL applied to a production forecasting system at a major e-commerce platform.

  \begin{figure*}[t] 
    \subfigure[RMSE]{\includegraphics[width=\columnwidth]{assets/bol_rmse.png}} \quad
    \subfigure[MAE]{\includegraphics[width=\columnwidth]{assets/bol_mae.png}}
    \caption{Forecasting results for the primary product forecasting model at our e-commerce partner bol.com. We show RMSE and MAE by sales bucket relative to the Tweedie Loss baseline for each forecasting horizon (week).}
    \label{fig:bol_results}
  \end{figure*}

  \begin{figure}[t] 
    \includegraphics[width=\columnwidth, keepaspectratio]{assets/bol_levels.png}
    \caption{Forecasting results for the primary product forecasting model at our e-commerce partner bol.com. We show RMSE and MAE by aggregation level relative to the Tweedie Loss baseline for each forecasting horizon (week).}
    \label{fig:bol_level_results}
  \end{figure}


\section{Conclusion} \label{sec:conclusion}
  We introduced a sparse hierarchical loss function to perform hierarchical forecasting in large-scale settings. We demonstrated that we are able to outperform existing hierarchical forecasting methods both in terms of performance as measured by RMSE and MAE by up to 10\% as well as in terms of computational time required to perform the end-to-end hierarchical forecasting in large-scale settings, reducing prediction time as compared to the best hierarchical forecasting reconciliation method by an order of magnitude. We empirically verified our sparse hierarchical loss in an offline test for bol.com, where we confirmed the results from our offline test on the public M5 dataset. 

  In addition to our main contributions, one of our main learnings has been that we could not find a benefit of having multiple models for separate aggregations in the hierarchy, as the bottom-up scenario we employed consistently outperformed other scenarios. Secondly, we did not find a benefit of training a model whilst adhering to both cross-sectional and temporal hierarchies jointly. 

  For future work, we aim to extend our work to the setting of probabilistic forecasting by combining our sparse hierarchical loss with existing probabilistic forecasting frameworks from, e.g. \cite{sprangers_probabilistic_2021, hasson_probabilistic_2021, stankeviciute_conformal_2021}. In addition, we seek to further investigate solutions for efficiently combining cross-sectional and temporal hierarchies.

\section*{Acknowledgments}
  This research was (partially) funded by the Hybrid Intelligence Center, a 10-year program funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research.\footnote{\url{https://www.hybrid-intelligence-centre.nl/}}

  All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
  
  % We thank the reviewers for their constructive feedback and help on improving our work. 


\bibliographystyle{elsarticle-num-names} 
\bibliography{lib}

\clearpage

\appendix
\onecolumn

\section{Derivation of gradient} \label{app:hl_derivation}
We have the following hierarchical forecasting problem:
\begin{align*}
  \tilde{\textbf{Y}} = \underbrace{
    \begin{bmatrix}
    1 &1 \\
    1 &0 \\
    0 &1
    \end{bmatrix}}_{S^c}
    \underbrace{    
    \begin{bmatrix}
      \hat{\textbf{y}}^0_{0} & \hat{\textbf{y}}^0_{1} \\
      \hat{\textbf{y}}^1_{0} & \hat{\textbf{y}}^1_{1} \\
    \end{bmatrix}}_{\hat{\textbf{Y}}}
    \underbrace{
    \begin{bmatrix}
      1 &1 &0\\
      1 &0 &1\\
      \end{bmatrix}}_{(S^t)^\intercal}     
    \;,
\end{align*}
which results in the following gradient and hessian:
\begin{align*} 
  \begin{bmatrix}
  \frac{\partial L}{\partial \hat{\textbf{y}}^0_{0}} & \frac{\partial L}{\partial \hat{\textbf{y}}^0_{1}} \\
  \frac{\partial L}{\partial \hat{\textbf{y}}^1_{1}} & \frac{\partial L}{\partial \hat{\textbf{y}}^1_{1}}
  \end{bmatrix} 
  &= (S^c)^\intercal \left[ \frac{S^c \left(\hat{\textbf{Y}} - \bar{\textbf{Y}}\right) (S^t)^\intercal}{ d^c \otimes (d^t)^\intercal } \right] S^t \\
  &= (S^c)^\intercal \left[ \vcenter{\hbox{$\displaystyle \frac{S^c \left(\hat{\textbf{Y}} - \bar{\textbf{Y}}\right) (S^t)^\intercal}{ \underbrace{\left[ l^{c} \sum_{j=1}^{n^c} S^c_{ij} \right]}_{d^c} \otimes \underbrace{\left[ l^{t} \sum_{j=1}^{n^t} S^t_{ij} \right]^\intercal}_{(d^t)^\intercal} }  $}} \right] S^t \\
  &= \begin{bmatrix}
    1 &1 &0\\
    1 &0 &1\\
    \end{bmatrix}     
    \left[ \frac{
    \begin{bmatrix}
      1 &1 \\
      1 &0 \\
      0 &1
    \end{bmatrix}
    \left(  
    \begin{bmatrix}
      \hat{\textbf{y}}^0_{0} &\hat{\textbf{y}}^0_{1} \\
      \hat{\textbf{y}}^1_{0} &\hat{\textbf{y}}^1_{1}
    \end{bmatrix}  
    - 
    \begin{bmatrix}
      \bar{\textbf{y}}^0_{0} &\bar{\textbf{y}}^0_{1} \\
      \bar{\textbf{y}}^1_{0} &\bar{\textbf{y}}^1_{1}
    \end{bmatrix}  
    \right)
    \begin{bmatrix}
      1 &1 &0\\
      1 &0 &1\\
    \end{bmatrix} 
    }{\left( 2 
    \begin{bmatrix}
      2 &1 &1
    \end{bmatrix}^\intercal 
    \right) 
    \otimes
    \left( 2 
    \begin{bmatrix}
      2 &1 &1
    \end{bmatrix}
    \right)
    } \right]
    \begin{bmatrix}
      1 &1 \\
      1 &0 \\
      0 &1
    \end{bmatrix}  \;, \nonumber \\
    &= \begin{bmatrix}
      1 &1 &0\\
      1 &0 &1\\
      \end{bmatrix}     
      \left[ \frac{
      \begin{bmatrix}
        1 &1 \\
        1 &0 \\
        0 &1
      \end{bmatrix}
      \left(  
        \begin{bmatrix}
          \overbrace{\hat{\textbf{y}}^0_{0} - \bar{\textbf{y}}^0_{0}}^{e^0_0} &\overbrace{\hat{\textbf{y}}^0_{1} - \bar{\textbf{y}}^0_{1}}^{e^0_1} \\
          \underbrace{\hat{\textbf{y}}^1_{0} - \bar{\textbf{y}}^1_{0}}_{e^1_0} &\underbrace{\hat{\textbf{y}}^1_{1} - \bar{\textbf{y}}^1_{1}}_{e^1_1}
        \end{bmatrix}  
      \right)
      \begin{bmatrix}
        1 &1 &0\\
        1 &0 &1\\
      \end{bmatrix} 
      }{
      \begin{bmatrix}
        16 &8 &8 \\
        8 &4 &4 \\
        8 &4 &4 \\
      \end{bmatrix}
      } \right]
      \begin{bmatrix}
        1 &1 \\
        1 &0 \\
        0 &1
      \end{bmatrix}  \;, \nonumber \\
      &= \begin{bmatrix}
        1 &1 &0\\
        1 &0 &1\\
        \end{bmatrix}     
        \begin{bmatrix}
          \frac{1}{16} (e^0_0 + e^1_0 + e^0_1 + e^1_1) &\frac{1}{8} (e^0_0 + e^1_0) &\frac{1}{8} (e^0_1 + e^1_1) \\
          \frac{1}{8} (e^0_0 + e^0_1)  &\frac{1}{4} e^0_0 & \frac{1}{4} e^0_1 \\
          \frac{1}{8} (e^1_0 + e^1_1)  &\frac{1}{4} e^1_0 & \frac{1}{4} e^1_1 \\
        \end{bmatrix}
        \begin{bmatrix}
          1 &1 \\
          1 &0 \\
          0 &1
        \end{bmatrix}  \;, \nonumber \\      
    &= 
      \begin{bmatrix}
        \frac{1}{16} (e^0_0 + e^1_0 + e^0_1 + e^1_1) + \frac{1}{8} (e^0_0 + e^1_0) + \frac{1}{8} (e^0_0 + e^0_1) + \frac{1}{4} e^0_0 & \frac{1}{16} (e^0_0 + e^1_0 + e^0_1 + e^1_1) + \frac{1}{8} (e^0_1 + e^1_1) + \frac{1}{8} (e^0_0 + e^0_1) + \frac{1}{4} e^0_1 \\
        \frac{1}{16} (e^0_0 + e^1_0 + e^0_1 + e^1_1) + \frac{1}{8} (e^0_0 + e^1_0) + \frac{1}{8} (e^1_0 + e^1_1) + \frac{1}{4} e^1_0 & \frac{1}{16} (e^0_0 + e^1_0 + e^0_1 + e^1_1) + \frac{1}{8} (e^0_1 + e^1_1) + \frac{1}{8} (e^1_0 + e^1_1) + \frac{1}{4} e^1_1 \\
      \end{bmatrix}  \;, \nonumber \\   
      &= 
      \begin{bmatrix}
        \frac{9}{16}e_0^0 + \frac{3}{16}e_1^0 + \frac{3}{16}e_0^1 + \frac{1}{16}e_1^1 & \frac{9}{16}e_0^1 + \frac{3}{16}e_0^1 + \frac{3}{16}e_1^1 + \frac{1}{16}e_1^0 \\
        \frac{9}{16}e_1^0 + \frac{3}{16}e_0^0 + \frac{3}{16}e_1^1 + \frac{1}{16}e_0^1 & \frac{9}{16}e_1^1 + \frac{3}{16}e_1^0 + \frac{3}{16}e_0^1 + \frac{1}{16}e_0^0 \\
      \end{bmatrix}  \;. \nonumber \\     
 \end{align*}  

\section{M5 Dataset} \label{app:m5dataset}
For each of the scenarios of our experiments in Section~\ref{sec:experiments}, we construct a set of features for the LightGBM model as given in Table~\ref{tab:features}. To facilitate the most `fair' comparison across methods, each model has the same features, and for the time series aggregations in the hierarchy we construct the features taken over the aggregation. 
\begin{table}
  \caption{Features used for the M5 dataset in our experiments.}
  \label{tab:features}
  \begin{center}
  {\small\setlength{\tabcolsep}{1pt} 
  \begin{tabular}{l l }
  \toprule 
  Feature & Description \\
  \midrule
  \texttt{Aggregation} & Aggregation level in the hierarchy \\
  \texttt{Value} & Identifier of time series of this aggregation \\   
  \texttt{sales\_lag1-7} & Lagged sales (target) (7 features) \\
  \texttt{sales\_lag28} & Sales 28 days ago \\ 
  \texttt{sales\_lag56} & Sales 56 days ago \\ 
  \texttt{sales\_lag364} & Sales last year \\ 
  \texttt{sales\_lag1\_mavg7} & Moving average of sales last 7 days \\
  \texttt{sales\_lag1\_mavg28} & Moving average of sales last 28 days \\ 
  \texttt{sales\_lag1\_mavg56} & Moving average of sales last 56 days\\ 
  \texttt{dayofweek} & Day of the week \\ 
  \texttt{dayofmonth} & Day of the month \\
  \texttt{weekofyear} & Week of year \\ 
  \texttt{monthofyear} & Month of year \\ 
  \texttt{sell\_price\_avg} & Sell price (average if aggregation)\\ 
  \texttt{sell\_price\_change} & Day-to-day change in sell price\\
  \texttt{weeks\_on\_sale\_avg} & Weeks on sale \\ 
  \texttt{snap\_CA} & State indicator for California \\ 
  \texttt{snap\_TX} & State indicator for Texas \\ 
  \texttt{snap\_WI} & State indicator for Wyoming \\
  \texttt{event\_type\_1\_enc} & Encoded events \\ 
  \texttt{event\_type\_2\_enc} & Encoded events \\
  \bottomrule
  \end{tabular}}
  \end{center}
\end{table}

\section{M5 model training \& optimization} \label{app:hyperparam}
We optimize the hyperparameters of our LightGBM models using Optuna \cite{akiba_optuna_2019}, using the settings found in Table~\ref{tab:hyperparams}. The validation is performed on a rolling-forward basis for 3 validation sets, where we use three years of data to predict the next 28 days ahead. After the hyperparameter optimization procedure, we use the average number of iterations at which the lowest validation loss was achieved across the 3 validation sets as the number of estimators to use in our final model. The final model uses the last three years of data preceding the first day in the test set.
\begin{table*}
  \caption{Key hyperparameters used in our experiments. The parameters with a search range included are optimized in a hyperparameter search.}
  \label{tab:hyperparams}
  \begin{center}
  {\small\setlength{\tabcolsep}{1pt} 
  \begin{tabular}{l l c c }
  \toprule 
  Parameter & Description & Default value & Search range \\
  \midrule
  \texttt{n\_estimators} & Number of trees in each model & 2000 & Lowest validation loss \\
  \texttt{n\_trials} & Number of optimization trials to run & 100 & \\  
  \texttt{learning\_rate} & Learning rate & 0.05 & \\  
  \texttt{n\_validation\_sets} & Number of validation sets & 3 & \\  
  \texttt{n\_days\_test} & Number of days in validation and test sets & 28 & \\
  \texttt{max\_levels\_random} & Max. number of levels when using a random hierarchy & 2 & \\  
  \texttt{\shortstack{max\_categories\_per \\ \_random\_level}} & Max. categories per level in the random hierarchy & 1000 & \\  
  \texttt{hier\_freq} & Frequency of performing the randomized hierarchical aggregation & 1 & \texttt{uniform($1$, $10$)}  \\  
  \texttt{lambda\_l1} & L1-regularization & 0 & \texttt{log\_uniform($10^{-8}$, $10^{1}$)} \\  
  \texttt{lambda\_l2} & L2-regularization & 0 & \texttt{log\_uniform($10^{-8}$, $10^{1}$)} \\  
  \texttt{num\_leaves} & Max. number of leaves per tree & 31 & \texttt{uniform($2^{3}$, $2^{10}$)} \\  
  \texttt{feature\_fraction} & Fraction of features to use to build a tree & 1.0 & \texttt{uniform($0.4$, $1.0$)} \\  
  \texttt{bagging\_fraction} & Fraction of training samples to use to build a tree & 1.0 & \texttt{uniform($0.4$, $1.0$)} \\  
  \texttt{bagging\_freq} & Frequency at which to create a new bagging batch & 1.0 & \texttt{uniform($1$, $7$)} \\  
  \texttt{min\_child\_samples} & Minimum number of samples per leaf & 20 & \texttt{log\_uniform($5$, $5000$)} \\  
  \bottomrule
  \end{tabular}}
  \end{center}
\end{table*}



\section{Experiments} \label{app:experiments}

\begin{sidewaystable*}[t]
  \caption{Forecasting results for all stores on the M5 dataset. We report mean RMSE scores across 10 random seeds with standard deviation in brackets. Lower is better.}
  \label{tab:allstores_abs_rmse}
  \begin{center}\scalebox{0.8}{
  {\small\setlength{\tabcolsep}{1pt} 
  \begin{tabular}{l c cccccccccccccc}
  \toprule 
   &&&& &  &\multicolumn{3}{ c }{Store}   &\multicolumn{2}{ c }{Product} &\multicolumn{3}{ c }{State} \\
   \cmidrule(r){7-9} \cmidrule(r){10-11} \cmidrule(r){12-14}
  Scen./Obj. & Metric  & Reconciliation &Product	&Department	&Category &Department	&Category	&Total &Store	&State &Department &Category &Total	&Total	&All series \\
  \midrule																	
  \texttt{Bottom-up}																	\\
  \hspace{0.1cm} 	SL	&SL	&None	&2.11 (0.00)	&580 (3.95)	&1096 (13.71)	&110 (0.60)	&207 (1.21)	&416 (2.07)	&8.8 (0.03)	&4.26 (0.01)	&263 (1.30)	&510 (3.81)	&997 (8.44)	&2086 (38.53)	&22.4 (0.16)	\\
  \hspace{0.1cm} 	SL	&HL	&None	&2.11 (0.00)	&564 (5.58)	&1039 (14.64)	&107 (0.74)	&200 (1.69)	&408 (2.37)	&8.76 (0.06)	&4.25 (0.01)	&256 (1.30)	&490 (3.93)	&983 (5.93)	&2079 (33.46)	&21.8 (0.15)	\\
  \hspace{0.1cm} 	HL	&HL	&None	&2.11 (0.00)	&511 (8.93)	&882 (20.54)	&102 (0.90)	&185 (1.48)	&388 (4.21)	&8.5 (0.02)	&4.21 (0.01)	&233 (2.42)	&427 (4.78)	&878 (17.17)	&1819 (78.42)	&19.5 (0.38)	\\
  \hspace{0.1cm} 	HL	&SL	&None	&2.11 (0.00)	&511 (9.47)	&886 (24.67)	&103 (1.08)	&187 (2.12)	&390 (3.80)	&8.46 (0.04)	&4.19 (0.01)	&234 (3.15)	&430 (7.18)	&881 (15.27)	&1813 (64.6)	&19.6 (0.36)	\\
  \hspace{0.1cm} 	TL	&HL	&None	&2.11 (0.00)	&549 (9.21)	&1055 (22.68)	&109 (0.58)	&207 (1.27)	&415 (2.58)	&8.73 (0.03)	&4.25 (0.01)	&256 (2.09)	&502 (5.04)	&978 (13.8)	&2007 (63.57)	&21.8 (0.34)	\\
  \hspace{0.1cm} 	TL	&SL	&None	&2.11 (0.00)	&544 (7.66)	&1017 (25.23)	&110 (0.40)	&208 (1.20)	&416 (2.41)	&8.68 (0.04)	&4.25 (0.01)	&256 (1.55)	&497 (5.23)	&972 (7.67)	&1943 (41.21)	&21.5 (0.25)	\\
  \hspace{0.1cm} 	TL	&TL	&None	&2.46 (0.00)	&1579 (9.71)	&3075 (20.5)	&193 (0.87)	&379 (1.84)	&717 (5.17)	&13.41 (0.05)	&5.68 (0.02)	&566 (3.11)	&1111 (6.42)	&2074 (18.68)	&5646 (60.40)	&53.3 (0.39)	\\
  \midrule																	
  \texttt{Sep. agg.}																	\\
  \hspace{0.1cm} 	SL	&SL	&Base	&2.11 (0.00)	&835 (23.47)	&1417 (69.78)	&130 (1.86)	&236 (5.25)	&474 (8.71)	&8.88 (0.02)	&4.23 (0.01)	&322 (11.48)	&682 (19.02)	&1269 (47.11)	&3339 (86.79)	&30.1 (0.21)	\\
  \hspace{0.1cm} 	SL	&SL	&OLS	&2.11 (0.00)	&804 (14.22)	&1548 (22.24)	&120 (1.49)	&220 (1.95)	&443 (2.37)	&8.77 (0.02)	&4.25 (0.01)	&315 (4.15)	&605 (7.98)	&1222 (9.69)	&3125 (24.25)	&29.1 (0.22)	\\
  \hspace{0.1cm} 	SL	&SL	&WLS-struct	&2.10 (0.00)	&731 (8.55)	&1505 (16.09)	&113 (0.58)	&217 (1.13)	&426 (2.17)	&8.71 (0.02)	&4.23 (0.01)	&291 (2.38)	&591 (4.82)	&1153 (7.53)	&2909 (23.52)	&27.6 (0.22)	\\
  \hspace{0.1cm} 	SL	&SL	&WLS-var	&2.11 (0.00)	&647 (7.23)	&1345 (18.95)	&109 (0.51)	&211 (1.43)	&411 (2.06)	&8.72 (0.03)	&4.24 (0.01)	&272 (2.06)	&555 (5.58)	&1068 (8.77)	&2541 (31.06)	&25.1 (0.25)	\\
  \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&2.11 (0.00)	&667 (18.15)	&1397 (44.32)	&106 (1.15)	&206 (3.14)	&405 (5.45)	&8.76 (0.02)	&4.24 (0.01)	&272 (4.92)	&558 (12.59)	&1091 (22.52)	&2706 (80.28)	&25.8 (0.63)	\\
  \hspace{0.1cm} 	SL	&SL	&ERM	&2.58 (0.01)	&728 (48.01)	&1411 (118.36)	&117 (3.66)	&213 (8.73)	&443 (20.5)	&10.30 (0.10)	&5.21 (0.04)	&307 (14.61)	&583 (33.15)	&1215 (75.3)	&3101 (258.29)	&28.1 (1.83)	\\
  \midrule																	
  \texttt{Global}																	\\
  \hspace{0.1cm} 	SL	&SL	&Base	&2.16 (0.00)	&771 (54.94)	&1587 (256.06)	&120 (2.11)	&227 (8.81)	&458 (18.45)	&9.09 (0.05)	&4.4 (0.01)	&329 (18.51)	&650 (31.09)	&1807 (1203.62)	&3273 (338.71)	&32.8 (6.60)	\\
  \hspace{0.1cm} 	SL	&SL	&OLS	&2.13 (0.01)	&764 (58.91)	&1524 (181.38)	&117 (10.32)	&227 (27.24)	&484 (109.14)	&8.99 (0.05)	&4.34 (0.03)	&316 (42.99)	&637 (107.63)	&1371 (417.84)	&3114 (417.38)	&29.9 (2.52)	\\
  \hspace{0.1cm} 	SL	&SL	&WLS-struct	&2.13 (0.00)	&800 (45.37)	&1686 (103.86)	&118 (2.52)	&235 (5.96)	&463 (11.90)	&9.09 (0.03)	&4.35 (0.01)	&314 (10.72)	&651 (24.97)	&1264 (55.94)	&3231 (279.96)	&30.4 (1.68)	\\
  \hspace{0.1cm} 	SL	&SL	&WLS-var	&2.14 (0.01)	&876 (43.62)	&1867 (106.77)	&130 (2.31)	&263 (7.10)	&508 (18.51)	&9.10 (0.05)	&4.35 (0.01)	&345 (11.71)	&727 (30.30)	&1374 (80.66)	&3463 (283.05)	&33.2 (1.84)	\\
  \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&2.18 (0.02)	&730 (50.73)	&1545 (155.65)	&115 (2.42)	&228 (5.97)	&478 (19.61)	&9.29 (0.16)	&4.46 (0.06)	&293 (12.06)	&599 (33.27)	&1235 (89.41)	&3211 (432.02)	&29.1 (2.33)	\\
  \hspace{0.1cm} 	SL	&SL	&ERM	&2.56 (0.14)	&923 (113.39)	&1857 (271.23)	&138 (16.52)	&265 (34.54)	&558 (105.87)	&10.52 (0.62)	&5.23 (0.34)	&382 (56.45)	&762 (118.54)	&1607 (371.34)	&3752 (669.36)	&35.7 (3.74)	\\
  
  \bottomrule
  
\end{tabular}}}
  \end{center}
  \end{sidewaystable*}

  \begin{sidewaystable*}[t]
    \caption{Forecasting results for all stores on the M5 dataset. We report mean MAE scores across 10 random seeds with standard deviation in brackets. Lower is better.}
    \label{tab:allstores_abs_mae}
    \begin{center}\scalebox{0.8}{
    {\small\setlength{\tabcolsep}{1pt} 
    \begin{tabular}{l c cccccccccccccc}
    \toprule 
     &&&& &  &\multicolumn{3}{ c }{Store}   &\multicolumn{2}{ c }{Product} &\multicolumn{3}{ c }{State} \\
     \cmidrule(r){7-9} \cmidrule(r){10-11} \cmidrule(r){12-14}
    Scen./Obj. & Metric  & Reconciliation &Product	&Department	&Category &Department	&Category	&Total &Store	&State &Department &Category &Total	&Total	&All series \\
    \midrule																	
    \texttt{Bottom-up}																	\\
    \hspace{0.1cm} 	SL	&SL	&None	&1.07 (0)	&420 (5.51)	&772 (12.94)	&70 (0.43)	&131 (0.84)	&312 (2.27)	&4.41 (0.01)	&2.2 (0)	&181 (1.45)	&343 (2.57)	&812 (7.46)	&1628 (24.81)	&2.2 (0.01)	\\
    \hspace{0.1cm} 	SL	&HL	&None	&1.07 (0)	&410 (5.82)	&743 (12.96)	&68 (0.37)	&128 (0.75)	&309 (1.79)	&4.39 (0.01)	&2.19 (0)	&177 (1.29)	&332 (2.25)	&808 (5.34)	&1653 (40.22)	&2.2 (0.01)	\\
    \hspace{0.1cm} 	HL	&HL	&None	&1.06 (0)	&342 (7.54)	&598 (18.24)	&63 (0.58)	&117 (1.36)	&293 (4.26)	&4.33 (0.01)	&2.18 (0)	&154 (2.01)	&283 (4.61)	&724 (14.31)	&1427 (81.18)	&2.1 (0.01)	\\
    \hspace{0.1cm} 	HL	&SL	&None	&1.06 (0)	&340 (6.2)	&597 (18.44)	&63 (0.56)	&117 (1.19)	&294 (3.06)	&4.33 (0.01)	&2.18 (0)	&154 (1.91)	&284 (4.49)	&725 (13.35)	&1417 (57.53)	&2.1 (0.01)	\\
    \hspace{0.1cm} 	TL	&HL	&None	&1.05 (0)	&350 (4.5)	&634 (10.36)	&65 (0.17)	&122 (0.49)	&299 (1.69)	&4.27 (0.01)	&2.15 (0)	&160 (0.69)	&305 (1.95)	&747 (9.07)	&1425 (50.6)	&2.1 (0)	\\
    \hspace{0.1cm} 	TL	&SL	&None	&1.06 (0)	&359 (4.18)	&647 (10.08)	&66 (0.25)	&124 (0.52)	&305 (1.54)	&4.3 (0.01)	&2.17 (0)	&164 (1.27)	&311 (2.84)	&778 (4.19)	&1457 (28.24)	&2.1 (0)	\\
    \hspace{0.1cm} 	TL	&TL	&None	&1.09 (0)	&866 (4.9)	&1843 (11.88)	&102 (0.44)	&210 (1.1)	&515 (4.95)	&5.03 (0.01)	&2.35 (0)	&306 (1.48)	&645 (3.75)	&1601 (17.55)	&4647 (59.66)	&2.8 (0.01)	\\
    \midrule																	
    \texttt{Sep. agg.}																	\\
    \hspace{0.1cm} 	SL	&SL	&Base	&1.07 (0)	&467 (13.86)	&819 (36.81)	&71 (0.62)	&144 (2.96)	&336 (9.36)	&4.23 (0.01)	&2.13 (0)	&187 (4.24)	&393 (11.11)	&971 (50.11)	&2523 (105.3)	&2.2 (0)	\\
    \hspace{0.1cm} 	SL	&SL	&OLS	&1.03 (0)	&455 (8.08)	&895 (10.88)	&70 (0.73)	&129 (0.87)	&311 (1.76)	&4.24 (0.01)	&2.13 (0)	&181 (1.81)	&345 (2.84)	&910 (9.07)	&2430 (45.28)	&2.2 (0.01)	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-struct	&1.03 (0)	&417 (2.56)	&876 (8.99)	&64 (0.28)	&124 (0.66)	&298 (1.64)	&4.22 (0)	&2.12 (0)	&168 (1)	&334 (2.27)	&842 (6.11)	&2315 (22.33)	&2.1 (0)	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-var	&1.04 (0)	&383 (3.33)	&771 (10.62)	&63 (0.25)	&120 (0.66)	&286 (1.48)	&4.24 (0)	&2.13 (0)	&162 (1)	&318 (2.75)	&770 (6.87)	&1887 (32.1)	&2.1 (0)	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&1.04 (0)	&393 (10.24)	&811 (27.92)	&62 (0.56)	&118 (1.64)	&283 (4.33)	&4.23 (0.01)	&2.13 (0)	&162 (2.44)	&316 (7.33)	&796 (18.35)	&2116 (81.24)	&2.1 (0.01)	\\
    \hspace{0.1cm} 	SL	&SL	&ERM	&1.26 (0)	&490 (28.29)	&935 (91.26)	&76 (1.99)	&139 (5.8)	&335 (17.11)	&5.26 (0.03)	&2.67 (0.01)	&201 (7.9)	&383 (22.22)	&966 (63.05)	&2559 (296.35)	&2.6 (0.04)	\\
    \midrule																	
    \texttt{Global}																	\\
    \hspace{0.1cm} 	SL	&SL	&Base	&1.12 (0.01)	&436 (22.31)	&900 (156.94)	&70 (1.01)	&133 (3.84)	&326 (15.88)	&4.37 (0.01)	&2.18 (0.01)	&188 (8.04)	&373 (13.48)	&1314 (611.36)	&2569 (320.87)	&2.3 (0.04)	\\
    \hspace{0.1cm} 	SL	&SL	&OLS	&1.05 (0.01)	&456 (32.4)	&906 (97.58)	&73 (11.48)	&139 (28.62)	&352 (93.72)	&4.38 (0.03)	&2.19 (0.04)	&197 (41.58)	&388 (103.17)	&1033 (336.46)	&2441 (389.89)	&2.3 (0.13)	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-struct	&1.05 (0)	&456 (17.42)	&970 (53.68)	&68 (0.83)	&133 (2.1)	&323 (7.31)	&4.4 (0.01)	&2.18 (0.01)	&181 (3.3)	&368 (10.47)	&937 (40.46)	&2552 (258.94)	&2.2 (0.02)	\\
    \hspace{0.1cm} 	SL	&SL	&WLS-var	&1.06 (0)	&522 (18.96)	&1082 (59.2)	&76 (0.8)	&148 (3.13)	&353 (13.72)	&4.43 (0.02)	&2.19 (0.01)	&206 (4.75)	&415 (14.39)	&1017 (62.93)	&2744 (260.65)	&2.3 (0.02)	\\
    \hspace{0.1cm} 	SL	&SL	&MinT-shrink	&1.06 (0.01)	&474 (24.59)	&960 (92.69)	&74 (1.56)	&141 (3.13)	&358 (11.28)	&4.49 (0.06)	&2.22 (0.02)	&190 (7)	&367 (22.7)	&935 (87.38)	&2605 (424.61)	&2.3 (0.03)	\\
    \hspace{0.1cm} 	SL	&SL	&ERM	&1.25 (0.06)	&625 (60.87)	&1252 (148.15)	&90 (9.25)	&175 (24.13)	&426 (85.57)	&5.26 (0.29)	&2.65 (0.14)	&252 (34.4)	&512 (89.23)	&1285 (310.6)	&3190 (636.58)	&2.8 (0.19)	\\   
    
    \bottomrule
    
  \end{tabular}}}
    \end{center}
    \end{sidewaystable*}

\end{document}
\endinput
