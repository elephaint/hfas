@misc{__,
  urldate = {2019-04-16},
  howpublished = {https://dl.acm.org/citation.cfm?id=3219841},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZXCNSR38\citation.html}
}

@misc{__a,
  urldate = {2023-03-06},
  howpublished = {https://probml.github.io/pml-book/book2.html},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WMS7AXYH\book2.html}
}

@misc{__b,
  urldate = {2023-03-20},
  howpublished = {https://probml.github.io/pml-book/book2.html},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\CXULXDQ3\book2.html}
}

@misc{_1709_,
  title = {[1709.03159] {{R2N2}}: {{Residual Recurrent Neural Networks}} for {{Multivariate Time Series Forecasting}}},
  urldate = {2019-04-26},
  howpublished = {https://arxiv.org/abs/1709.03159},
  annotation = {00007},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5S4K4UNJ\1709.html}
}

@misc{_1909_,
  title = {[1909.06312] {{Neural Oblivious Decision Ensembles}} for {{Deep Learning}} on {{Tabular Data}}},
  urldate = {2019-09-20},
  howpublished = {https://arxiv.org/abs/1909.06312},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MV3EPHTR\1909.html}
}

@misc{_2016graves_,
  title = {{2016-graves.pdf}},
  journal = {Dropbox},
  urldate = {2019-03-19},
  abstract = {Gedeeld met Dropbox},
  howpublished = {https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf},
  langid = {dutch},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WMGJDRBV\2016-graves.html}
}

@misc{_alternative_,
  title = {An Alternative to the {{Cauchy}} Distribution | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.mex.2019.02.025},
  urldate = {2019-12-03},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S2215016119300366?token=CCA64417BD2FFF11F1D15B9A83F61DA57C9996BB42DFFEC12222645A2E23C4CF03BEAD97AF59C21B47C76CE939E9FF00},
  langid = {english},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ES3H9XTH\\An alternative to the Cauchy distribution  Elsevi.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4TLV7UAG\\S2215016119300366.html}
}

@misc{_another_,
  title = {Another Look at Estimators for Intermittent Demand | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.ijpe.2016.04.017},
  urldate = {2019-06-03},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S092552731630041X?token=596D9CF70DBCE00A7377C89CFB6B5EBB6C09FAF0CECA929BA3690BF9B12E29CDFEDA53AD2CFE4110D0BB94FFD5C666AC},
  langid = {english},
  annotation = {00016},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WLDJDPVP\\Another look at estimators for intermittent demand.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\APRMLTDQ\\S092552731630041X.html}
}

@misc{_caltrans_,
  title = {Caltrans {{PeMS}}},
  urldate = {2019-04-15},
  howpublished = {http://pems.dot.ca.gov/},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\23TAXRGN\pems.dot.ca.gov.html}
}

@misc{_common_,
  title = {Common Statistical Tests Are Linear Models (or: How to Teach Stats)},
  urldate = {2019-04-03},
  howpublished = {https://lindeloev.github.io/tests-as-linear/\#8\_sources\_and\_further\_equivalences},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\L22KE43I\tests-as-linear.html}
}

@misc{_convolutional_,
  title = {Convolutional {{Decision Trees}} for {{Feature Learning}} and {{Segmentation}} | {{SpringerLink}}},
  urldate = {2019-09-09},
  howpublished = {https://link.springer.com/chapter/10.1007/978-3-319-11752-2\_8},
  annotation = {00010},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\GLW9EGAL\978-3-319-11752-2_8.html}
}

@misc{_corporacion_,
  title = {Corporaci\'on {{Favorita Grocery Sales Forecasting}}},
  urldate = {2019-04-18},
  abstract = {Can you accurately predict sales for a large grocery chain?},
  howpublished = {https://kaggle.com/c/favorita-grocery-sales-forecasting},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QIVBM9EK\data.html}
}

@misc{_datasets_,
  title = {Datasets},
  urldate = {2019-04-25},
  howpublished = {http://www.neural-forecasting-competition.com/downloads/NN5/datasets/download.htm},
  annotation = {01527},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\CPZ45ZEX\download.html}
}

@misc{_distributional_2018,
  title = {Distributional {{Regression Forests}} for {{Probabilistic Precipitation Forecasting}} in {{Complex Terrain}}},
  year = {2018},
  month = apr,
  journal = {DeepAI},
  urldate = {2020-08-27},
  abstract = {04/09/18 - To obtain a probabilistic model for a dependent variable based on some set of explanatory variables, a distributional approach is ...},
  howpublished = {https://deepai.org/publication/distributional-regression-forests-for-probabilistic-precipitation-forecasting-in-complex-terrain},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TVQGR3GA\distributional-regression-forests-for-probabilistic-precipitation-forecasting-in-complex-terrai.html}
}

@misc{_doi_,
  title = {Doi:10.1016/j.Ejor.2006.02.006 | {{Elsevier Enhanced Reader}}},
  shorttitle = {Doi},
  doi = {10.1016/j.ejor.2006.02.006},
  urldate = {2019-11-21},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0377221706000737?token=5C24739A16E4A24229F3BC2E26F2D703E5A3BD147D88E7DADCA728C6B55C66655B0289A264F2E51653600D21982DE992},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\NP7WQQ72\S0377221706000737.html}
}

@misc{_elsevier_,
  title = {Elsevier {{Enhanced Reader}}},
  doi = {10.1016/j.jmva.2021.104796},
  urldate = {2022-06-29},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0047259X21000749?token=6A13B381D4B7D783CDE5896BA4D1A2515C21ED27AC4F378C2CDD902A8649ED76D12C5530CBCCE1BCB7A705FB47CC6EEB\&originRegion=eu-west-1\&originCreation=20220629125315},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\95UADWML\\Elsevier Enhanced Reader.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\JUVRLCTP\\S0047259X21000749.html}
}

@misc{_expectation_2013,
  title = {The Expectation and Variance of the Ratio of Two Random Variables},
  year = {2013},
  month = aug,
  journal = {Statistical Reflections of a Medical Doctor},
  urldate = {2020-08-31},
  abstract = {I was recently revising a paper concerning statistical simulations of hemodialysis trials, in which I examine the effects of different technical aspects of the dialysis prescription at the populati\ldots},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FYZ7TV92\the-expectation-of-the-ratio-of-two-random-variables.html}
}

@misc{_forecasting_,
  title = {Forecasting Weakly Correlated Time Series in Tasks of Electronic Commerce},
  urldate = {2021-02-04},
  abstract = {Forecasting of weakly correlated time series of conversion rate by methods of exponential smoothing, neural network and decision tree on the example of conversion percent series for an electronic store is considered in the paper. The advantages and disadvantages of each method are considered.},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/8098793},
  langid = {american},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ADV5ZPQY\8098793.html}
}

@misc{_gaussian_,
  title = {Gaussian {{Processes}} for {{Dummies}} {$\cdot$}},
  urldate = {2019-08-06},
  howpublished = {https://katbailey.github.io/post/gaussian-processes-for-dummies/},
  annotation = {00015},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\IHL6TSWR\gaussian-processes-for-dummies.html}
}

@misc{_gentle_,
  title = {{$\mu$} \& {$\sigma$} - {{A Gentle Introduction}} to {{Conformal Prediction}} and {{Distribution-Free Uncertainty Quantification}}},
  urldate = {2023-03-03},
  howpublished = {https://people.eecs.berkeley.edu/\textasciitilde angelopoulos/blog/posts/gentle-intro/}
}

@misc{_gradient_,
  title = {Gradient Boosting Machines, a Tutorial},
  urldate = {2019-08-06},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/},
  annotation = {00302},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4KYASN3Q\\Gradient boosting machines, a tutorial.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HEDDUAUH\\PMC3885826.html}
}

@misc{_hierarchical_2022,
  title = {Hierarchical ðŸ‘‘ {{Forecast}}},
  year = {2022},
  month = aug,
  urldate = {2022-08-10},
  abstract = {Hierarchical ðŸ‘‘ forecasting with statistical and econometric methods.},
  copyright = {MIT},
  howpublished = {Nixtla},
  keywords = {bottomup,coherent,econometrics,erm,forecasting,forecasting-models,hierarchical,hierarchical-forecast,middleout,mint,reconciliation,statistics,time-series,topdown}
}

@misc{_hourly_,
  title = {Hourly {{Energy Consumption}}},
  urldate = {2019-04-17},
  abstract = {Over 10 years of hourly energy consumption data from PJM in Megawatts},
  howpublished = {https://kaggle.com/robikscube/hourly-energy-consumption},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\XESP52TB\3.html}
}

@misc{_identifying_,
  title = {Identifying Demand Factors for Promotional Planning and Forecasting {{A}} Case of a Soft Drink Company in the {{UK}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.ijpe.2010.07.007},
  urldate = {2019-11-21},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S092552731000232X?token=0874AE0E954F5B21571F143F681F1FDFBF396F2F0919773076D6B60EC5C7E35EB8E01EE385D0A268306B036FFCED399B},
  langid = {english},
  annotation = {00084},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\XD8DE8LW\S092552731000232X.html}
}

@misc{_m3competition_,
  title = {M3-{{Competition}}},
  journal = {International Institute of Forecasters},
  urldate = {2019-10-11},
  abstract = {The 3003 time series of the M3-Competition are distributed as follows: Types of Time Series Data Interval Micro Industry Macro Finance Demog Other Total Yearly 146 102 83 58 245 11 645 Quarterly 204 83 336 76 57 0 756 Monthly 474 \ldots},
  langid = {american},
  annotation = {01441},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TXTBCNVH\m3-competition.html}
}

@misc{_mathmode_,
  title = {'math-Mode' Tag Wiki},
  journal = {TeX - LaTeX Stack Exchange},
  urldate = {2019-04-03},
  howpublished = {https://tex.stackexchange.com/tags/math-mode/info},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\H5DZ2JBS\info.html}
}

@misc{_nasdaq_,
  title = {{{NASDAQ}} 100 Stock Data},
  urldate = {2019-04-03},
  howpublished = {http://cseweb.ucsd.edu/\textasciitilde yaq007/NASDAQ100\_stock\_data.html},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\G4K8LAPI\NASDAQ100_stock_data.html}
}

@misc{_ngboost_,
  title = {Ngboost.Pptx},
  journal = {Google Docs},
  urldate = {2020-06-18},
  howpublished = {https://drive.google.com/file/d/183BWFAdFms81MKy6hSku8qI97OwS\_JH\_/view?usp=sharing\&usp=embed\_facebook},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZJJGF9FL\view.html}
}

@misc{_papers_,
  title = {Papers {{With Code}} : {{Explainable Text-Driven Neural Network}} for {{Stock Prediction}}},
  shorttitle = {Papers {{With Code}}},
  urldate = {2019-04-17},
  abstract = {Implemented in one code library. Click to access code and evaluation tables.},
  howpublished = {http://paperswithcode.com/paper/explainable-text-driven-neural-network-for},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LWBZ5IGT\explainable-text-driven-neural-network-for.html}
}

@misc{_pii_,
  title = {{{PII}}: 0166-{{218X}}(90)90128-{{Y}} | {{Elsevier Enhanced Reader}}},
  shorttitle = {{{PII}}},
  issn = {0166-218X},
  doi = {10.1016/0166-218X(90)90128-Y},
  urldate = {2022-02-15},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/0166218X9090128Y?token=6960AAEAFFE745587225311D705A537FDBF639568957A55B891A351786DB96F3FB2BC8734D9A15872A4AB034CD666895\&originRegion=eu-west-1\&originCreation=20220215164307},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\G35WQHRW\0166218X9090128Y.html}
}

@misc{_predict_,
  title = {Predict {{Future Sales}}},
  urldate = {2019-04-23},
  abstract = {Final project for "How to win a data science competition" Coursera course},
  howpublished = {https://kaggle.com/c/competitive-data-science-predict-future-sales},
  annotation = {00002},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\F7MNT58Y\data.html}
}

@article{_preliminary_,
  title = {{{PRELIMINARY VERSION DO NOT CITE}}},
  pages = {9},
  abstract = {Traffic forecasting is attracting considerable interest due to its widespread application in intelligent transportation systems. Given the complex and dynamic traffic data, many methods focus on how to establish a spatial-temporal model to express the non-stationary traffic patterns. Recently, the latest Graph Convolution Network (GCN) has been introduced to learn spatial features while the time neural networks are used to learn temporal features. These GCN based methods obtain state-of-the-art performance. However, the current GCN based methods ignore the natural hierarchical structure of traffic systems which is composed of the micro layers of road networks and the macro layers of region networks, in which the nodes are obtained through pooling method and could include some hot traffic regions such as downtown and CBD etc., while the current GCN is only applied on the micro graph of road networks. In this paper, we propose a novel Hierarchical Graph Convolution Networks (HGCN) for traffic forecasting by operating on both the micro and macro traffic graphs. The proposed method is evaluated on two complex city traffic speed datasets. Compared to the latest GCN based methods like Graph WaveNet, the proposed HGCN gets higher traffic forecasting precision with lower computational cost.The website of the code is https://github.com/guokan987/HGCN.git.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\UL3I557F\PRELIMINARY VERSION DO NOT CITE.pdf}
}

@misc{_psagan_,
  title = {{{PSA-GAN}}: {{Progressive}} Self Attention {{GANs}} for Synthetic Time Series},
  shorttitle = {{{PSA-GAN}}},
  journal = {Amazon Science},
  urldate = {2023-05-31},
  abstract = {Realistic synthetic time series data of sufficient length enables practical applications in time series modeling tasks, such as forecasting, but remains a challenge. In this paper we present PSAGAN, a generative adversarial network (GAN) that generates long time series samples of high quality using\ldots},
  howpublished = {https://www.amazon.science/publications/psa-gan-progressive-self-attention-gans-for-synthetic-time-series},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\437RT5HT\\Jeha et al. - PSA-GAN Progressive Self Attention GANs for Synth.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9EMD9STA\\psa-gan-progressive-self-attention-gans-for-synthetic-time-series.html}
}

@misc{_rossmann_,
  title = {Rossmann {{Store Sales}}},
  urldate = {2019-04-23},
  abstract = {Forecast sales using store, promotion, and competitor data},
  howpublished = {https://kaggle.com/c/rossmann-store-sales},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\XTUIAX6P\data.html}
}

@misc{_time_,
  title = {Time {{Series Classification Website}}},
  urldate = {2019-04-12},
  howpublished = {http://www.timeseriesclassification.com/index.php},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\G7DHR3I4\index.html}
}

@misc{_timeseries_2018,
  title = {Time-Series {{Prediction}} Using {{XGBoost}}},
  year = {2018},
  month = may,
  journal = {George Burry},
  urldate = {2019-04-18},
  abstract = {Machine Learning, XGBoost, time-series},
  howpublished = {http://www.georgeburry.com//time-series-xgb/},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\E84F33B2\time-series-xgb.html}
}

@misc{_tree_,
  title = {Tree {{Echo State Networks}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.neucom.2012.08.017},
  urldate = {2019-09-07},
  howpublished = {https://reader.elsevier.com/reader/sd/pii/S0925231212006571?token=B079EBE3E1DD969A58FDA461DE0D21199B81E002B68757ACD7989CF61594735B8C3200D1ABBDDC3D4A6FF2A0AF1002D5},
  langid = {english},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\J8RQTGPK\\Tree Echo State Networks  Elsevier Enhanced Reade.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4XZW5G83\\S0925231212006571.html}
}

@misc{_tutorial_,
  title = {[{{Tutorial}}] {{Time Series}} Forecasting with {{XGBoost}}},
  urldate = {2019-04-17},
  abstract = {Using data from Hourly Energy Consumption},
  howpublished = {https://kaggle.com/robikscube/tutorial-time-series-forecasting-with-xgboost},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\KTFMPQ5X\data.html}
}

@misc{_uci_,
  title = {{{UCI Machine Learning Repository}}: {{SML2010 Data Set}}},
  urldate = {2019-04-03},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/SML2010},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\T29JR2YF\SML2010.html}
}

@misc{_uci_a,
  title = {{{UCI Machine Learning Repository}}: {{Data Sets}}},
  urldate = {2019-04-03},
  howpublished = {https://archive.ics.uci.edu/ml/datasets.html?format=\&task=\&att=\&area=\&numAtt=\&numIns=\&type=ts\&sort=nameUp\&view=table},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3JARR6XI\datasets.html}
}

@misc{_uci_b,
  title = {{{UCI Machine Learning Repository}}: {{Beijing PM2}}.5 {{Data Data Set}}},
  urldate = {2019-04-03},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3H4LFHXR\Beijing+PM2.html}
}

@misc{_uci_c,
  title = {{{UCI Machine Learning Repository}}: {{PEMS-SF Data Set}}},
  urldate = {2019-04-24},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/PEMS-SF},
  annotation = {00002},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WMPPYA2N\PEMS-SF.html}
}

@misc{_uci_d,
  title = {{{UCI Machine Learning Repository}}: {{ElectricityLoadDiagrams20112014 Data Set}}},
  urldate = {2019-04-24},
  howpublished = {https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014},
  annotation = {00005},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\44I243LS\ElectricityLoadDiagrams20112014.html}
}

@misc{_web_,
  title = {Web {{Traffic Time Series Forecasting}}},
  urldate = {2019-04-24},
  abstract = {Forecast future traffic to Wikipedia pages},
  howpublished = {https://kaggle.com/c/web-traffic-time-series-forecasting},
  annotation = {00002},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FI7HIASY\data.html}
}

@misc{_welcome_,
  title = {Welcome to the {{UCR Time Series Classification}}/{{Clustering Page}}},
  urldate = {2019-04-12},
  howpublished = {https://www.cs.ucr.edu/\%7Eeamonn/time\_series\_data\_2018/},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6EXNKAN9\time_series_data_2018.html}
}

@misc{_xgboost_,
  title = {{{XGBoost}}: {{A Concise Technical Overview}}},
  shorttitle = {{{XGBoost}}},
  urldate = {2019-08-06},
  langid = {american},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\EJ6HBWPL\xgboost-concise-technical-overview.html}
}

@article{abolghasemi_machine_2019,
  title = {Machine Learning Applications in Time Series Hierarchical Forecasting},
  author = {Abolghasemi, Mahdi and Hyndman, Rob J. and Tarr, Garth and Bergmeir, Christoph},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.00370 [cs, stat]},
  eprint = {1912.00370},
  primaryclass = {cs, stat},
  urldate = {2021-02-04},
  abstract = {Hierarchical forecasting (HF) is needed in many situations in the supply chain (SC) because managers often need different levels of forecasts at different levels of SC to make a decision. Top-Down (TD), Bottom-Up (BU) and Optimal Combination (COM) are common HF models. These approaches are static and often ignore the dynamics of the series while disaggregating them. Consequently, they may fail to perform well if the investigated group of time series are subject to large changes such as during the periods of promotional sales. We address the HF problem of predicting real-world sales time series that are highly impacted by promotion. We use three machine learning (ML) models to capture sales variations over time. Artificial neural networks (ANN), extreme gradient boosting (XGboost), and support vector regression (SVR) algorithms are used to estimate the proportions of lower-level time series from the upper level. We perform an in-depth analysis of 61 groups of time series with different volatilities and show that ML models are competitive and outperform some well-established models in the literature.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UAMFTH8B\\Abolghasemi et al. - 2019 - Machine learning applications in time series hiera.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6DYK894F\\1912.html}
}

@misc{abolghasemi_model_2020,
  title = {Model Selection in Reconciling Hierarchical Time Series},
  author = {Abolghasemi, Mahdi and Hyndman, Rob J. and Spiliotis, Evangelos and Bergmeir, Christoph},
  year = {2020},
  month = oct,
  number = {arXiv:2010.10742},
  eprint = {2010.10742},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-05-24},
  abstract = {Model selection has been proven an effective strategy for improving accuracy in time series forecasting applications. However, when dealing with hierarchical time series, apart from selecting the most appropriate forecasting model, forecasters have also to select a suitable method for reconciling the base forecasts produced for each series to make sure they are coherent. Although some hierarchical forecasting methods like minimum trace are strongly supported both theoretically and empirically for reconciling the base forecasts, there are still circumstances under which they might not produce the most accurate results, being outperformed by other methods. In this paper we propose an approach for dynamically selecting the most appropriate hierarchical forecasting method and succeeding better forecasting accuracy along with coherence. The approach, to be called conditional hierarchical forecasting, is based on Machine Learning classification methods and uses time series features as leading indicators for performing the selection for each hierarchy examined considering a variety of alternatives. Our results suggest that conditional hierarchical forecasting leads to significantly more accurate forecasts than standard approaches, especially at lower hierarchical levels.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZZK65TBZ\Abolghasemi et al. - 2020 - Model selection in reconciling hierarchical time s.pdf}
}

@article{agarwal_two_2020,
  title = {Two {{Burning Questions}} on {{COVID-19}}: {{Did}} Shutting down the Economy Help? {{Can}} We (Partially) Reopen the Economy without Risking the Second Wave?},
  shorttitle = {Two {{Burning Questions}} on {{COVID-19}}},
  author = {Agarwal, Anish and Alomar, Abdullah and Sarker, Arnab and Shah, Devavrat and Shen, Dennis and Yang, Cindy},
  year = {2020},
  month = may,
  journal = {arXiv:2005.00072 [cs, econ, stat]},
  eprint = {2005.00072},
  primaryclass = {cs, econ, stat},
  urldate = {2020-05-12},
  abstract = {As we reach the apex of the COVID-19 pandemic across the globe, the most pressing question facing us all is: can we, even partially, reopen the economy without risking the second wave? Towards answering this question, we first need to understand if shutting down the economy helped. And second if it did, is it possible to achieve similar gains in the war against the pandemic while partially opening up the economy? To do so, it is critical to understand the effects of the various interventions that can be put into place and their corresponding health and economic implications. Since many possible interventions exist, the key challenge facing policy makers is understanding the potential trade-offs between them, and choosing the particular set of interventions that works best for their circumstance \textendash{} for example, what would be the effect on COVID-19 related deaths if the U.S. had decreased mobility by 20\% instead of 60\% versus if Italy had done the same? In this memo, we provide an overview of Synthetic Interventions [3] (a natural generalization of the widely applied Synthetic Control method [2]), which is a data-driven and statistically principled method to perform what-if scenario planning, i.e., for policy makers to understand the trade-offs between different interventions before having to actually enact them. In essence, the Synthetic Interventions method leverages information from different interventions that have already been enacted across the world and fits it to a policy maker's setting of interest \textendash{} for instance, to estimate the effect of mobility-restricting interventions on the U.S., we use daily death data from countries that enforced severe mobility restrictions to create a ``synthetic low mobility U.S.'' and predict the counterfactual trajectory of the U.S. if it had indeed applied a similar intervention.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,Statistics - Applications},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\Z5AGQP8G\Agarwal et al. - 2020 - Two Burning Questions on COVID-19 Did shutting do.pdf}
}

@article{ahn_gmm_2001,
  title = {{{GMM}} Estimation of Linear Panel Data Models with Time-Varying Individual e!Ects},
  author = {Ahn, Seung Chan and Lee, Young Hoon and Schmidt, Peter},
  year = {2001},
  journal = {Journal of Econometrics},
  pages = {37},
  abstract = {This paper considers models for panel data in which the individual e!ects vary over time. The temporal pattern of variation is arbitrary, but it is the same for all individuals. The model thus allows one to control for time-varying unobservables that are faced by all individuals (e.g., macro-economic events) and to which individuals may respond di!erently. A generalized within estimator is consistent under strong assumptions on the errors, but it is dominated by a generalized method of moments estimator. This is perhaps surprising, because the generalized within estimator is the MLE under normality. The e\$ciency gains from imposing second-moment error assumptions are evaluated; they are substantial when the regressors and e!ects are weakly correlated. 2001 Elsevier Science S.A. All rights reserved.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5A9QWJ43\Ahn et al. - 2001 - GMM estimation of linear panel data models with ti.pdf}
}

@inproceedings{akiba_optuna_2019,
  title = {Optuna: {{A Next-generation Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {2623--2631},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3292500.3330701},
  urldate = {2021-08-30},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  isbn = {978-1-4503-6201-6},
  keywords = {Bayesian optimization,black-box optimization,hyperparameter optimization,machine learning system}
}

@article{aksan_stcn_2019,
  title = {{{STCN}}: {{STOCHASTIC TEMPORAL CONVOLUTIONAL NETWORKS}}},
  author = {Aksan, Emre and Hilliges, Otmar},
  year = {2019},
  pages = {13},
  abstract = {Convolutional architectures have recently been shown to be competitive on many sequence modelling tasks when compared to the de-facto standard of recurrent neural networks (RNNs), while providing computational and modeling advantages due to inherent parallelism. However, currently there remains a performance gap to more expressive stochastic RNN variants, especially those with several layers of dependent random variables. In this work, we propose stochastic temporal convolutional networks (STCNs), a novel architecture that combines the computational advantages of temporal convolutional networks (TCN) with the representational power and robustness of stochastic latent spaces. In particular, we propose a hierarchy of stochastic latent variables that captures temporal dependencies at different time-scales. The architecture is modular and flexible due to decoupling of deterministic and stochastic layers. We show that the proposed architecture achieves state of the art log-likelihoods across several tasks. Finally, the model is capable of predicting high-quality synthetic samples over a long-range temporal horizon in modeling of handwritten text.},
  langid = {english},
  annotation = {00002},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2FXDN83K\Aksan and Hilliges - 2019 - STCN STOCHASTIC TEMPORAL CONVOLUTIONAL NETWORKS.pdf}
}

@article{akyildiz_probabilistic_2019,
  title = {Probabilistic Sequential Matrix Factorization},
  author = {Akyildiz, {\"O}mer Deniz and Damoulas, Theodoros and Steel, Mark F. J.},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.03906 [cs, stat]},
  eprint = {1910.03906},
  primaryclass = {cs, stat},
  urldate = {2020-02-27},
  abstract = {We introduce the probabilistic sequential matrix factorization (PSMF) method for factorizing time-varying and non-stationary datasets consisting of high-dimensional time-series. In particular, we consider nonlinear-Gaussian state-space models in which sequential approximate inference results in the factorization of a data matrix into a dictionary and time-varying coefficients with (possibly nonlinear) Markovian dependencies. The assumed Markovian structure on the coefficients enables us to encode temporal dependencies into a low-dimensional feature space. The proposed inference method is solely based on an approximate extended Kalman filtering scheme which makes the resulting method particularly efficient. The PSMF can account for temporal nonlinearities and, more importantly, can be used to calibrate and estimate generic differentiable nonlinear subspace models. We show that the PSMF can be used in multiple contexts: modelling time series with a periodic subspace, robustifying changepoint detection methods, and imputing missing-data in high-dimensional time-series of air pollutants measured across London.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\73LGYWPU\\Akyildiz et al. - 2019 - Probabilistic sequential matrix factorization.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\46IDYZH7\\1910.html}
}

@misc{alcaraz_diffusionbased_2022,
  title = {Diffusion-Based {{Time Series Imputation}} and {{Forecasting}} with {{Structured State Space Models}}},
  author = {Alcaraz, Juan Miguel Lopez and Strodthoff, Nils},
  year = {2022},
  month = aug,
  number = {arXiv:2208.09399},
  eprint = {2208.09399},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.09399},
  urldate = {2022-11-03},
  abstract = {The imputation of missing values represents a significant obstacle for many real-world data analysis pipelines. Here, we focus on time series data and put forward SSSD, an imputation model that relies on two emerging technologies, (conditional) diffusion models as state-of-the-art generative models and structured state space models as internal model architecture, which are particularly suited to capture long-term dependencies in time series data. We demonstrate that SSSD matches or even exceeds state-of-the-art probabilistic imputation and forecasting performance on a broad range of data sets and different missingness scenarios, including the challenging blackout-missing scenarios, where prior approaches failed to provide meaningful results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RSARK45B\\Alcaraz and Strodthoff - 2022 - Diffusion-based Time Series Imputation and Forecas.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\AFXLPJRZ\\2208.html}
}

@article{alexander_treed_1996,
  title = {Treed {{Regression}}},
  author = {Alexander, William P. and Grimshaw, Scott D.},
  year = {1996},
  month = jun,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {5},
  number = {2},
  pages = {156--175},
  issn = {1061-8600},
  doi = {10.1080/10618600.1996.10474702},
  urldate = {2019-09-05},
  abstract = {Given a data set consisting of n observations on p independent variables and a single dependent variable, treed regression creates a binary tree with a simple linear regression function at each of the leaves. Each node of the tree consists of an inequality condition on one of the independent variables. The tree is generated from the training data by a recursive partitioning algorithm. Treed regression models are more parsimonious than CART models because there are fewer splits. Additionally, monotonicity in some or all of the variables can be imposed.},
  annotation = {00104},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\DYVRSJ8S\10618600.1996.html}
}

@article{alexandrov_gluonts_2020,
  title = {{{GluonTS}}: {{Probabilistic}} and {{Neural Time Series Modeling}} in {{Python}}},
  shorttitle = {{{GluonTS}}},
  author = {Alexandrov, Alexander and Benidis, Konstantinos and {Bohlke-Schneider}, Michael and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Maddix, Danielle C. and Rangapuram, Syama and Salinas, David and Schulz, Jasper and Stella, Lorenzo and T{\"u}rkmen, Ali Caner and Wang, Yuyang},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {116},
  pages = {1--6},
  issn = {1533-7928},
  urldate = {2021-02-01},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UC5KA7B4\\Alexandrov et al. - 2020 - GluonTS Probabilistic and Neural Time Series Mode.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\JTSW3FBH\\19-820.html}
}

@article{alhajjhassan_reinforcement_2020,
  title = {Reinforcement Learning Framework for Freight Demand Forecasting to Support Operational Planning Decisions},
  author = {Al Hajj Hassan, Lama and Mahmassani, Hani S. and Chen, Ying},
  year = {2020},
  month = may,
  journal = {Transportation Research Part E: Logistics and Transportation Review},
  volume = {137},
  pages = {101926},
  issn = {1366-5545},
  doi = {10.1016/j.tre.2020.101926},
  urldate = {2020-05-25},
  abstract = {Freight forecasting is essential for managing, planning operating and optimizing the use of resources. Multiple market factors contribute to the highly variable nature of freight flows, which calls for adaptive and responsive forecasting models. This paper presents a demand forecasting methodology that supports freight operation planning over short to long term horizons. The method combines time series models and machine learning algorithms in a Reinforcement Learning framework applied over a rolling horizon. The objective is to develop an efficient method that reduces the prediction error by taking full advantage of the traditional time series models and machine learning models. In a case study applied to container shipment data for a US intermodal company, the approach succeeded in reducing the forecast error margin. It also allowed predictions to closely follow recent trends and fluctuations in the market while minimizing the need for user intervention. The results indicate that the proposed approach is an effective method to predict freight demand. In addition to clustering and Reinforcement Learning, a method for converting monthly forecasts to long-term weekly forecasts was developed and tested. The results suggest that these monthly-to-weekly long-term forecasts outperform the direct long term forecasts generated through typical time series approaches.},
  langid = {english},
  keywords = {Freight demand forecasting,Reinforcement learning,Rolling horizon,Time series},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FM4AYRXT\\Al Hajj Hassan et al. - 2020 - Reinforcement learning framework for freight deman.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9NTX9MAK\\S1366554519315169.html}
}

@misc{allard_what_2019,
  title = {What Is a {{Transformer}}?},
  author = {Allard, Maxime},
  year = {2019},
  month = jan,
  journal = {Inside Machine learning},
  urldate = {2019-04-28},
  abstract = {An Introduction to Transformers and Sequence-to-Sequence Learning for Machine Learning},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\T6BZ5NCS\what-is-a-transformer-d07dd1fbec04.html}
}

@article{alom_history_,
  title = {The {{History Began}} from {{AlexNet}}: {{A Comprehensive Survey}} on {{Deep Learning Approaches}}},
  author = {Alom, Zahangir and Taha, Tarek M and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima},
  pages = {39},
  abstract = {In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly, and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bio-informatics, natural language processing (NLP), cybersecurity, and many others.},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WVTRLEBW\Alom et al. - The History Began from AlexNet A Comprehensive Su.pdf}
}

@article{alonso_single_2019,
  title = {A {{Single Scalable LSTM Model}} for {{Short-Term Forecasting}} of {{Disaggregated Electricity Loads}}},
  author = {Alonso, Andr{\'e}s M. and Nogales, F. Javier and Ruiz, Carlos},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.06640 [cs, eess, stat]},
  eprint = {1910.06640},
  primaryclass = {cs, eess, stat},
  urldate = {2020-02-19},
  abstract = {As a powerful tool to improve their efficiency and sustainability, most electricity systems worldwide are deploying advanced metering infrastructures to collect relevant operational data. In particular, smart meters play a key role in this transformation as they allow tracking electricity load consumption at a very disaggregated level and at high frequency rates. This data opens the possibility of developing new forecasting models with a potential positive impact in both electricity distribution and retailing activities. In this work, we present a general methodology that is able to process and forecast a large number of smart meter time series. Instead of using traditional and univariate approaches for each time series, we develop a single but complex recurrent neural network model with long short-term memory that is able to capture individual consumption patterns and also the cross-sectional relations among different households. The resulting model can accurately predict future loads (short-term) of individual consumers, even if these were not included in the original training set (out-of-sample consumers). This entails a great potential for large scale applications (Big Data) as once the single network is trained, accurate individual forecast for new consumers can be obtained at almost no computational cost. The performance of the proposed model is tested under a large set of numerical experiments by using a real world dataset with thousands of disaggregated electricity consumption time series. Furthermore, we exploit the considered dataset to explore how geo-demographic segmentation of consumers can improve the forecasting accuracy of the proposed model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Applications,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IL2X85DH\\Alonso et al. - 2019 - A Single Scalable LSTM Model for Short-Term Foreca.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XRSDUAVM\\1910.html}
}

@article{alquier_matrix_2019,
  title = {Matrix Factorization for Multivariate Time Series Analysis},
  author = {Alquier, Pierre and Marie, Nicolas},
  year = {2019},
  month = oct,
  journal = {arXiv:1903.05589 [math, stat]},
  eprint = {1903.05589},
  primaryclass = {math, stat},
  urldate = {2020-02-19},
  abstract = {Matrix factorization is a powerful data analysis tool. It has been used in multivariate time series analysis, leading to the decomposition of the series in a small set of latent factors. However, little is known on the statistical performances of matrix factorization for time series. In this paper, we extend the results known for matrix estimation in the i.i.d setting to time series. Moreover, we prove that when the series exhibit some additional structure like periodicity or smoothness, it is possible to improve on the classical rates of convergence.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WQWTJNKD\\Alquier and Marie - 2019 - Matrix factorization for multivariate time series .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\2RSP3W68\\1903.html}
}

@article{alsabti_clouds_,
  title = {{{CLOUDS}}: {{A Decision Tree Classifier}} for {{Large Datasets}}},
  author = {Alsabti, Khaled and Ranka, Sanjay and Singh, Vineet},
  pages = {7},
  abstract = {Classification for very large datasets has many practical applications in data mining. Techniques such as discretization and dataset sampling can be used to scale up decision tree classifiers to large datasets. Unfortunately, both of these techniques can cause a significant loss in accuracy. Wepresent a noveldecision tree classifier called CLOUDwS,hich samples the splitting points for numericattributes followedby an estimation step to narrow the search space of the best split. CLOUDrSeduces computation and I/O complexity substantially comparedto state of the art classifters, whilemaintainingthe quality of the generated trees in terms of accuracyand tree size. We provide experimentalresults with a numberof real andsynthetic data\textasciitilde ets.},
  langid = {english},
  annotation = {00002},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5PPSMDUR\Alsabti et al. - CLOUDS A Decision Tree Classifier for Large Datas.pdf}
}

@article{amagata_reverse_2021,
  title = {Reverse {{Maximum Inner Product Search}}: {{How}} to Efficiently Find Users Who Would like to Buy My Item?},
  shorttitle = {Reverse {{Maximum Inner Product Search}}},
  author = {Amagata, Daichi and Hara, Takahiro},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.07131 [cs]},
  eprint = {2110.07131},
  primaryclass = {cs},
  urldate = {2022-03-01},
  abstract = {The MIPS (maximum inner product search), which finds the item with the highest inner product with a given query user, is an essential problem in the recommendation field. It is usual that e-commerce companies face situations where they want to promote and sell new or discounted items. In these situations, we have to consider a question: who are interested in the items and how to find them? This paper answers this question by addressing a new problem called reverse maximum inner product search (reverse MIPS). Given a query vector and two sets of vectors (user vectors and item vectors), the problem of reverse MIPS finds a set of user vectors whose inner product with the query vector is the maximum among the query and item vectors. Although the importance of this problem is clear, its straightforward implementation incurs a computationally expensive cost. We therefore propose Simpfer, a simple, fast, and exact algorithm for reverse MIPS. In an offline phase, Simpfer builds a simple index that maintains a lower-bound of the maximum inner product. By exploiting this index, Simpfer judges whether the query vector can have the maximum inner product or not, for a given user vector, in a constant time. Besides, our index enables filtering user vectors, which cannot have the maximum inner product with the query vector, in a batch. We theoretically demonstrate that Simpfer outperforms baselines employing state-of-the-art MIPS techniques. Furthermore, our extensive experiments on real datasets show that Simpfer is at least two orders magnitude faster than the baselines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Databases},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Q3R9AXX7\\Amagata and Hara - 2021 - Reverse Maximum Inner Product Search How to effic.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8NSRF28M\\2110.html}
}

@article{amornbunchornvej_framework_2020,
  title = {Framework for {{Inferring Following Strategies}} from {{Time Series}} of {{Movement Data}}},
  author = {Amornbunchornvej, Chainarong and {Berger-Wolf}, Tanya},
  year = {2020},
  month = jan,
  journal = {arXiv:1911.01366 [physics, stat]},
  eprint = {1911.01366},
  primaryclass = {physics, stat},
  urldate = {2020-02-27},
  abstract = {How do groups of individuals achieve consensus in movement decisions? Do individuals follow their friends, the one predetermined leader, or whomever just happens to be nearby? To address these questions computationally, we formalize "Coordination Strategy Inference Problem". In this setting, a group of multiple individuals moves in a coordinated manner towards a target path. Each individual uses a specific strategy to follow others (e.g. nearest neighbors, pre-defined leaders, preferred friends). Given a set of time series that includes coordinated movement and a set of candidate strategies as inputs, we provide the first methodology (to the best of our knowledge) to infer whether each individual uses local-agreement-system or dictatorship-like strategy to achieve movement coordination at the group level. We evaluate and demonstrate the performance of the proposed framework by predicting the direction of movement of an individual in a group in both simulated datasets as well as two real-world datasets: a school of fish and a troop of baboons. Moreover, since there is no prior methodology for inferring individual-level strategies, we compare our framework with the state-of-the-art approach for the task of classification of group-level-coordination models. The results show that our approach is highly accurate in inferring the correct strategy in simulated datasets even in complicated mixed strategy settings, which no existing method can infer. In the task of classification of group-level-coordination models, our framework performs better than the state-of-the-art approach in all datasets. Animal data experiments show that fish, as expected, follow their neighbors, while baboons have a preference to follow specific individuals. Our methodology generalizes to arbitrary time series data of real numbers, beyond movement data.},
  archiveprefix = {arxiv},
  keywords = {{37M10, 62F07, 92B99, 91C99, 68P99},Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,G.3,I.2.11,I.2.3,I.2.6,J.4,{Physics - Data Analysis, Statistics and Probability},Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TU3SKXZB\\Amornbunchornvej and Berger-Wolf - 2020 - Framework for Inferring Following Strategies from .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UIBTX4Y4\\1911.html}
}

@inproceedings{an_map_2016,
  title = {{{MAP}}: {{Frequency-Based Maximization}} of {{Airline Profits}} Based on an {{Ensemble Forecasting Approach}}},
  shorttitle = {{{MAP}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '16},
  author = {An, Bo and Chen, Haipeng and Park, Noseong and Subrahmanian, V.S.},
  year = {2016},
  pages = {421--430},
  publisher = {{ACM Press}},
  address = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2939726},
  urldate = {2019-04-17},
  abstract = {Though there are numerous traditional models to predict market share and demand along airline routes, the prediction of existing models is not precise enough and, to the best of our knowledge, there is no use of data-mining based forecasting techniques to improve airline profitability. We propose the MAP (Maximizing Airline Profits) architecture designed to help airlines and make two key contributions in airline market share and route demand prediction and prediction-based airline profit optimization. Compared with past methods to forecast market share and demand along airline routes, we introduce a novel Ensemble Forecasting (MAP-EF) approach considering two new classes of features: (i) features derived from clusters of similar routes, and (ii) features based on equilibrium pricing. We show that MAP-EF achieves much better Pearson Correlation Coefficients (over 0.95 vs. 0.82 for market share, 0.98 vs. 0.77 for demand) and R2-values compared with three stateof-the-art works for forecasting market share and demand, while showing much lower variance. Using the results of MAP-EF, we develop MAP-Bilevel Branch and Bound (MAP-BBB) and MAPGreedy (MAP-G) algorithms to optimally allocate flight frequencies over multiple routes, to maximize an airline's profit. Experimental results show that airlines can increase profits by a significant margin. All experiments were conducted with data aggregated from four sources: US Bureau of Transportation Statistics (BTS), US Bureau of Economic Analysis (BEA), the National Transportation Safety Board (NTSB), and the US Census Bureau (CB).},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  keywords = {read - not in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\B2WD592P\An et al. - 2016 - MAP Frequency-Based Maximization of Airline Profi.pdf}
}

@article{anderer_hierarchical_2022,
  title = {Hierarchical Forecasting with a Top-down Alignment of Independent Level Forecasts},
  author = {Anderer, Matthias and Li, Feng},
  year = {2022},
  month = feb,
  journal = {International Journal of Forecasting},
  eprint = {2103.08250},
  primaryclass = {cs, stat},
  pages = {S0169207021002211},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2021.12.015},
  urldate = {2022-05-25},
  abstract = {Hierarchical forecasting with intermittent time series is a challenge in both research and empirical studies. Extensive research focuses on improving the accuracy of each hierarchy, especially the intermittent time series at bottom levels. Then hierarchical reconciliation could be used to improve the overall performance further. In this paper, we present a \textbackslash emph\{hierarchical-forecasting-with-alignment\} approach that treats the bottom level forecasts as mutable to ensure higher forecasting accuracy on the upper levels of the hierarchy. We employ a pure deep learning forecasting approach N-BEATS for continuous time series at the top levels and a widely used tree-based algorithm LightGBM for the intermittent time series at the bottom level. The \textbackslash emph\{hierarchical-forecasting-with-alignment\} approach is a simple yet effective variant of the bottom-up method, accounting for biases that are difficult to observe at the bottom level. It allows suboptimal forecasts at the lower level to retain a higher overall performance. The approach in this empirical study was developed by the first author during the M5 Forecasting Accuracy competition, ranking second place. The method is also business orientated and could benefit for business strategic planning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HWJNXUKD\\Anderer and Li - 2022 - Hierarchical forecasting with a top-down alignment.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\AUZ387KU\\2103.html}
}

@incollection{andersen_chapter_2006,
  title = {Chapter 15 {{Volatility}} and {{Correlation Forecasting}}},
  booktitle = {Handbook of {{Economic Forecasting}}},
  author = {Andersen, Torben G. and Bollerslev, Tim and Christoffersen, Peter F. and Diebold, Francis X.},
  editor = {Elliott, G. and Granger, C. W. J. and Timmermann, A.},
  year = {2006},
  month = jan,
  volume = {1},
  pages = {777--878},
  publisher = {{Elsevier}},
  doi = {10.1016/S1574-0706(05)01015-3},
  urldate = {2021-02-04},
  abstract = {Volatility has been one of the most active and successful areas of research in time series econometrics and economic forecasting in recent decades. This chapter provides a selective survey of the most important theoretical developments and empirical insights to emerge from this burgeoning literature, with a distinct focus on forecasting applications. Volatility is inherently latent, and Section 1 begins with a brief intuitive account of various key volatility concepts. Section 2 then discusses a series of different economic situations in which volatility plays a crucial role, ranging from the use of volatility forecasts in portfolio allocation to density forecasting in risk management. Sections 3\textendash 5 present a variety of alternative procedures for univariate volatility modeling and forecasting based on the GARCH, stochastic volatility and realized volatility paradigms, respectively. Section 6 extends the discussion to the multivariate problem of forecasting conditional covariances and correlations, and Section 7 discusses volatility forecast evaluation methods in both univariate and multivariate cases. Section 8 concludes briefly.},
  langid = {english},
  keywords = {covariance forecasting,GARCH,realized volatility,stochastic volatility,volatility modeling},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RKA4NHYE\S1574070605010153.html}
}

@article{andreas_differential_2019,
  title = {Differential {{Bayesian Neural Nets}}},
  author = {Andreas, Look and Kandemir, Melih},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.00796 [cs, stat]},
  eprint = {1912.00796},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {Neural Ordinary Differential Equations (N-ODEs) are a powerful building block for learning systems, which extend residual networks to a continuous-time dynamical system. We propose a Bayesian version of N-ODEs that enables well-calibrated quantification of prediction uncertainty, while maintaining the expressive power of their deterministic counterpart. We assign Bayesian Neural Nets (BNNs) to both the drift and the diffusion terms of a Stochastic Differential Equation (SDE) that models the flow of the activation map in time. We infer the posterior on the BNN weights using a straightforward adaptation of Stochastic Gradient Langevin Dynamics (SGLD). We illustrate significantly improved stability on two synthetic time series prediction tasks and report better model fit on UCI regression benchmarks with our method when compared to its non-Bayesian counterpart.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QCSUI4CX\Andreas and Kandemir - 2019 - Differential Bayesian Neural Nets.pdf}
}

@article{andreoli_convolution_2020,
  title = {Convolution, Attention and Structure Embedding},
  author = {Andreoli, Jean-Marc},
  year = {2020},
  month = mar,
  journal = {arXiv:1905.01289 [cs, stat]},
  eprint = {1905.01289},
  primaryclass = {cs, stat},
  urldate = {2020-03-11},
  abstract = {Deep neural networks are composed of layers of parametrised linear operations intertwined with non linear activations. In basic models, such as the multi-layer perceptron, a linear layer operates on a simple input vector embedding of the instance being processed, and produces an output vector embedding by straight multiplication by a matrix parameter. In more complex models, the input and output are structured and their embeddings are higher order tensors. The parameter of each linear operation must then be controlled so as not to explode with the complexity of the structures involved. This is essentially the role of convolution models, which exist in many flavours dependent on the type of structure they deal with (grids, networks, time series etc.). We present here a unified framework which aims at capturing the essence of these diverse models, allowing a systematic analysis of their properties and their mutual enrichment. We also show that attention models naturally fit in the same framework: attention is convolution in which the structure itself is adaptive, and learnt, instead of being given a priori.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NX3Y5TBK\\Andreoli - 2020 - Convolution, attention and structure embedding.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7R44VIBA\\1905.html}
}

@article{andrychowicz_learning_2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and {de Freitas}, Nando},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.04474 [cs]},
  eprint = {1606.04474},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\Y4AHH3V4\Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient .pdf}
}

@misc{angelopoulos_gentle_2022,
  title = {A {{Gentle Introduction}} to {{Conformal Prediction}} and {{Distribution-Free Uncertainty Quantification}}},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen},
  year = {2022},
  month = dec,
  number = {arXiv:2107.07511},
  eprint = {2107.07511},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.07511},
  urldate = {2023-03-07},
  abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ITHAHT24\\Angelopoulos and Bates - 2022 - A Gentle Introduction to Conformal Prediction and .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\J4NCPRX6\\2107.html}
}

@article{angelopoulos_private_2022,
  title = {Private {{Prediction Sets}}},
  author = {Angelopoulos, Anastasios Nikolas and Bates, Stephen and Zrnic, Tijana and Jordan, Michael I.},
  year = {2022},
  month = apr,
  journal = {Harvard Data Science Review},
  doi = {10.1162/99608f92.16c71dad},
  urldate = {2023-03-07},
  abstract = {In real-world settings involving consequential decision-making, the deployment of machine learning systems generally requires both reliable uncertainty quantification and protection of individuals' privacy. We present a framework that treats these two desiderata jointly. Our framework is based on conformal prediction, a methodology that augments predictive models to return prediction sets that provide uncertainty quantification\textemdash they provably cover the true response with a user-specified probability, such as 90\%. One might hope that when used with privately trained models, conformal prediction would yield privacy guarantees for the resulting prediction sets; unfortunately this is not the case. To remedy this key problem, we develop a method that takes any pretrained predictive model and outputs differentially private prediction sets. Our method follows the general approach of split conformal prediction; we use holdout data to calibrate the size of the prediction sets but preserve privacy by using a privatized quantile subroutine. This subroutine compensates for the noise introduced to preserve privacy in order to guarantee correct coverage. We evaluate the method on large-scale computer vision data sets.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\Z6BVSPG2\Angelopoulos et al. - 2022 - Private Prediction Sets.pdf}
}

@inproceedings{aoki_luck_2017,
  title = {Luck Is {{Hard}} to {{Beat}}: {{The Difficulty}} of {{Sports Prediction}}},
  shorttitle = {Luck Is {{Hard}} to {{Beat}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Aoki, Raquel Y.S. and Assuncao, Renato M. and {Vaz de Melo}, Pedro O.S.},
  year = {2017},
  series = {{{KDD}} '17},
  pages = {1367--1376},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3097983.3098045},
  urldate = {2019-04-16},
  abstract = {Predicting the outcome of sports events is a hard task. We quantify this difficulty with a coefficient that measures the distance between the observed final results of sports leagues and idealized perfectly balanced competitions in terms of skill. This indicates the relative presence of luck and skill. We collected and analyzed all games from 198 sports leagues comprising 1503 seasons from 84 countries of 4 different sports: basketball, soccer, volleyball and handball. We measured the competitiveness by countries and sports. We also identify in each season which teams, if removed from its league, result in a completely random tournament. Surprisingly, not many of them are needed. As another contribution of this paper, we propose a probabilistic graphical model to learn about the teams' skills and to decompose the relative weights of luck and skill in each game. We break down the skill component into factors associated with the teams' characteristics. The model also allows to estimate as 0.36 the probability that an underdog team wins in the NBA league, with a home advantage adding 0.09 to this probability. As shown in the first part of the paper, luck is substantially present even in the most competitive championships, which partially explains why sophisticated and complex feature-based models hardly beat simple models in the task of forecasting sports' outcomes.},
  isbn = {978-1-4503-4887-4},
  keywords = {graphical model,sports analytics,uncertainty quantification},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\AKZFI44U\Aoki et al. - 2017 - Luck is Hard to Beat The Difficulty of Sports Pre.pdf}
}

@misc{aragon_scientific_2019,
  title = {Scientific Time Series and Deep Learning State of the Art: {{FrancisArgnR}}/{{Time-series---deep-learning---state-of-the-art}}},
  shorttitle = {Scientific Time Series and Deep Learning State of the Art},
  author = {Arag{\'o}n, Francisco},
  year = {2019},
  month = apr,
  urldate = {2019-05-01},
  annotation = {00000}
}

@article{ardia_forecasting_2018,
  title = {Forecasting Risk with {{Markov-switching GARCH}} Models:{{A}} Large-Scale Performance Study},
  shorttitle = {Forecasting Risk with {{Markov-switching GARCH}} Models},
  author = {Ardia, David and Bluteau, Keven and Boudt, Kris and Catania, Leopoldo},
  year = {2018},
  month = oct,
  journal = {International Journal of Forecasting},
  volume = {34},
  number = {4},
  pages = {733--747},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2018.05.004},
  urldate = {2019-03-19},
  abstract = {We perform a large-scale empirical study in order to compare the forecasting performances of single-regime and Markov-switching GARCH (MSGARCH) models from a risk management perspective. We find that MSGARCH models yield more accurate Value-at-Risk, expected shortfall, and left-tail distribution forecasts than their single-regime counterparts for daily, weekly, and ten-day equity log-returns. Also, our results indicate that accounting for parameter uncertainty improves the left-tail predictions, independently of the inclusion of the Markov-switching mechanism.},
  keywords = {Expected shortfall,Forecasting performance,GARCH,Large-scale study,MSGARCH,Risk management,Value-at-risk},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\V5RQV57A\\Ardia et al. - 2018 - Forecasting risk with Markov-switching GARCH model.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7DE24I7Q\\S0169207018300840.html}
}

@article{arik_tabnet_2019,
  title = {{{TabNet}}: {{Attentive Interpretable Tabular Learning}}},
  shorttitle = {{{TabNet}}},
  author = {Arik, Sercan O. and Pfister, Tomas},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.07442 [cs, stat]},
  eprint = {1908.07442},
  primaryclass = {cs, stat},
  urldate = {2019-09-07},
  abstract = {We propose a novel high-performance interpretable deep tabular data learning network, TabNet. TabNet utilizes a sequential attention mechanism to choose which features to reason from at each decision step and then aggregates the processed information towards the final decision. Explicit selection of sparse features enables more efficient learning as the model capacity at each decision step is fully utilized for the most relevant features, and also more interpretable decision making via visualization of selection masks. We demonstrate that TabNet outperforms other neural network and decision tree variants on a wide range of tabular data learning datasets while yielding interpretable feature attributions and insights into the global model behavior.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4HQCC29K\\Arik and Pfister - 2019 - TabNet Attentive Interpretable Tabular Learning.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZMMGSHBK\\1908.html}
}

@article{arumugam_deep_2019,
  title = {Deep {{Reinforcement Learning}} from {{Policy-Dependent Human Feedback}}},
  author = {Arumugam, Dilip and Lee, Jun Ki and Saskin, Sophie and Littman, Michael L.},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.04257 [cs, stat]},
  eprint = {1902.04257},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {To widen their accessibility and increase their utility, intelligent agents must be able to learn complex behaviors as specified by (non-expert) human users. Moreover, they will need to learn these behaviors within a reasonable amount of time while efficiently leveraging the sparse feedback a human trainer is capable of providing. Recent work has shown that human feedback can be characterized as a critique of an agent's current behavior rather than as an alternative reward signal to be maximized, culminating in the COnvergent Actor-Critic by Humans (COACH) algorithm for making direct policy updates based on human feedback. Our work builds on COACH, moving to a setting where the agent's policy is represented by a deep neural network. We employ a series of modifications on top of the original COACH algorithm that are critical for successfully learning behaviors from high-dimensional observations, while also satisfying the constraint of obtaining reduced sample complexity. We demonstrate the effectiveness of our Deep COACH algorithm in the rich 3D world of Minecraft with an agent that learns to complete tasks by mapping from raw pixels to actions using only real-time human feedback in 10-15 minutes of interaction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MYRR5QGC\\Arumugam et al. - 2019 - Deep Reinforcement Learning from Policy-Dependent .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\L24XILCZ\\1902.html}
}

@article{asadi_spatialtemporal_2019,
  title = {A {{Spatial-Temporal Decomposition Based Deep Neural Network}} for {{Time Series Forecasting}}},
  author = {Asadi, Reza and Regan, Amelia},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.00636 [cs, stat]},
  eprint = {1902.00636},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {Spatial time series forecasting problems arise in a broad range of applications, such as environmental and transportation problems. These problems are challenging because of the existence of specific spatial, short-term and long-term patterns, and the curse of dimensionality. In this paper, we propose a deep neural network framework for large-scale spatial time series forecasting problems. We explicitly designed the neural network architecture for capturing various types of patterns. In preprocessing, a time series decomposition method is applied to separately feed short-term, long-term and spatial patterns into different components of a neural network. A fuzzy clustering method finds cluster of neighboring time series based on similarity of time series residuals; as they can be meaningful short-term patterns for spatial time series. In neural network architecture, each kernel of a multi-kernel convolution layer is applied to a cluster of time series to extract short-term features in neighboring areas. The output of convolution layer is concatenated by trends and followed by convolution-LSTM layer to capture long-term patterns in larger regional areas. To make a robust prediction when faced with missing data, an unsupervised pretrained denoising autoencoder reconstructs the output of the model in a fine-tuning step. The experimental results illustrate the model outperforms baseline and state of the art models in a traffic flow prediction dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WB9NVXIM\\Asadi and Regan - 2019 - A Spatial-Temporal Decomposition Based Deep Neural.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\JG9TBPH2\\1902.html}
}

@article{ashouri_fast_2022,
  title = {Fast {{Forecast Reconciliation Using Linear Models}}},
  author = {Ashouri, Mahsa and Hyndman, Rob J and Shmueli, Galit},
  year = {2022},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {31},
  number = {1},
  pages = {263--282},
  issn = {1061-8600, 1537-2715},
  doi = {10.1080/10618600.2021.1939038},
  urldate = {2022-05-24},
  abstract = {Forecasting hierarchical or grouped time series usually involves two steps: computing base forecasts and reconciling the forecasts. Base forecasts can be computed by popular time series forecasting methods such as Exponential Smoothing (ETS) and Autoregressive Integrated Moving Average (ARIMA) models. The reconciliation step is a linear process that adjusts the base forecasts to ensure they are coherent. However using ETS or ARIMA for base forecasts can be computationally challenging when there are a large number of series to forecast, as each model must be numerically optimized for each series. We propose a linear model that avoids this computational problem and handles the forecasting and reconciliation in a single step. The proposed method is very flexible in incorporating external data, handling missing values and model selection. We illustrate our approach using two datasets: monthly Australian domestic tourism and daily Wikipedia pageviews. We compare our approach to reconciliation using ETS and ARIMA, and show that our approach is much faster while providing similar levels of forecast accuracy.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\32B9G6W6\Ashouri et al. - 2022 - Fast Forecast Reconciliation Using Linear Models.pdf}
}

@article{assaad_new_2008,
  title = {A New Boosting Algorithm for Improved Time-Series Forecasting with Recurrent Neural Networks},
  author = {Assaad, Mohammad and Bon{\'e}, Romuald and Cardot, Hubert},
  year = {2008},
  month = jan,
  journal = {Information Fusion},
  series = {Special {{Issue}} on {{Applications}} of {{Ensemble Methods}}},
  volume = {9},
  number = {1},
  pages = {41--55},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2006.10.009},
  urldate = {2019-06-27},
  abstract = {Ensemble methods for classification and regression have focused a great deal of attention in recent years. They have shown, both theoretically and empirically, that they are able to perform substantially better than single models in a wide range of tasks. We have adapted an ensemble method to the problem of predicting future values of time series using recurrent neural networks (RNNs) as base learners. The improvement is made by combining a large number of RNNs, each of which is generated by training on a different set of examples. This algorithm is based on the boosting algorithm where difficult points of the time series are concentrated on during the learning process however, unlike the original algorithm, we introduce a new parameter for tuning the boosting influence on available examples. We test our boosting algorithm for RNNs on single-step-ahead and multi-step-ahead prediction problems. The results are then compared to other regression methods, including those of different local approaches. The overall results obtained through our ensemble method are more accurate than those obtained through the standard method, backpropagation through time, on these datasets and perform significantly better even when long-range dependencies play an important role.},
  keywords = {Boosting,Learning algorithm,Multi-step-ahead prediction,Recurrent neural networks,Time series forecasting},
  annotation = {00111},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IQRYFNAU\\Assaad et al. - 2008 - A new boosting algorithm for improved time-series .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\A3ECGSZ3\\S1566253506000820.html}
}

@inproceedings{assaf_explainable_2019,
  title = {Explainable {{Deep Neural Networks}} for {{Multivariate Time Series Predictions}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Assaf, Roy and Schumann, Anika},
  year = {2019},
  month = aug,
  pages = {6488--6490},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Macao, China}},
  doi = {10.24963/ijcai.2019/932},
  urldate = {2019-10-06},
  abstract = {We demonstrate that CNN deep neural networks can not only be used for making predictions based on multivariate time series data, but also for explaining these predictions. This is important for a number of applications where predictions are the basis for decisions and actions. Hence, confidence in the prediction result is crucial. We design a two stage convolutional neural network architecture which uses particular kernel sizes. This allows us to utilise gradient based techniques for generating saliency maps for both the time dimension and the features. These are then used for explaining which features during which time interval are responsible for a given prediction, as well as explaining during which time intervals was the joint contribution of all features most important for that prediction. We demonstrate our approach for predicting the average energy production of photovoltaic power plants and for explaining these predictions.},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\T86VSBAU\Assaf and Schumann - 2019 - Explainable Deep Neural Networks for Multivariate .pdf}
}

@article{assimakopoulos_theta_2000,
  title = {The Theta Model: A Decomposition Approach to Forecasting},
  shorttitle = {The Theta Model},
  author = {Assimakopoulos, V. and Nikolopoulos, K.},
  year = {2000},
  month = oct,
  journal = {International Journal of Forecasting},
  series = {The {{M3- Competition}}},
  volume = {16},
  number = {4},
  pages = {521--530},
  issn = {0169-2070},
  doi = {10.1016/S0169-2070(00)00066-2},
  urldate = {2019-09-16},
  abstract = {This paper presents a new univariate forecasting method. The method is based on the concept of modifying the local curvature of the time-series through a coefficient `Theta' (the Greek letter \texttheta ), that is applied directly to the second differences of the data. The resulting series that are created maintain the mean and the slope of the original data but not their curvatures. These new time series are named Theta-lines. Their primary qualitative characteristic is the improvement of the approximation of the long-term behavior of the data or the augmentation of the short-term features, depending on the value of the Theta coefficient. The proposed method decomposes the original time series into two or more different Theta-lines. These are extrapolated separately and the subsequent forecasts are combined. The simple combination of two Theta-lines, the Theta=0 (straight line) and Theta=2 (double local curves) was adopted in order to produce forecasts for the 3003 series of the M3 competition. The method performed well, particularly for monthly series and for microeconomic data.},
  keywords = {M3-Competition,Time series,Univariate forecasting method},
  annotation = {00240},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6PBRQ3XI\\Assimakopoulos and Nikolopoulos - 2000 - The theta model a decomposition approach to forec.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\SPK4L7G2\\S0169207000000662.html}
}

@article{athanasopoulos_evaluation_,
  title = {On the Evaluation of Hierarchical Forecasts},
  author = {Athanasopoulos, George and Kourentzes, Nikolaos},
  pages = {23},
  abstract = {The aim of this note is to provide a thinking road-map and a practical guide to researchers and practitioners working on hierarchical forecasting problems. Evaluating the performance of hierarchical forecasts comes with new challenges stemming from both the statistical structure of the hierarchy and the application context. We discuss four relevant dimensions for researchers and analysts: the scale and units of time series, the issue of sparsity, the decision context and the importance of multiple evaluation windows. We conclude with a series of practical recommendations.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\XH3E4A4Z\Athanasopoulos and Kourentzes - On the evaluation of hierarchical forecasts.pdf}
}

@article{athanasopoulos_forecasting_2017,
  title = {Forecasting with Temporal Hierarchies},
  author = {Athanasopoulos, George and Hyndman, Rob J. and Kourentzes, Nikolaos and Petropoulos, Fotios},
  year = {2017},
  month = oct,
  journal = {European Journal of Operational Research},
  volume = {262},
  number = {1},
  pages = {60--74},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2017.02.046},
  urldate = {2021-02-04},
  abstract = {This paper introduces the concept of Temporal Hierarchies for time series forecasting. A temporal hierarchy can be constructed for any time series by means of non-overlapping temporal aggregation. Predictions constructed at all aggregation levels are combined with the proposed framework to result in temporally reconciled, accurate and robust forecasts. The implied combination mitigates modelling uncertainty, while the reconciled nature of the forecasts results in a unified prediction that supports aligned decisions at different planning horizons: from short-term operational up to long-term strategic planning. The proposed methodology is independent of forecasting models. It can embed high level managerial forecasts that incorporate complex and unstructured information with lower level statistical forecasts. Our results show that forecasting with temporal hierarchies increases accuracy over conventional forecasting, particularly under increased modelling uncertainty. We discuss organisational implications of the temporally reconciled forecasts using a case study of Accident \& Emergency departments.},
  langid = {english},
  keywords = {Forecast combination,Forecasting,Hierarchical forecasting,Reconciliation,Temporal aggregation},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JLLJMPLA\Athanasopoulos et al. - 2017 - Forecasting with temporal hierarchies.pdf}
}

@incollection{athanasopoulos_hierarchical_2020,
  title = {Hierarchical {{Forecasting}}},
  booktitle = {Macroeconomic {{Forecasting}} in the {{Era}} of {{Big Data}}},
  author = {Athanasopoulos, George and Gamakumara, Puwasala and Panagiotelis, Anastasios and Hyndman, Rob J. and Affan, Mohamed},
  editor = {Fuleky, Peter},
  year = {2020},
  volume = {52},
  pages = {689--719},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-31150-6_21},
  urldate = {2022-05-24},
  abstract = {Accurate forecasts of macroeconomic variables are crucial inputs into the decisions of economic agents and policy makers. Exploiting inherent aggregation structures of such variables, we apply forecast reconciliation methods to generate forecasts that are coherent with the aggregation constraints. We generate both point and probabilistic forecasts for the first time in the macroeconomic setting. Using Australian GDP we show that forecast reconciliation not only returns coherent forecasts but also improves the overall forecast accuracy in both point and probabilistic frameworks.},
  isbn = {978-3-030-31149-0 978-3-030-31150-6},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\UW7PAZ5G\Athanasopoulos et al. - 2020 - Hierarchical Forecasting.pdf}
}

@article{auguste_better_2020,
  title = {A Better Method to Enforce Monotonic Constraints in Regression and Classification Trees},
  author = {Auguste, Charles and Malory, Sean and Smirnov, Ivan},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.00986 [cs, stat]},
  eprint = {2011.00986},
  primaryclass = {cs, stat},
  urldate = {2021-10-11},
  abstract = {In this report we present two new ways of enforcing monotone constraints in regression and classification trees. One yields better results than the current LightGBM, and has a similar computation time. The other one yields even better results, but is much slower than the current LightGBM. We also propose a heuristic that takes into account that greedily splitting a tree by choosing a monotone split with respect to its immediate gain is far from optimal. Then, we compare the results with the current implementation of the constraints in the LightGBM library, using the well known Adult public dataset. Throughout the report, we mostly focus on the implementation of our methods that we made for the LightGBM library, even though they are general and could be implemented in any regression or classification tree. The best method we propose (a smarter way to split the tree coupled to a penalization of monotone splits) consistently beats the current implementation of LightGBM. With small or average trees, the loss reduction can be as high as 1\% in the early stages of training and decreases to around 0.1\% at the loss peak for the Adult dataset. The results would be even better with larger trees. In our experiments, we didn't do a lot of tuning of the regularization parameters, and we wouldn't be surprised to see that increasing the performance of our methods on test sets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3VZ8LKF6\\Auguste et al. - 2020 - A better method to enforce monotonic constraints i.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3886NKPE\\2011.html}
}

@article{avigad_formally_2017,
  title = {A {{Formally Verified Proof}} of the {{Central Limit Theorem}}},
  author = {Avigad, Jeremy and H{\"o}lzl, Johannes and Serafin, Luke},
  year = {2017},
  month = dec,
  journal = {Journal of Automated Reasoning},
  volume = {59},
  number = {4},
  pages = {389--423},
  issn = {0168-7433, 1573-0670},
  doi = {10.1007/s10817-017-9404-x},
  urldate = {2020-08-21},
  abstract = {We describe a proof of the Central Limit Theorem that has been formally verified in the Isabelle proof assistant. Our formalization builds upon and extends Isabelle's libraries for analysis and measure-theoretic probability. The proof of the theorem uses characteristic functions, which are a kind of Fourier transform, to demonstrate that, under suitable hypotheses, sums of random variables converge weakly to the standard normal distribution. We also discuss the libraries and infrastructure that supported the formalization, and reflect on some of the lessons we have learned from the effort.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JWAYBTFG\Avigad et al. - 2017 - A Formally Verified Proof of the Central Limit The.pdf}
}

@article{ba_layer_2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.06450 [cs, stat]},
  eprint = {1607.06450},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feedforward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BY4DBTPR\Ba et al. - 2016 - Layer Normalization.pdf}
}

@article{bader_improved_2004,
  title = {An Improved, Randomized Algorithm for Parallel Selection with an Experimental Study},
  author = {Bader, David A.},
  year = {2004},
  month = sep,
  journal = {Journal of Parallel and Distributed Computing},
  volume = {64},
  number = {9},
  pages = {1051--1059},
  issn = {07437315},
  doi = {10.1016/j.jpdc.2004.06.010},
  urldate = {2022-02-15},
  abstract = {A common statistical problem is that of finding the median element in a set of data. This paper presents an efficient randomized highlevel parallel algorithm for finding the median given a set of elements distributed across a parallel machine. In fact, our algorithm solves the general selection problem that requires the determination of the element of rank k, for an arbitrarily given integer k.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FZY8TILF\Bader - 2004 - An improved, randomized algorithm for parallel sel.pdf}
}

@article{bagnall_great_2016,
  title = {The {{Great Time Series Classification Bake Off}}: {{An Experimental Evaluation}} of {{Recently Proposed Algorithms}}. {{Extended Version}}},
  shorttitle = {The {{Great Time Series Classification Bake Off}}},
  author = {Bagnall, Anthony and Bostrom, Aaron and Large, James and Lines, Jason},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.01711 [cs]},
  eprint = {1602.01711},
  primaryclass = {cs},
  urldate = {2019-04-12},
  abstract = {In the last five years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only 9 of these algorithms are significantly more accurate than both benchmarks and that one classifier, the Collective of Transformation Ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more rigorous testing of new algorithms in the future.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read - not in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ADHLLAXZ\Bagnall et al. - 2016 - The Great Time Series Classification Bake Off An .pdf}
}

@article{bagnall_uea_2018,
  title = {The {{UEA}} Multivariate Time Series Classification Archive, 2018},
  author = {Bagnall, Anthony and Dau, Hoang Anh and Lines, Jason and Flynn, Michael and Large, James and Bostrom, Aaron and Southam, Paul and Keogh, Eamonn},
  year = {2018},
  month = oct,
  journal = {arXiv:1811.00075 [cs, stat]},
  eprint = {1811.00075},
  primaryclass = {cs, stat},
  urldate = {2019-04-12},
  abstract = {In 2002, the UCR time series classification archive was first released with sixteen datasets. It gradually expanded, until 2015 when it increased in size from 45 datasets to 85 datasets. In October 2018 more datasets were added, bringing the total to 128. The new archive contains a wide range of problems, including variable length series, but it still only contains univariate time series classification problems. One of the motivations for introducing the archive was to encourage researchers to perform a more rigorous evaluation of newly proposed time series classification (TSC) algorithms. It has worked: most recent research into TSC uses all 85 datasets to evaluate algorithmic advances. Research into multivariate time series classification, where more than one series are associated with each class label, is in a position where univariate TSC research was a decade ago. Algorithms are evaluated using very few datasets and claims of improvement are not based on statistical comparisons. We aim to address this problem by forming the first iteration of the MTSC archive, to be hosted at the website www.timeseriesclassification.com. Like the univariate archive, this formulation was a collaborative effort between researchers at the University of East Anglia (UEA) and the University of California, Riverside (UCR). The 2018 vintage consists of 30 datasets with a wide range of cases, dimensions and series lengths. For this first iteration of the archive we format all data to be of equal length, include no series with missing data and provide train/test splits.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\9ZT78GWB\Bagnall et al. - 2018 - The UEA multivariate time series classification ar.pdf}
}

@article{bahadori_temporalclustering_2019,
  title = {Temporal-{{Clustering Invariance}} in {{Irregular Healthcare Time Series}}},
  author = {Bahadori, Mohammad Taha and Lipton, Zachary Chase},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.12206 [cs, q-bio, stat]},
  eprint = {1904.12206},
  primaryclass = {cs, q-bio, stat},
  urldate = {2019-05-17},
  abstract = {Electronic records contain sequences of events, some of which take place all at once in a single visit, and others that are dispersed over multiple visits, each with a different timestamp. We postulate that fine temporal detail, e.g., whether a series of blood tests are completed at once or in rapid succession should not alter predictions based on this data. Motivated by this intuition, we propose models for analyzing sequences of multivariate clinical time series data that are invariant to this temporal clustering. We propose an efficient data augmentation technique that exploits the postulated temporal-clustering invariance to regularize deep neural networks optimized for several clinical prediction tasks. We introduce two techniques to temporally coarsen (downsample) irregular time series: (i) grouping the data points based on regularly-spaced timestamps; and (ii) clustering them, yielding irregularly-paced timestamps. Moreover, we propose a MultiResolution Ensemble (MRE) model, improving predictive accuracy by ensembling predictions based on inputs sequences transformed by different coarsening operators. Our experiments show that MRE improves the mAP on the benchmark mortality prediction task from 51.53\% to 53.92\%.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RM32QISW\Bahadori and Lipton - 2019 - Temporal-Clustering Invariance in Irregular Health.pdf}
}

@article{bahdanau_neural_2014,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  month = sep,
  journal = {arXiv:1409.0473 [cs, stat]},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder\textendash decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder\textendash decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LZAF2CQM\Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf}
}

@article{bahdanau_neural_2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  journal = {arXiv:1409.0473 [cs, stat]},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  urldate = {2020-01-14},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {10204},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6JNLDCNE\\Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\K8Q939FC\\1409.html}
}

@article{bai_empirical_2018,
  title = {An {{Empirical Evaluation}} of {{Generic Convolutional}} and {{Recurrent Networks}} for {{Sequence Modeling}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  year = {2018},
  month = apr,
  journal = {arXiv:1803.01271 [cs]},
  eprint = {1803.01271},
  primaryclass = {cs},
  urldate = {2019-11-19},
  abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\NVT4LBSW\Bai et al. - 2018 - An Empirical Evaluation of Generic Convolutional a.pdf}
}

@article{bai_panel_2009,
  title = {Panel {{Data Models With Interactive Fixed Effects}}},
  author = {Bai, Jushan},
  year = {2009},
  journal = {Econometrica},
  volume = {77},
  number = {4},
  pages = {1229--1279},
  issn = {1468-0262},
  doi = {10.3982/ECTA6135},
  urldate = {2020-02-24},
  abstract = {This paper considers large N and large T panel data models with unobservable multiple interactive effects, which are correlated with the regressors. In earnings studies, for example, workers' motivation, persistence, and diligence combined to influence the earnings in addition to the usual argument of innate ability. In macroeconomics, interactive effects represent unobservable common shocks and their heterogeneous impacts on cross sections. We consider identification, consistency, and the limiting distribution of the interactive-effects estimator. Under both large N and large T, the estimator is shown to be consistent, which is valid in the presence of correlations and heteroskedasticities of unknown form in both dimensions. We also derive the constrained estimator and its limiting distribution, imposing additivity coupled with interactive effects. The problem of testing additive versus interactive effects is also studied. In addition, we consider identification and estimation of models in the presence of a grand mean, time-invariant regressors, and common regressors. Given identification, the rate of convergence and limiting results continue to hold.},
  langid = {english},
  keywords = {Additive effects,bias-corrected estimator,common regressors,factor error structure,Hausman tests,interactive effects,time-invariant regressors},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\R3WWE8WP\\Bai - 2009 - Panel Data Models With Interactive Fixed Effects.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\F3II5JTT\\ECTA6135.html}
}

@inproceedings{bai_understanding_2021,
  title = {Understanding the {{Under-Coverage Bias}} in {{Uncertainty Estimation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bai, Yu and Mei, Song and Wang, Huan and Xiong, Caiming},
  year = {2021},
  volume = {34},
  pages = {18307--18319},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-07},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\32L7FXY3\Bai et al. - 2021 - Understanding the Under-Coverage Bias in Uncertain.pdf}
}

@article{balduzzi_mechanics_2018,
  title = {The {{Mechanics}} of N-{{Player Differentiable Games}}},
  author = {Balduzzi, David and Racaniere, Sebastien and Martens, James and Foerster, Jakob and Tuyls, Karl and Graepel, Thore},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.05642 [cs]},
  eprint = {1802.05642},
  primaryclass = {cs},
  urldate = {2019-04-16},
  abstract = {The cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood \textendash{} and is becoming increasingly important as adversarial and multiobjective architectures proliferate. In this paper, we develop new techniques to understand and control the dynamics in general games. The key result is to decompose the second-order dynamics into two components. The first is related to potential games, which reduce to gradient descent on an implicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in general games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs \textendash{} whilst at the same time being applicable to \textendash{} and having guarantees in \textendash{} much more general games.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6XF5K3L9\Balduzzi et al. - 2018 - The Mechanics of n-Player Differentiable Games.pdf}
}

@article{balestriero_neural_2017,
  title = {Neural {{Decision Trees}}},
  author = {Balestriero, Randall},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.07360 [cs, stat]},
  eprint = {1702.07360},
  primaryclass = {cs, stat},
  urldate = {2019-08-30},
  abstract = {In this paper we propose a synergistic melting of neural networks and decision trees (DT) we call neural decision trees (NDT). NDT is an architecture a la decision tree where each splitting node is an independent multilayer perceptron allowing oblique decision functions or arbritrary nonlinear decision function if more than one layer is used. This way, each MLP can be seen as a node of the tree. We then show that with the weight sharing asumption among those units, we end up with a Hashing Neural Network (HNN) which is a multilayer perceptron with sigmoid activation function for the last layer as opposed to the standard softmax. The output units then jointly represent the probability to be in a particular region. The proposed framework allows for global optimization as opposed to greedy in DT and differentiability w.r.t. all parameters and the input, allowing easy integration in any learnable pipeline, for example after CNNs for computer vision tasks. We also demonstrate the modeling power of HNN allowing to learn union of disjoint regions for final clustering or classification making it more general and powerful than standard softmax MLP requiring linear separability thus reducing the need on the inner layer to perform complex data transformations. We finally show experiments for supervised, semi-suppervised and unsupervised tasks and compare results with standard DTs and MLPs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2BT3CVYN\Balestriero - 2017 - Neural Decision Trees.pdf}
}

@article{bandara_sales_2019,
  title = {Sales {{Demand Forecast}} in {{E-commerce}} Using a {{Long Short-Term Memory Neural Network Methodology}}},
  author = {Bandara, Kasun and Shi, Peibei and Bergmeir, Christoph and Hewamalage, Hansika and Tran, Quoc and Seaman, Brian},
  year = {2019},
  month = aug,
  journal = {arXiv:1901.04028 [cs, stat]},
  eprint = {1901.04028},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {Generating accurate and reliable sales forecasts is crucial in the E-commerce business. The current state-of-the-art techniques are typically univariate methods, which produce forecasts considering only the historical sales data of a single product. However, in a situation where large quantities of related time series are available, conditioning the forecast of an individual time series on past behaviour of similar, related time series can be beneficial. Since the product assortment hierarchy in an E-commerce platform contains large numbers of related products, in which the sales demand patterns can be correlated, our attempt is to incorporate this cross-series information in a unified model. We achieve this by globally training a Long Short-Term Memory network (LSTM) that exploits the non-linear demand relationships available in an E-commerce product assortment hierarchy. Aside from the forecasting framework, we also propose a systematic pre-processing framework to overcome the challenges in the E-commerce business. We also introduce several product grouping strategies to supplement the LSTM learning schemes, in situations where sales patterns in a product portfolio are disparate. We empirically evaluate the proposed forecasting framework on a real-world online marketplace dataset from Walmart.com. Our method achieves competitive results on category level and super-departmental level datasets, outperforming state-of-the-art techniques.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\KVNDVXEE\Bandara et al. - 2019 - Sales Demand Forecast in E-commerce using a Long S.pdf}
}

@misc{barber_conformal_2023,
  title = {Conformal Prediction beyond Exchangeability},
  author = {Barber, Rina Foygel and Candes, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
  year = {2023},
  month = feb,
  number = {arXiv:2202.13415},
  eprint = {2202.13415},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.13415},
  urldate = {2023-03-07},
  abstract = {Conformal prediction is a popular, modern technique for providing valid predictive inference for arbitrary machine learning models. Its validity relies on the assumptions of exchangeability of the data, and symmetry of the given model fitting algorithm as a function of the data. However, exchangeability is often violated when predictive models are deployed in practice. For example, if the data distribution drifts over time, then the data points are no longer exchangeable; moreover, in such settings, we might want to use a nonsymmetric algorithm that treats recent observations as more relevant. This paper generalizes conformal prediction to deal with both aspects: we employ weighted quantiles to introduce robustness against distribution drift, and design a new randomization technique to allow for algorithms that do not treat data points symmetrically. Our new methods are provably robust, with substantially less loss of coverage when exchangeability is violated due to distribution drift or other challenging features of real data, while also achieving the same coverage guarantees as existing conformal prediction methods if the data points are in fact exchangeable. We demonstrate the practical utility of these new tools with simulations and real-data experiments on electricity and election forecasting.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9Y2WU6LY\\Barber et al. - 2023 - Conformal prediction beyond exchangeability.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9GQA2JVK\\2202.html}
}

@article{barigozzi_quasi_2019,
  title = {Quasi {{Maximum Likelihood Estimation}} and {{Inference}} of {{Large Approximate Dynamic Factor Models}} via the {{EM}} Algorithm},
  author = {Barigozzi, Matteo and Luciani, Matteo},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.03821 [econ, math, stat]},
  eprint = {1910.03821},
  primaryclass = {econ, math, stat},
  urldate = {2020-02-27},
  abstract = {This paper studies Quasi Maximum Likelihood estimation of dynamic factor models for large panels of time series. Specifically, we consider the case in which the autocorrelation of the factors is explicitly accounted for and therefore the factor model has a state-space form. Estimation of the factors and their loadings is implemented by means of the Expectation Maximization algorithm, jointly with the Kalman smoother. We prove that, as both the dimension of the panel \$n\$ and the sample size \$T\$ diverge to infinity, the estimated loadings, factors, and common components are \$\textbackslash min(\textbackslash sqrt n,\textbackslash sqrt T)\$-consistent and asymptotically normal. Although the model is estimated under the unrealistic constraint of independent idiosyncratic errors, this mis-specification does not affect consistency. Moreover, we give conditions under which the derived asymptotic distribution can still be used for inference even in case of mis-specifications. Our results are confirmed by a MonteCarlo simulation exercise where we compare the performance of our estimators with Principal Components.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics,Mathematics - Statistics Theory},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\SV5SKBFJ\\Barigozzi and Luciani - 2019 - Quasi Maximum Likelihood Estimation and Inference .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\H2P8EJY3\\1910.html}
}

@article{bastings_interpretable_2019,
  title = {Interpretable {{Neural Predictions}} with {{Differentiable Binary Variables}}},
  author = {Bastings, Joost and Aziz, Wilker and Titov, Ivan},
  year = {2019},
  month = may,
  journal = {arXiv:1905.08160 [cs]},
  eprint = {1905.08160},
  primaryclass = {cs},
  urldate = {2019-11-12},
  abstract = {The success of neural networks comes hand in hand with a desire for more interpretability. We focus on text classifiers and make them more interpretable by having them provide a justification, a rationale, for their predictions. We approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classifier that learns from the words in the rationale alone. Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as L0 regularisation. We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE. In our formulation, we can tractably compute the expected value of penalties such as L0, which allows us to directly optimise the model towards a pre-specified text selection rate. We show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {00003},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2LZMBJGU\Bastings et al. - 2019 - Interpretable Neural Predictions with Differentiab.pdf}
}

@article{bengio_decision_2010,
  title = {Decision {{Trees Do Not Generalize}} to {{New Variations}}},
  author = {Bengio, Yoshua and Delalleau, Olivier and Simard, Clarence},
  year = {2010},
  journal = {Computational Intelligence},
  volume = {26},
  number = {4},
  pages = {449--467},
  issn = {1467-8640},
  doi = {10.1111/j.1467-8640.2010.00366.x},
  urldate = {2019-08-30},
  abstract = {The family of decision tree learning algorithms is among the most widespread and studied. Motivated by the desire to develop learning algorithms that can generalize when learning highly varying functions such as those presumably needed to achieve artificial intelligence, we study some theoretical limitations of decision trees. We demonstrate formally that they can be seriously hurt by the curse of dimensionality in a sense that is a bit different from other nonparametric statistical methods, but most importantly, that they cannot generalize to variations not seen in the training set. This is because a decision tree creates a partition of the input space and needs at least one example in each of the regions associated with a leaf to make a sensible prediction in that region. A better understanding of the fundamental reasons for this limitation suggests that one should use forests or even deeper architectures instead of trees, which provide a form of distributed representation and can generalize to variations not encountered in the training data.},
  copyright = {\textcopyright{} 2010 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {curse of dimensionality,decision trees,parity function},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WIZTRIAD\j.1467-8640.2010.00366.html}
}

@article{bengio_deep_,
  title = {Deep {{Learning}} of {{Representations}} for {{Unsupervised}} and {{Transfer Learning}}},
  author = {Bengio, Yoshua},
  pages = {21},
  abstract = {Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higherlevel representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution P (x) is structurally related to some task of interest, say predicting P (y|x). This paper focuses on the context of the Unsupervised and Transfer Learning Challenge, on why unsupervised pre-training of representations can be useful, and how it can be exploited in the transfer learning scenario, where we care about predictions on examples that are not from the same distribution as the training distribution.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6P9R4PSK\Bengio - Deep Learning of Representations for Unsupervised .pdf}
}

@article{benidis_deep_2023,
  title = {Deep {{Learning}} for {{Time Series Forecasting}}: {{Tutorial}} and {{Literature Survey}}},
  shorttitle = {Deep {{Learning}} for {{Time Series Forecasting}}},
  author = {Benidis, Konstantinos and Rangapuram, Syama Sundar and Flunkert, Valentin and Wang, Yuyang and Maddix, Danielle and Turkmen, Caner and Gasthaus, Jan and {Bohlke-Schneider}, Michael and Salinas, David and Stella, Lorenzo and Aubet, Francois-Xavier and Callot, Laurent and Januschowski, Tim},
  year = {2023},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {6},
  eprint = {2004.10240},
  primaryclass = {cs, stat},
  pages = {1--36},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3533382},
  urldate = {2023-05-31},
  abstract = {Deep learning based forecasting methods have become the methods of choice in many applications of time series prediction or forecasting often outperforming other approaches. Consequently, over the last years, these methods are now ubiquitous in large-scale industrial forecasting applications and have consistently ranked among the best entries in forecasting competitions (e.g., M4 and M5). This practical success has further increased the academic interest to understand and improve deep forecasting methods. In this article we provide an introduction and overview of the field: We present important building blocks for deep forecasting in some depth; using these building blocks, we then survey the breadth of the recent deep forecasting literature.},
  archiveprefix = {arxiv},
  keywords = {A.1,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HQ6JHLN4\\Benidis et al. - 2023 - Deep Learning for Time Series Forecasting Tutoria.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Z864YHQS\\2004.html}
}

@article{bentaieb_gradient_2014,
  title = {A Gradient Boosting Approach to the {{Kaggle}} Load Forecasting Competition},
  author = {Ben Taieb, Souhaib and Hyndman, Rob J.},
  year = {2014},
  month = apr,
  journal = {International Journal of Forecasting},
  volume = {30},
  number = {2},
  pages = {382--394},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2013.07.005},
  urldate = {2019-08-06},
  abstract = {We describe and analyse the approach used by Team TinTin (Souhaib Ben Taieb and Rob J Hyndman) in the Load Forecasting track of the Kaggle Global Energy Forecasting Competition 2012. The competition involved a hierarchical load forecasting problem for a US utility with 20 geographical zones. The data available consisted of the hourly loads for the 20 zones and hourly temperatures from 11 weather stations, for four and a half years. For each zone, the hourly electricity loads for nine different weeks needed to be predicted without having the locations of either the zones or stations. We used separate models for each hourly period, with component-wise gradient boosting for estimating each model using univariate penalised regression splines as base learners. The models allow for the electricity demand changing with the time-of-year, day-of-week, time-of-day, and on public holidays, with the main predictors being current and past temperatures, and past demand. Team TinTin ranked fifth out of 105 participating teams.},
  keywords = {Additive models,Gradient boosting,Kaggle competition,Machine learning,Multi-step forecasting,Short-term load forecasting},
  annotation = {00120},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NLAX3HVH\\Ben Taieb and Hyndman - 2014 - A gradient boosting approach to the Kaggle load fo.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GFW3UTBJ\\S0169207013000812.html}
}

@inproceedings{bentaieb_regularized_2019,
  title = {Regularized {{Regression}} for {{Hierarchical Forecasting Without Unbiasedness Conditions}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Ben Taieb, Souhaib and Koo, Bonsoo},
  year = {2019},
  month = jul,
  pages = {1337--1347},
  publisher = {{ACM}},
  address = {{Anchorage AK USA}},
  doi = {10.1145/3292500.3330976},
  urldate = {2021-10-06},
  abstract = {Forecasting a large set of time series with hierarchical aggregation constraints is a central problem for many organizations. However, it is particularly challenging to forecast these hierarchical structures. In fact, it requires not only good forecast accuracy at each level of the hierarchy, but also the coherency between di\dbend erent levels, i.e. the forecasts should satisfy the hierarchical aggregation constraints. Given some incoherent base forecasts, the state-of-the-art methods compute revised forecasts based on forecast combination which ensures that the aggregation constraints are satis\dbend ed. However, these methods assume the base forecasts are unbiased and constrain the revised forecasts to be also unbiased. We propose a new forecasting method which relaxes these unbiasedness conditions, and seeks the revised forecasts with the best tradeo\dbend{} between bias and forecast variance. We also present a regularization method which allows us to deal with high-dimensional hierarchies, and provide its theoretical justi\dbend cation. Finally, we compare the proposed method with the state-of-the-art methods both theoretically and empirically. The results on both simulated and real-world data indicate that our methods provide competitive results compared to the state-of-the-art methods.},
  isbn = {978-1-4503-6201-6},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RN83WH4V\Ben Taieb and Koo - 2019 - Regularized Regression for Hierarchical Forecastin.pdf}
}

@article{bentaieb_review_2012,
  title = {A Review and Comparison of Strategies for Multi-Step Ahead Time Series Forecasting Based on the {{NN5}} Forecasting Competition},
  author = {Ben Taieb, Souhaib and Bontempi, Gianluca and Atiya, Amir F. and Sorjamaa, Antti},
  year = {2012},
  month = jun,
  journal = {Expert Systems with Applications},
  volume = {39},
  number = {8},
  pages = {7067--7083},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2012.01.039},
  urldate = {2019-04-25},
  abstract = {Multi-step ahead forecasting is still an open challenge in time series forecasting. Several approaches that deal with this complex problem have been proposed in the literature but an extensive comparison on a large number of tasks is still missing. This paper aims to fill this gap by reviewing existing strategies for multi-step ahead forecasting and comparing them in theoretical and practical terms. To attain such an objective, we performed a large scale comparison of these different strategies using a large experimental benchmark (namely the 111 series from the NN5 forecasting competition). In addition, we considered the effects of deseasonalization, input variable selection, and forecast combination on these strategies and on multi-step ahead forecasting at large. The following three findings appear to be consistently supported by the experimental results: Multiple-Output strategies are the best performing approaches, deseasonalization leads to uniformly improved forecast accuracy, and input selection is more effective when performed in conjunction with deseasonalization.},
  keywords = {Friedman test,Lazy Learning,Long-term forecasting,Machine learning,Multi-step ahead forecasting,NN5 forecasting competition,read - in related work,Strategies of forecasting,Time series forecasting},
  annotation = {00196},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\392XKP53\\Ben Taieb et al. - 2012 - A review and comparison of strategies for multi-st.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GRVQAT9J\\S0957417412000528.html}
}

@incollection{berglund_bidirectional_2015,
  title = {Bidirectional {{Recurrent Neural Networks}} as {{Generative Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Berglund, Mathias and Raiko, Tapani and Honkala, Mikko and K{\"a}rkk{\"a}inen, Leo and Vetek, Akos and Karhunen, Juha T},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {856--864},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IW2N6MTY\\Berglund et al. - 2015 - Bidirectional Recurrent Neural Networks as Generat.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YUS8JS4M\\5651-bidirectional-recurrent-neural-networks-as-generative-models.html}
}

@article{bergmeir_use_2012,
  title = {On the Use of Cross-Validation for Time Series Predictor Evaluation},
  author = {Bergmeir, Christoph and Ben{\'i}tez, Jos{\'e} M.},
  year = {2012},
  month = may,
  journal = {Information Sciences},
  volume = {191},
  pages = {192--213},
  issn = {00200255},
  doi = {10.1016/j.ins.2011.12.028},
  urldate = {2019-04-10},
  abstract = {In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand. In traditional forecasting, it is common practice to reserve a part from the end of each time series for testing, and to use the rest of the series for training. Thus it is not made full use of the data, but theoretical problems with respect to temporal evolutionary effects and dependencies within the data as well as practical problems regarding missing values are eliminated. On the other hand, when evaluating machine learning and other regression methods used for time series forecasting, often cross-validation is used for evaluation, paying little attention to the fact that those theoretical problems invalidate the fundamental assumptions of cross-validation. To close this gap and examine the consequences of different model selection procedures in practice, we have developed a rigorous and extensive empirical study. Six different model selection procedures, based on (i) cross-validation and (ii) evaluation using the series' last part, are used to assess the performance of four machine learning and other regression techniques on synthetic and real-world time series. No practical consequences of the theoretical flaws were found during our study, but the use of crossvalidation techniques led to a more robust model selection. To make use of the ``best of both worlds'', we suggest that the use of a blocked form of cross-validation for time series evaluation became the standard procedure, thus using all available information and circumventing the theoretical problems.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\GDN3UL4S\Bergmeir and BenÃ­tez - 2012 - On the use of cross-validation for time series pre.pdf}
}

@inproceedings{berliner_hierarchical_1996,
  title = {Hierarchical {{Bayesian Time Series Models}}},
  booktitle = {Maximum {{Entropy}} and {{Bayesian Methods}}},
  author = {Berliner, L. Mark},
  editor = {Hanson, Kenneth M. and Silver, Richard N.},
  year = {1996},
  series = {Fundamental {{Theories}} of {{Physics}}},
  pages = {15--22},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-011-5430-7_3},
  abstract = {Notions of Bayesian analysis are reviewed, with emphasis on Bayesian modeling and Bayesian calculation. A general hierarchical model for time series analysis is then presented and discussed. Both discrete time and continuous time formulations are discussed. An brief overview of generalizations of the fundamental hierarchical time series model concludes the article.},
  isbn = {978-94-011-5430-7},
  langid = {english},
  keywords = {Dynamical model,Fokker-Planck equation,Markov process,Prediction,Stochastic differential equation}
}

@article{berry_probabilistic_2020,
  title = {Probabilistic Forecasting of Heterogeneous Consumer Transaction\textendash Sales Time Series},
  author = {Berry, Lindsay R. and Helman, Paul and West, Mike},
  year = {2020},
  month = apr,
  journal = {International Journal of Forecasting},
  volume = {36},
  number = {2},
  pages = {552--569},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.07.007},
  urldate = {2020-05-15},
  abstract = {We present new Bayesian methodology for consumer sales forecasting. Focusing on the multi-step-ahead forecasting of daily sales of many supermarket items, we adapt dynamic count mixture models for forecasting individual customer transactions, and introduce novel dynamic binary cascade models for predicting counts of items per transaction. These transaction\textendash sales models can incorporate time-varying trends, seasonality, price, promotion, random effects and other outlet-specific predictors for individual items. Sequential Bayesian analysis involves fast, parallel filtering on sets of decoupled items, and is adaptable across items that may exhibit widely-varying characteristics. A multi-scale approach enables information to be shared across items with related patterns over time in order to improve prediction, while maintaining the scalability to many items. A motivating case study in many-item, multi-period, multi-step-ahead supermarket sales forecasting provides examples that demonstrate an improved forecast accuracy on multiple metrics, and illustrates the benefits of full probabilistic models for forecast accuracy evaluation and comparison.},
  langid = {english},
  keywords = {Bayesian forecasting,Decouple/recouple,Dynamic binary cascade,Forecast calibration,Intermittent demand,Multi-scale forecasting,Predicting rare events,Sales per transaction,Supermarket sales forecasting},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7D529KW2\\Berry et al. - 2020 - Probabilistic forecasting of heterogeneous consume.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\2B2TW7XX\\S0169207019302055.html}
}

@article{bhanja_impact_2018,
  title = {Impact of {{Data Normalization}} on {{Deep Neural Network}} for {{Time Series Forecasting}}},
  author = {Bhanja, Samit and Das, Abhishek},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.05519 [cs, stat]},
  eprint = {1812.05519},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {For the last few years it has been observed that the Deep Neural Networks (DNNs) has achieved an excellent success in image classification, speech recognition. But DNNs are suffer great deal of challenges for time series forecasting because most of the time series data are nonlinear in nature and highly dynamic in behaviour. The time series forecasting has a great impact on our socio-economic environment. Hence, to deal with these challenges its need to be redefined the DNN model and keeping this in mind, data pre-processing, network architecture and network parameters are need to be consider before feeding the data into DNN models. Data normalization is the basic data pre-processing technique form which learning is to be done. The effectiveness of time series forecasting is heavily depend on the data normalization technique. In this paper, different normalization methods are used on time series data before feeding the data into the DNN model and we try to find out the impact of each normalization technique on DNN to forecast the time series. Here the Deep Recurrent Neural Network (DRNN) is used to predict the closing index of Bombay Stock Exchange (BSE) and New York Stock Exchange (NYSE) by using BSE and NYSE time series data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - not in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XIZTIBBZ\\Bhanja and Das - 2018 - Impact of Data Normalization on Deep Neural Networ.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MALYLJBM\\1812.html}
}

@article{biau_accelerated_2018,
  title = {Accelerated {{Gradient Boosting}}},
  author = {Biau, G{\'e}rard and Cadre, Beno{\^i}t and Rouv{\`i}{\`e}re, Laurent},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.02042 [cs, stat]},
  eprint = {1803.02042},
  primaryclass = {cs, stat},
  urldate = {2019-07-16},
  abstract = {Gradient tree boosting is a prediction algorithm that sequentially produces a model in the form of linear combinations of decision trees, by solving an infinite-dimensional optimization problem. We combine gradient boosting and Nesterov's accelerated descent to design a new algorithm, which we call AGB (for Accelerated Gradient Boosting). Substantial numerical evidence is provided on both synthetic and real-life data sets to assess the excellent performance of the method in a large variety of prediction problems. It is empirically shown that AGB is much less sensitive to the shrinkage parameter and outputs predictors that are considerably more sparse in the number of trees, while retaining the exceptional performance of gradient boosting.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00009},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\DHIQ9QA2\Biau et al. - 2018 - Accelerated Gradient Boosting.pdf}
}

@article{binkowski_autoregressive_2018,
  title = {Autoregressive {{Convolutional Neural}}  {{Networks}} for {{Asynchronous Time Series}}},
  author = {Binkowski, Miko{\l}aj and Marti, Gautier and Donnat, Philippe},
  year = {2018},
  journal = {Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.},
  pages = {10},
  abstract = {We propose Significance-Offset Convolutional Neural Network, a deep convolutional network architecture for regression of multivariate asynchronous time series. The model is inspired by standard autoregressive (AR) models and gating mechanisms used in recurrent neural networks. It involves an AR-like weighting system, where the final predictor is obtained as a weighted sum of adjusted regressors, while the weights are datadependent functions learnt through a convolutional network. The architecture was designed for applications on asynchronous time series and is evaluated on such datasets: a hedge fund proprietary dataset of over 2 million quotes for a credit derivative index, an artificially generated noisy autoregressive series and UCI household electricity consumption dataset. The proposed architecture achieves promising results as compared to convolutional and recurrent neural networks.},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\CG3LJL2F\Binkowski et al. - Autoregressive Convolutional Neural  Networks for .pdf}
}

@article{bocquet_bayesian_2020,
  title = {Bayesian Inference of Dynamics from Partial and Noisy Observations Using Data Assimilation and Machine Learning},
  author = {Bocquet, Marc and Brajard, Julien and Carrassi, Alberto and Bertino, Laurent},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.06270 [physics, stat]},
  eprint = {2001.06270},
  primaryclass = {physics, stat},
  urldate = {2020-02-20},
  abstract = {The reconstruction from observations of high-dimensional chaotic dynamics such as geophysical flows is hampered by (i) the partial and noisy observations that can realistically be obtained, (ii) the need to learn from long time series of data, and (iii) the unstable nature of the dynamics. To achieve such inference from the observations over long time series, it has been suggested to combine data assimilation and machine learning in several ways. We show how to unify these approaches from a Bayesian perspective using expectation-maximization and coordinate descents. Implementations and approximations of these methods are also discussed. Finally, we numerically and successfully test the approach on two relevant low-order chaotic models with distinct identifiability.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Physics - Atmospheric and Oceanic Physics,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\AUINYT3L\\Bocquet et al. - 2020 - Bayesian inference of dynamics from partial and no.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\79NZP63M\\2001.html}
}

@misc{bohlke-schneider_resilient_2022,
  title = {Resilient {{Neural Forecasting Systems}}},
  author = {{Bohlke-Schneider}, Michael and Kapoor, Shubham and Januschowski, Tim},
  year = {2022},
  month = mar,
  number = {arXiv:2203.08492},
  eprint = {2203.08492},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.08492},
  urldate = {2023-05-30},
  abstract = {Industrial machine learning systems face data challenges that are often under-explored in the academic literature. Common data challenges are data distribution shifts, missing values and anomalies. In this paper, we discuss data challenges and solutions in the context of a Neural Forecasting application on labor planning.We discuss how to make this forecasting system resilient to these data challenges. We address changes in data distribution with a periodic retraining scheme and discuss the critical importance of model stability in this setting. Furthermore, we show how our deep learning model deals with missing values natively without requiring imputation. Finally, we describe how we detect anomalies in the input data and mitigate their effect before they impact the forecasts. This results in a fully autonomous forecasting system that compares favorably to a hybrid system consisting of the algorithm and human overrides.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\H3AYUPFQ\\Bohlke-Schneider et al. - 2022 - Resilient Neural Forecasting Systems.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3JI66YIH\\2203.html}
}

@techreport{boivin_understanding_2005,
  title = {Understanding and {{Comparing Factor-Based Forecasts}}},
  author = {Boivin, Jean and Ng, Serena},
  year = {2005},
  month = may,
  number = {w11285},
  pages = {w11285},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w11285},
  urldate = {2020-03-13},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JIBMUENU\Boivin and Ng - 2005 - Understanding and Comparing Factor-Based Forecasts.pdf}
}

@article{bojanowski_unsupervised_2017,
  title = {Unsupervised {{Learning}} by {{Predicting Noise}}},
  author = {Bojanowski, Piotr and Joulin, Armand},
  year = {2017},
  month = apr,
  journal = {arXiv:1704.05310 [cs, stat]},
  eprint = {1704.05310},
  primaryclass = {cs, stat},
  urldate = {2019-05-10},
  abstract = {Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision. This paper introduces a generic framework to train deep networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with state-of-the-art unsupervised methods on ImageNet and PASCAL VOC.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00061},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7YLPVFEP\Bojanowski and Joulin - 2017 - Unsupervised Learning by Predicting Noise.pdf}
}

@article{bollobas_parallel_1990,
  title = {Parallel {{Selection}} with {{High Probability}}},
  author = {Bollob{\'a}s, B{\'e}la and Brightwell, Graham},
  year = {1990},
  month = feb,
  journal = {SIAM Journal on Discrete Mathematics},
  volume = {3},
  number = {1},
  pages = {21--31},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0895-4801},
  doi = {10.1137/0403003},
  urldate = {2022-02-15},
  abstract = {Given a set of n elements in some unknown order, parallel comparison algorithms to select the tth highest with probability \$1- o(1)\$ as \$n \textbackslash to \textbackslash infty \$ are considered, where each order is assumed to be equally likely. Such an algorithm is given using four rounds and \$cn\$ comparisons per round, and it is shown that no such algorithm exists using three rounds and \$cn\$ comparisons per round.},
  keywords = {68E05,median,parallel comparison algorithms,selecting}
}

@article{bonneel_wasserstein_2016,
  title = {Wasserstein Barycentric Coordinates: Histogram Regression Using Optimal Transport},
  shorttitle = {Wasserstein Barycentric Coordinates},
  author = {Bonneel, Nicolas and Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2016},
  month = jul,
  journal = {ACM Transactions on Graphics},
  volume = {35},
  number = {4},
  pages = {1--10},
  issn = {07300301},
  doi = {10.1145/2897824.2925918},
  urldate = {2019-10-02},
  abstract = {This article defines a new way to perform intuitive and geometrically faithful regressions on histogram-valued data. It leverages the theory of optimal transport, and in particular the definition of Wasserstein barycenters, to introduce for the first time the notion of barycentric coordinates for histograms. These coordinates take into account the underlying geometry of the ground space on which the histograms are defined, and are thus particularly meaningful for applications in graphics to shapes, color or material modification. Beside this abstract construction, we propose a fast numerical optimization scheme to solve this backward problem (finding the barycentric coordinates of a given histogram) with a low computational overhead with respect to the forward problem (computing the barycenter). This scheme relies on a backward algorithmic differentiation of the Sinkhorn algorithm which is used to optimize the entropic regularization of Wasserstein barycenters. We showcase an illustrative set of applications of these Wasserstein coordinates to various problems in computer graphics: shape approximation, BRDF acquisition and color editing.},
  langid = {english},
  annotation = {00047},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\SXYZMK5S\Bonneel et al. - 2016 - Wasserstein barycentric coordinates histogram reg.pdf}
}

@article{boone_forecasting_2019,
  title = {Forecasting Sales in the Supply Chain: {{Consumer}} Analytics in the Big Data Era},
  shorttitle = {Forecasting Sales in the Supply Chain},
  author = {Boone, Tonya and Ganeshan, Ram and Jain, Aditya and Sanders, Nada R.},
  year = {2019},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {Special {{Section}}: {{Supply Chain Forecasting}}},
  volume = {35},
  number = {1},
  pages = {170--180},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2018.09.003},
  urldate = {2019-03-19},
  abstract = {Forecasts have traditionally served as the basis for planning and executing supply chain activities. Forecasts drive supply chain decisions, and they have become critically important due to increasing customer expectations, shortening lead times, and the need to manage scarce resources. Over the last ten years, advances in technology and data collection systems have resulted in the generation of huge volumes of data on a wide variety of topics and at great speed. This paper reviews the impact that this explosion of data is having on product forecasting and how it is improving it. While much of this review will focus on time series data, we will also explore how such data can be used to obtain insights into consumer behavior, and the impact of such data on organizational forecasting.},
  keywords = {Big data,Literature review,Predictive analytics,Sales forecasting},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZRSFYPIV\\Boone et al. - 2019 - Forecasting sales in the supply chain Consumer an.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Q9C6HY33\\S0169207018301523.html}
}

@article{boone_perspectives_2019,
  title = {Perspectives on Supply Chain Forecasting},
  author = {Boone, Tonya and Boylan, John E. and Fildes, Robert and Ganeshan, Ram and Sanders, Nada},
  year = {2019},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {Special {{Section}}: {{Supply Chain Forecasting}}},
  volume = {35},
  number = {1},
  pages = {121--127},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2018.11.002},
  urldate = {2019-03-19},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Q7N8MMMV\\Boone et al. - 2019 - Perspectives on supply chain forecasting.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4YAZSFZF\\S0169207018301638.html}
}

@techreport{borovykh_dilated_2018,
  type = {{{SSRN Scholarly Paper}}},
  title = {Dilated {{Convolutional Neural Networks}} for {{Time Series Forecasting}}},
  author = {Borovykh, Anastasia and Bohte, Sander and Oosterlee, Cornelis W.},
  year = {2018},
  month = oct,
  number = {ID 3272962},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  urldate = {2021-02-04},
  abstract = {We present a method for conditional time series forecasting based on an adaptation of the recent deep convolutional WaveNet architecture. The proposed network contains stacks of dilated convolutions that allow it to access a broad range of historical data when forecasting. It also uses a rectified linear unit (ReLU) activation function, and conditioning is performed by applying multiple convolutional filters in parallel to separate time series, which allows for the fast processing of data and the exploitation of the correlation structure between the multivariate time series. We test and analyze the performance of the convolutional network both unconditionally and conditionally for financial time series forecasting using the Standard \& Poor's 500 index, the volatility index, the Chicago Board Options Exchange interest rate and several exchange rates, and we extensively compare its performance with those of the well-known autoregressive model and a long short-term memory network. We show that a convolutional network is well suited to regression-type problems and is able to effectively learn dependencies in and between the series without the need for long historical time series, that it is a time-efficient and easy-to-implement alternative to recurrent-type networks, and that it tends to outperform linear and recurrent models.},
  langid = {english},
  keywords = {convolutional neural network (CNN),deep learning,financial time series,forecasting,multivariate time series.}
}

@article{bose_probabilistic_2017,
  title = {Probabilistic Demand Forecasting at Scale},
  author = {B{\"o}se, Joos-Hendrik and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Lange, Dustin and Salinas, David and Schelter, Sebastian and Seeger, Matthias and Wang, Yuyang},
  year = {2017},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {10},
  number = {12},
  pages = {1694--1705},
  issn = {21508097},
  doi = {10.14778/3137765.3137775},
  urldate = {2020-02-10},
  abstract = {We present a platform built on large-scale, data-centric machine learning (ML) approaches, whose particular focus is demand forecasting in retail. At its core, this platform enables the training and application of probabilistic demand forecasting models, and provides convenient abstractions and support functionality for forecasting problems. The platform comprises of a complex end-to-end machine learning system built on Apache Spark, which includes data preprocessing, feature engineering, distributed learning, as well as evaluation, experimentation and ensembling. Furthermore, it meets the demands of a production system and scales to large catalogues containing millions of items.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\H95LGMUB\BÃ¶se et al. - 2017 - Probabilistic demand forecasting at scale.pdf}
}

@inproceedings{botezatu_predicting_2016,
  title = {Predicting {{Disk Replacement}} towards {{Reliable Data Centers}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '16},
  author = {Botezatu, Mirela Madalina and Giurgiu, Ioana and Bogojeska, Jasmina and Wiesmann, Dorothea},
  year = {2016},
  pages = {39--48},
  publisher = {{ACM Press}},
  address = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2939699},
  urldate = {2019-04-17},
  abstract = {Disks are among the most frequently failing components in today's IT environments. Despite a set of defense mechanisms such as RAID, the availability and reliability of the system are still often impacted severely.},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  keywords = {read - not in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BYYLD8BF\Botezatu et al. - 2016 - Predicting Disk Replacement towards Reliable Data .pdf}
}

@article{box_distribution_1970,
  title = {Distribution of {{Residual Autocorrelations}} in {{Autoregressive-Integrated Moving Average Time Series Models}}},
  author = {Box, G. E. P. and Pierce, David A.},
  year = {1970},
  journal = {Journal of the American Statistical Association},
  volume = {65},
  number = {332},
  eprint = {2284333},
  eprinttype = {jstor},
  pages = {1509--1526},
  issn = {0162-1459},
  doi = {10.2307/2284333},
  urldate = {2019-04-15},
  abstract = {[Many statistical models, and in particular autoregressive-moving average time series models, can be regarded as means of transforming the data to white noise, that is, to an uncorrelated sequence of errors. If the parameters are known exactly, this random sequence can be computed directly from the observations; when this calculation is made with estimates substituted for the true parameter values, the resulting sequence is referred to as the "residuals," which can be regarded as estimates of the errors. If the appropriate model has been chosen, there will be zero autocorrelation in the errors. In checking adequacy of fit it is therefore logical to study the sample autocorrelation function of the residuals. For large samples the residuals from a correctly fitted model resemble very closely the true errors of the process; however, care is needed in interpreting the serial correlations of the residuals. It is shown here that the residual autocorrelations are to a close approximation representable as a singular linear transformation of the autocorrelations of the errors so that they possess a singular normal distribution. Failing to allow for this results in a tendency to overlook evidence of lack of fit. Tests of fit and diagnostic checks are devised which take these facts into account.]},
  keywords = {read - in related work},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\63I4M9EP\Box and Pierce - 1970 - Distribution of Residual Autocorrelations in Autor.pdf}
}

@article{bracher_evaluating_2021,
  title = {Evaluating Epidemic Forecasts in an Interval Format},
  author = {Bracher, Johannes and Ray, Evan L. and Gneiting, Tilmann and Reich, Nicholas G.},
  year = {2021},
  month = feb,
  journal = {PLoS Computational Biology},
  volume = {17},
  number = {2},
  pages = {e1008618},
  issn = {1553-734X},
  doi = {10.1371/journal.pcbi.1008618},
  urldate = {2023-10-09},
  abstract = {For practical reasons, many forecasts of case, hospitalization, and death counts in the context of the current Coronavirus Disease 2019 (COVID-19) pandemic are issued in the form of central predictive intervals at various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub (https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score, which has been applied in several infectious disease forecasting challenges, are then not available as they require full predictive distributions. This article provides an overview of how established methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts in this format. Specifically, we discuss the computation and interpretation of the weighted interval score, which is a proper score that approximates the continuous ranked probability score. It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows for a decomposition into a measure of sharpness and penalties for over- and underprediction., During the COVID-19 pandemic, model-based probabilistic forecasts of case, hospitalization, and death numbers can help to improve situational awareness and guide public health interventions. The COVID-19 Forecast Hub (https://covid19forecasthub.org/) collects such forecasts from numerous national and international groups. Systematic and statistically sound evaluation of forecasts is an important prerequisite to revise and improve models and to combine different forecasts into ensemble predictions. We provide an intuitive introduction to scoring methods, which are suitable for the interval/quantile-based format used in the Forecast Hub, and compare them to other commonly used performance measures.},
  pmcid = {PMC7880475},
  pmid = {33577550},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5HWUES7M\Bracher et al. - 2021 - Evaluating epidemic forecasts in an interval forma.pdf}
}

@article{braverman_parallel_2016,
  title = {Parallel {{Algorithms}} for {{Select}} and {{Partition}} with {{Noisy Comparisons}}},
  author = {Braverman, Mark and Mao, Jieming and Weinberg, S. Matthew},
  year = {2016},
  month = mar,
  journal = {arXiv:1603.04941 [cs]},
  eprint = {1603.04941},
  primaryclass = {cs},
  urldate = {2022-02-15},
  abstract = {We consider the problem of finding the \$k\^\{th\}\$ highest element in a totally ordered set of \$n\$ elements (select), and partitioning a totally ordered set into the top \$k\$ and bottom \$n-k\$ elements (partition) using pairwise comparisons. Motivated by settings like peer grading or crowdsourcing, where multiple rounds of interaction are costly and queried comparisons may be inconsistent with the ground truth, we evaluate algorithms based both on their total runtime and the number of interactive rounds in three comparison models: noiseless (where the comparisons are correct), erasure (where comparisons are erased with probability \$1-\textbackslash gamma\$), and noisy (where comparisons are correct with probability \$1/2+\textbackslash gamma/2\$ and incorrect otherwise). We provide numerous matching upper and lower bounds in all three models. Even our results in the noiseless model, which is quite well-studied in the TCS literature on parallel algorithms, are novel.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\K552PP6W\\Braverman et al. - 2016 - Parallel Algorithms for Select and Partition with .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UKSVRRZK\\1603.html}
}

@article{bregere_online_2020,
  title = {Online {{Hierarchical Forecasting}} for {{Power Consumption Data}}},
  author = {Br{\'e}g{\`e}re, Margaux and Huard, Malo},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.00585 [cs, stat]},
  eprint = {2003.00585},
  primaryclass = {cs, stat},
  urldate = {2021-02-04},
  abstract = {We study the forecasting of the power consumptions of a population of households and of subpopulations thereof. These subpopulations are built according to location, to exogenous information and/or to profiles we determined from historical households consumption time series. Thus, we aim to forecast the electricity consumption time series at several levels of households aggregation. These time series are linked through some summation constraints which induce a hierarchy. Our approach consists in three steps: feature generation, aggregation and projection. Firstly (feature generation step), we build, for each considering group for households, a benchmark forecast (called features), using random forests or generalized additive models. Secondly (aggregation step), aggregation algorithms, run in parallel, aggregate these forecasts and provide new predictions. Finally (projection step), we use the summation constraints induced by the time series underlying hierarchy to re-conciliate the forecasts by projecting them in a well-chosen linear subspace. We provide some theoretical guaranties on the average prediction error of this methodology, through the minimization of a quantity called regret. We also test our approach on households power consumption data collected in Great Britain by multiple energy providers in the Energy Demand Research Project context. We build and compare various population segmentations for the evaluation of our approach performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\H6XAWJQI\\BrÃ©gÃ¨re and Huard - 2020 - Online Hierarchical Forecasting for Power Consumpt.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\G56X65ND\\2003.html}
}

@misc{brinkmeyer_fewshot_2022,
  title = {Few-{{Shot Forecasting}} of {{Time-Series}} with {{Heterogeneous Channels}}},
  author = {Brinkmeyer, Lukas and Drumond, Rafael Rego and Burchert, Johannes and {Schmidt-Thieme}, Lars},
  year = {2022},
  month = apr,
  number = {arXiv:2204.03456},
  eprint = {2204.03456},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2204.03456},
  urldate = {2022-05-25},
  abstract = {Learning complex time series forecasting models usually requires a large amount of data, as each model is trained from scratch for each task/data set. Leveraging learning experience with similar datasets is a well-established technique for classification problems called few-shot classification. However, existing approaches cannot be applied to time-series forecasting because i) multivariate time-series datasets have different channels and ii) forecasting is principally different from classification. In this paper we formalize the problem of few-shot forecasting of time-series with heterogeneous channels for the first time. Extending recent work on heterogeneous attributes in vector data, we develop a model composed of permutation-invariant deep set-blocks which incorporate a temporal embedding. We assemble the first meta-dataset of 40 multivariate time-series datasets and show through experiments that our model provides a good generalization, outperforming baselines carried over from simpler scenarios that either fail to learn across tasks or miss temporal information.},
  archiveprefix = {arxiv},
  keywords = {68,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3NSSGPWA\\Brinkmeyer et al. - 2022 - Few-Shot Forecasting of Time-Series with Heterogen.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\D26LX5SW\\2204.html}
}

@article{bubeck_regret_2012,
  title = {Regret {{Analysis}} of {{Stochastic}} and {{Nonstochastic Multi-armed Bandit Problems}}},
  author = {Bubeck, S{\'e}bastien},
  year = {2012},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {5},
  number = {1},
  pages = {1--122},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000024},
  urldate = {2019-09-12},
  abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration\textendash exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the 1930s, exploration\textendash exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
  langid = {english},
  annotation = {01354},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2H5ADKNW\Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf}
}

@misc{burba_trainable_2021,
  title = {A {{Trainable Reconciliation Method}} for {{Hierarchical Time-Series}}},
  author = {Burba, Davide and Chen, Trista},
  year = {2021},
  month = jan,
  number = {arXiv:2101.01329},
  eprint = {2101.01329},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2101.01329},
  urldate = {2022-05-25},
  abstract = {In numerous applications, it is required to produce forecasts for multiple time-series at different hierarchy levels. An obvious example is given by the supply chain in which demand forecasting may be needed at a store, city, or country level. The independent forecasts typically do not add up properly because of the hierarchical constraints, so a reconciliation step is needed. In this paper, we propose a new general, flexible, and easy-to-implement reconciliation strategy based on an encoder-decoder neural network. By testing our method on four real-world datasets, we show that it can consistently reach or surpass the performance of existing methods in the reconciliation setting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5BH2IZZB\\Burba and Chen - 2021 - A Trainable Reconciliation Method for Hierarchical.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\43787EHG\\2101.html}
}

@article{burg_evaluation_2020,
  title = {An {{Evaluation}} of {{Change Point Detection Algorithms}}},
  author = {van den Burg, Gerrit J. J. and Williams, Christopher K. I.},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.06222 [cs, stat]},
  eprint = {2003.06222},
  primaryclass = {cs, stat},
  urldate = {2020-05-12},
  abstract = {Change point detection is an important part of time series analysis, as the presence of a change point indicates an abrupt and significant change in the data generating process. While many algorithms for change point detection exist, little attention has been paid to evaluating their performance on real-world time series. Algorithms are typically evaluated on simulated data and a small number of commonly-used series with unreliable ground truth. Clearly this does not provide sufficient insight into the comparative performance of these algorithms. Therefore, instead of developing yet another change point detection method, we consider it vastly more important to properly evaluate existing algorithms on real-world data. To achieve this, we present the first data set specifically designed for the evaluation of change point detection algorithms, consisting of 37 time series from various domains. Each time series was annotated by five expert human annotators to provide ground truth on the presence and location of change points. We analyze the consistency of the human annotators, and describe evaluation metrics that can be used to measure algorithm performance in the presence of multiple ground truth annotations. Subsequently, we present a benchmark study where 13 existing algorithms are evaluated on each of the time series in the data set. This study shows that binary segmentation (Scott and Knott, 1974) and Bayesian online change point detection (Adams and MacKay, 2007) are among the best performing methods. Our aim is that this data set will serve as a proving ground in the development of novel change point detection algorithms.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {62M10,Computer Science - Machine Learning,G.3,Statistics - Machine Learning,Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\U3JCLHC2\Burg and Williams - 2020 - An Evaluation of Change Point Detection Algorithms.pdf}
}

@article{burgess_monet_2019,
  title = {{{MONet}}: {{Unsupervised Scene Decomposition}} and {{Representation}}},
  shorttitle = {{{MONet}}},
  author = {Burgess, Christopher P. and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.11390 [cs, stat]},
  eprint = {1901.11390},
  primaryclass = {cs, stat},
  urldate = {2019-04-11},
  abstract = {The ability to decompose scenes in terms of abstract building blocks is crucial for general intelligence. Where those basic building blocks share meaningful properties, interactions and other regularities across scenes, such decompositions can simplify reasoning and facilitate imagination of novel scenarios. In particular, representing perceptual observations in terms of entities should improve data efficiency and transfer performance on a wide range of tasks. Thus we need models capable of discovering useful decompositions of scenes by identifying units with such regularities and representing them in a common format. To address this problem, we have developed the Multi-Object Network (MONet). In this model, a VAE is trained end-to-end together with a recurrent attention network \textendash{} in a purely unsupervised manner \textendash{} to provide attention masks around, and reconstructions of, regions of images. We show that this model is capable of learning to decompose and represent challenging 3D scenes into semantically meaningful components, such as objects and background elements.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2WMCBZJR\Burgess et al. - 2019 - MONet Unsupervised Scene Decomposition and Repres.pdf}
}

@inproceedings{cai_facets_2015,
  title = {Facets: {{Fast Comprehensive Mining}} of {{Coevolving High-order Time Series}}},
  shorttitle = {Facets},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '15},
  author = {Cai, Yongjie and Tong, Hanghang and Fan, Wei and Ji, Ping and He, Qing},
  year = {2015},
  pages = {79--88},
  publisher = {{ACM Press}},
  address = {{Sydney, NSW, Australia}},
  doi = {10.1145/2783258.2783348},
  urldate = {2019-04-17},
  abstract = {Mining time series data has been a very active research area in the past decade, exactly because of its prevalence in many high-impact applications, ranging from environmental monitoring, intelligent transportation systems, computer network forensics, to smart buildings and many more. It has posed many fascinating research questions. Among others, three prominent challenges shared by a variety of real applications are (a) high-order; (b) contextual constraints and (c) temporal smoothness. The state-of-the-art mining algorithms are rich in addressing each of these challenges, but relatively short of comprehensiveness in attacking the coexistence of multiple or even all of these three challenges.},
  isbn = {978-1-4503-3664-2},
  langid = {english},
  keywords = {read - not in related work},
  annotation = {00031},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LAGAUZMU\Cai et al. - 2015 - Facets Fast Comprehensive Mining of Coevolving Hi.pdf}
}

@article{cao_information_2009,
  title = {The Information Content of an Open Limit-Order Book},
  author = {Cao, Charles and Hansch, Oliver and Wang, Xiaoxin},
  year = {2009},
  month = jan,
  journal = {Journal of Futures Markets},
  volume = {29},
  number = {1},
  pages = {16--41},
  issn = {02707314, 10969934},
  doi = {10.1002/fut.20334},
  urldate = {2021-07-29},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\J6RNJ6GJ\Cao et al. - 2009 - The information content of an open limit-order boo.pdf}
}

@article{cao_spectral_,
  title = {Spectral {{Temporal Graph Neural Network}} for {{Multivariate Time-series Forecasting}}},
  author = {Cao, Defu and Wang, Yujing and Duan, Juanyong and Zhang, Ce and Zhu, Xia and Huang, Conguri and Tong, Yunhai and Xu, Bixiong and Bai, Jing and Tong, Jie and Zhang, Qi},
  pages = {13},
  abstract = {Multivariate time-series forecasting plays a crucial role in many real-world applications. It is a challenging problem as one needs to consider both intra-series temporal correlations and inter-series correlations simultaneously. Recently, there have been multiple works trying to capture both correlations, but most, if not all of them only capture temporal correlations in the time domain and resort to pre-defined priors as inter-series relationships.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\89BADQV7\Cao et al. - Spectral Temporal Graph Neural Network for Multiva.pdf}
}

@article{carriere_perslay_2019,
  title = {{{PersLay}}: {{A Neural Network Layer}} for {{Persistence Diagrams}} and {{New Graph Topological Signatures}}},
  shorttitle = {{{PersLay}}},
  author = {Carri{\`e}re, Mathieu and Chazal, Fr{\'e}d{\'e}ric and Ike, Yuichi and Lacombe, Th{\'e}o and Royer, Martin and Umeda, Yuhei},
  year = {2019},
  month = oct,
  journal = {arXiv:1904.09378 [cs, math, stat]},
  eprint = {1904.09378},
  primaryclass = {cs, math, stat},
  urldate = {2019-12-16},
  abstract = {Persistence diagrams, the most common descriptors of Topological Data Analysis, encode topological properties of data and have already proved pivotal in many different applications of data science. However, since the (metric) space of persistence diagrams is not Hilbert, they end up being difficult inputs for most Machine Learning techniques. To address this concern, several vectorization methods have been put forward that embed persistence diagrams into either finite-dimensional Euclidean space or (implicit) infinite dimensional Hilbert space with kernels. In this work, we focus on persistence diagrams built on top of graphs. Relying on extended persistence theory and the so-called heat kernel signature, we show how graphs can be encoded by (extended) persistence diagrams in a provably stable way. We then propose a general and versatile framework for learning vectorizations of persistence diagrams, which encompasses most of the vectorization techniques used in the literature. We finally showcase the experimental strength of our setup by achieving competitive scores on classification tasks on real-life graph datasets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\SYK8VQKT\CarriÃ¨re et al. - 2019 - PersLay A Neural Network Layer for Persistence Di.pdf}
}

@article{cerda_encoding_2020,
  title = {Encoding High-Cardinality String Categorical Variables},
  author = {Cerda, Patricio and Varoquaux, Ga{\"e}l},
  year = {2020},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  eprint = {1907.01860},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2020.2992529},
  urldate = {2021-11-17},
  abstract = {Statistical models usually require vector representations of categorical variables, using for instance one-hot encoding. This strategy breaks down when the number of categories grows, as it creates high-dimensional feature vectors. Additionally, for string entries, one-hot encoding does not capture information in their representation.Here, we seek low-dimensional encoding of high-cardinality string categorical variables. Ideally, these should be: scalable to many categories; interpretable to end users; and facilitate statistical analysis. We introduce two encoding approaches for string categories: a Gamma-Poisson matrix factorization on substring counts, and the min-hash encoder, for fast approximation of string similarities. We show that min-hash turns set inclusions into inequality relations that are easier to learn. Both approaches are scalable and streamable. Experiments on real and simulated data show that these methods improve supervised learning with high-cardinality categorical variables. We recommend the following: if scalability is central, the min-hash encoder is the best option as it does not require any data fit; if interpretability is important, the Gamma-Poisson factorization is the best alternative, as it can be interpreted as one-hot encoding on inferred categories with informative feature names. Both models enable autoML on the original string entries as they remove the need for feature engineering or data cleaning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YAYM5V9K\\Cerda and Varoquaux - 2020 - Encoding high-cardinality string categorical varia.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TGIJU6SI\\1907.html}
}

@article{chakraborty_bayesian_2019,
  title = {Bayesian {{Neural Tree Models}} for {{Nonparametric Regression}}},
  author = {Chakraborty, Tanujit and Kamat, Gauri and Chakraborty, Ashis Kumar},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.00515 [stat]},
  eprint = {1909.00515},
  primaryclass = {stat},
  urldate = {2019-09-05},
  abstract = {Frequentist and Bayesian methods differ in many aspects, but share some basic optimal properties. In real-life classification and regression problems, situations exist in which a model based on one of the methods is preferable based on some subjective criterion. Nonparametric classification and regression techniques, such as decision trees and neural networks, have frequentist (classification and regression trees (CART) and artificial neural networks) as well as Bayesian (Bayesian CART and Bayesian neural networks) approaches to learning from data. In this work, we present two hybrid models combining the Bayesian and frequentist versions of CART and neural networks, which we call the Bayesian neural tree (BNT) models. Both models exploit the architecture of decision trees and have lesser number of parameters to tune than advanced neural networks. Such models can simultaneously perform feature selection and prediction, are highly flexible, and generalize well in settings with a limited number of training observations. We study the consistency of the proposed models, and derive the optimal value of an important model parameter. We also provide illustrative examples using a wide variety of real-life regression data sets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TQDEAG5Q\Chakraborty et al. - 2019 - Bayesian Neural Tree Models for Nonparametric Regr.pdf}
}

@misc{challu_nhits_2022,
  title = {N-{{HiTS}}: {{Neural Hierarchical Interpolation}} for {{Time Series Forecasting}}},
  shorttitle = {N-{{HiTS}}},
  author = {Challu, Cristian and Olivares, Kin G. and Oreshkin, Boris N. and Garza, Federico and {Mergenthaler-Canseco}, Max and Dubrawski, Artur},
  year = {2022},
  month = feb,
  number = {arXiv:2201.12886},
  eprint = {2201.12886},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2201.12886},
  urldate = {2022-05-25},
  abstract = {Recent progress in neural forecasting accelerated improvements in the performance of large-scale forecasting systems. Yet, long-horizon forecasting remains a very difficult task. Two common challenges afflicting long-horizon forecasting are the volatility of the predictions and their computational complexity. In this paper, we introduce N-HiTS, a model which addresses both challenges by incorporating novel hierarchical interpolation and multi-rate data sampling techniques. These techniques enable the proposed method to assemble its predictions sequentially, selectively emphasizing components with different frequencies and scales, while decomposing the input signal and synthesizing the forecast. We conduct an extensive empirical evaluation demonstrating the advantages of N-HiTS over the state-of-the-art long-horizon forecasting methods. On an array of multivariate forecasting tasks, the proposed method provides an average accuracy improvement of 25\% over the latest Transformer architectures while reducing the computation time by an order of magnitude. Our code is available at https://bit.ly/3JLIBp8.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WWHG3WH8\\Challu et al. - 2022 - N-HiTS Neural Hierarchical Interpolation for Time.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\S7HLFR4S\\2201.html}
}

@article{chang_memorynetwork_2018,
  title = {A {{Memory-Network Based Solution}} for {{Multivariate Time-Series Forecasting}}},
  author = {Chang, Yen-Yu and Sun, Fan-Yun and Wu, Yueh-Hua and Lin, Shou-De},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.02105 [cs, stat]},
  eprint = {1809.02105},
  primaryclass = {cs, stat},
  urldate = {2019-04-03},
  abstract = {Multivariate time series forecasting is extensively studied throughout the years with ubiquitous applications in areas such as finance, traffic, environment, etc. Still, concerns have been raised on traditional methods for incapable of modeling complex patterns or dependencies lying in real word data. To address such concerns, various deep learning models, mainly Recurrent Neural Network (RNN) based methods, are proposed. Nevertheless, capturing extremely long-term patterns while effectively incorporating information from other variables remains a challenge for time-series forecasting. Furthermore, lack-of-explainability remains one serious drawback for deep neural network models. Inspired by Memory Network proposed for solving the question-answering task, we propose a deep learning based model named Memory Timeseries network (MTNet) for time series forecasting. MTNet consists of a large memory component, three separate encoders, and an autoregressive component to train jointly. Addtionally, the attention mechanism designed enable MTNet to be highly interpretable. We can easily tell which part of the historic data is referenced the most.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HGKUBUMC\Chang et al. - 2018 - A Memory-Network Based Solution for Multivariate T.pdf}
}

@inproceedings{chapados_augmented_2007,
  title = {Augmented {{Functional Time Series Representation}} and {{Forecasting}} with {{Gaussian Processes}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Chapados, Nicolas and Bengio, Yoshua},
  year = {2007},
  series = {{{NIPS}}'07},
  pages = {265--272},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}},
  urldate = {2019-04-17},
  abstract = {We introduce a functional representation of time series which allows forecasts to be performed over an unspecified horizon with progressively-revealed information sets. By virtue of using Gaussian processes, a complete covariance matrix between forecasts at several time-steps is available. This information is put to use in an application to actively trade price spreads between commodity futures contracts. The approach delivers impressive out-of-sample risk-adjusted returns after transaction costs on a portfolio of 30 spreads.},
  isbn = {978-1-60560-352-0},
  annotation = {00018},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\44FQZE2F\Chapados and Bengio - Augmented Functional Time Series Representation an.pdf}
}

@inproceedings{che_hierarchical_2018,
  title = {Hierarchical {{Deep Generative Models}} for {{Multi-Rate Multivariate Time Series}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Che, Zhengping and Purushotham, Sanjay and Li, Guangyu and Jiang, Bo and Liu, Yan},
  year = {2018},
  month = jul,
  pages = {784--793},
  urldate = {2019-04-17},
  abstract = {Multi-Rate Multivariate Time Series (MR-MTS) are the multivariate time series observations which come with various sampling rates and encode multiple temporal dependencies. State-space models such ...},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3Z3UWXQJ\\Che et al. - 2018 - Hierarchical Deep Generative Models for Multi-Rate.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\H8LR79MN\\che18a.html}
}

@article{che_recurrent_2018a,
  title = {Recurrent {{Neural Networks}} for {{Multivariate Time Series}} with {{Missing Values}}},
  author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},
  year = {2018},
  month = apr,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {6085},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-24271-9},
  urldate = {2019-04-23},
  abstract = {Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Unit (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provide useful insights for better understanding and utilization of missing values in time series analysis.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00191},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\H795CB4J\\Che et al. - 2018 - Recurrent Neural Networks for Multivariate Time Se.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NBTR6IBK\\s41598-018-24271-9.html}
}

@article{chen_dynamic_2019,
  title = {Dynamic {{Quantile Function Models}}},
  author = {Chen, Wilson Ye and Peters, Gareth W. and Gerlach, Richard H. and Sisson, Scott A.},
  year = {2019},
  month = nov,
  journal = {arXiv:1707.02587 [q-fin, stat]},
  eprint = {1707.02587},
  primaryclass = {q-fin, stat},
  urldate = {2020-02-17},
  abstract = {We offer a novel way of thinking about the modelling of the time-varying distributions of financial asset returns. Borrowing ideas from symbolic data analysis, we consider data representations beyond scalars and vectors. Specifically, we consider a quantile function as an observation, and develop a new class of dynamic models for quantile-function-valued (QF-valued) time series. In order to make statistical inferences and account for parameter uncertainty, we propose a method whereby a likelihood function can be constructed for QF-valued data, and develop an adaptive MCMC sampling algorithm for simulating from the posterior distribution. Compared to modelling realised measures, modelling the entire quantile functions of intra-daily returns allows one to gain more insight into the dynamic structure of price movements. Via simulations, we show that the proposed MCMC algorithm is effective in recovering the posterior distribution, and that the posterior means are reasonable point estimates of the model parameters. For empirical studies, the new model is applied to analysing one-minute returns of major international stock indices. Through quantile scaling, we further demonstrate the usefulness of our method by forecasting one-step-ahead the Value-at-Risk of daily returns.},
  archiveprefix = {arxiv},
  keywords = {Quantitative Finance - Risk Management,Statistics - Applications,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6A8IMJMJ\\Chen et al. - 2019 - Dynamic Quantile Function Models.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5R7GHK62\\1707.html}
}

@article{chen_improving_2019,
  title = {Improving {{Sequence-to-Sequence Learning}} via {{Optimal Transport}}},
  author = {Chen, Liqun and Zhang, Yizhe and Zhang, Ruiyi and Tao, Chenyang and Gan, Zhe and Zhang, Haichao and Li, Bai and Shen, Dinghan and Chen, Changyou and Carin, Lawrence},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.06283 [cs]},
  eprint = {1901.06283},
  primaryclass = {cs},
  urldate = {2019-10-02},
  abstract = {Sequence-to-sequence models are commonly trained via maximum likelihood estimation (MLE). However, standard MLE training considers a word-level objective, predicting the next word given the previous ground-truth partial sentence. This procedure focuses on modeling local syntactic patterns, and may fail to capture long-range semantic structure. We present a novel solution to alleviate these issues. Our approach imposes global sequence-level guidance via new supervision based on optimal transport, enabling the overall characterization and preservation of semantic features. We further show that this method can be understood as a Wasserstein gradient flow trying to match our model to the ground truth sequence distribution. Extensive experiments are conducted to validate the utility of the proposed approach, showing consistent improvements over a wide variety of NLP tasks, including machine translation, abstractive text summarization, and image captioning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {00003},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZJLTD6ZG\Chen et al. - 2019 - Improving Sequence-to-Sequence Learning via Optima.pdf}
}

@article{chen_introduction_,
  title = {Introduction to {{Boosted Trees}}},
  author = {Chen, Tianqi},
  pages = {44},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QIUFTRPR\Chen - Introduction to Boosted Trees.pdf}
}

@inproceedings{chen_learning_2016,
  title = {Learning {{Online Smooth Predictors}} for {{Realtime Camera Planning Using Recurrent Decision Trees}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chen, Jianhui and Le, Hoang M. and Carr, Peter and Yue, Yisong and Little, James J.},
  year = {2016},
  month = jun,
  pages = {4688--4696},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.507},
  urldate = {2019-06-18},
  abstract = {We study the problem of online prediction for realtime camera planning, where the goal is to predict smooth trajectories that correctly track and frame objects of interest (e.g., players in a basketball game). The conventional approach for training predictors does not directly consider temporal consistency, and often produces undesirable jitter. Although post-hoc smoothing (e.g., via a Kalman filter) can mitigate this issue to some degree, it is not ideal due to overly stringent modeling assumptions (e.g., Gaussian noise). We propose a recurrent decision tree framework that can directly incorporate temporal consistency into a data-driven predictor, as well as a learning algorithm that can efficiently learn such temporally smooth models. Our approach does not require any post-processing, making online smooth predictions much easier to generate when the noise model is unknown. We apply our approach to sports broadcasting: given noisy player detections, we learn where the camera should look based on human demonstrations. Our experiments exhibit significant improvements over conventional baselines and showcase the practicality of our approach.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  annotation = {00026},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\STEIJ6PW\Learning-Online-Smooth-Predictors-for-Realtime-Camera-Planning-using-Recurrent-Decision-Trees-Paper2.pdf}
}

@inproceedings{chen_learning_2021,
  title = {Learning {{Prediction Intervals}} for {{Regression}}: {{Generalization}} and {{Calibration}}},
  shorttitle = {Learning {{Prediction Intervals}} for {{Regression}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Chen, Haoxian and Huang, Ziyi and Lam, Henry and Qian, Huajie and Zhang, Haofeng},
  year = {2021},
  month = mar,
  pages = {820--828},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-03-07},
  abstract = {We study the generation of prediction intervals in regression for uncertainty quantification. This task can be formalized as an empirical constrained optimization problem that minimizes the average interval width while maintaining the coverage accuracy across data. We strengthen the existing literature by studying two aspects of this empirical optimization. First is a general learning theory to characterize the optimality-feasibility tradeoff that encompasses Lipschitz continuity and VC-subgraph classes, which are exemplified in regression trees and neural networks. Second is a calibration machinery and the corresponding statistical theory to optimally select the regularization parameter that manages this tradeoff, which bypasses the overfitting issues in previous approaches in coverage attainment. We empirically demonstrate the strengths of our interval generation and calibration algorithms in terms of testing performances compared to existing benchmarks.},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\AB7SSATT\\Chen et al. - 2021 - Learning Prediction Intervals for Regression Gene.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HYV6P535\\Chen et al. - 2021 - Learning Prediction Intervals for Regression Gene.pdf}
}

@article{chen_net2net_2015,
  title = {{{Net2Net}}: {{Accelerating Learning}} via {{Knowledge Transfer}}},
  shorttitle = {{{Net2Net}}},
  author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.05641 [cs]},
  eprint = {1511.05641},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of functionpreserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\8E6EFJFK\Chen et al. - 2015 - Net2Net Accelerating Learning via Knowledge Trans.pdf}
}

@article{chen_probabilistic_2019,
  title = {Probabilistic {{Forecasting}} with {{Temporal Convolutional Neural Network}}},
  author = {Chen, Yitian and Kang, Yanfei and Chen, Yixiong and Wang, Zizhuo},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.04397 [cs, stat]},
  eprint = {1906.04397},
  primaryclass = {cs, stat},
  urldate = {2020-01-14},
  abstract = {We present a probabilistic forecasting framework based on convolutional neural network for multiple related time series forecasting. The framework can be applied to estimate probability density under both parametric and non-parametric settings. More specifically, stacked residual blocks based on dilated causal convolutional nets are constructed to capture the temporal dependencies of the series. Combined with representation learning, our approach is able to learn complex patterns such as seasonality, holiday effects within and across series, and to leverage those patterns for more accurate forecasts, especially when historical data is sparse or unavailable. Extensive empirical studies are performed on several real-world datasets, including datasets from JD.com, China's largest online retailer. The results show that our framework outperforms other state-of-the-art methods in both accuracy and efficiency.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TY65XYYC\\Chen et al. - 2019 - Probabilistic Forecasting with Temporal Convolutio.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WC6CPCDP\\1906.html}
}

@article{chen_robust_2019,
  title = {Robust {{Decision Trees Against Adversarial Examples}}},
  author = {Chen, Hongge and Zhang, Huan and Boning, Duane and Hsieh, Cho-Jui},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.10660 [cs, stat]},
  eprint = {1902.10660},
  primaryclass = {cs, stat},
  urldate = {2019-08-30},
  abstract = {Although adversarial examples and model robustness have been extensively studied in the context of linear models and neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversarial examples is still limited. In this paper, we show that tree based models are also vulnerable to adversarial examples and develop a novel algorithm to learn robust trees. At its core, our method aims to optimize the performance under the worstcase perturbation of input features, which leads to a max-min saddle point problem. Incorporating this saddle point objective into the decision tree building procedure is non-trivial due to the discrete nature of trees\textemdash a naive approach to finding the best split according to this saddle point objective will take exponential time. To make our approach practical and scalable, we propose efficient tree building algorithms by approximating the inner minimizer in this saddle point problem, and present efficient implementations for classical information gain based trees as well as state-of-the-art tree boosting models such as XGBoost. Experimental results on real world datasets demonstrate that the proposed algorithms can substantially improve the robustness of tree-based models against adversarial examples.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VYLK28TB\Chen et al. - 2019 - Robust Decision Trees Against Adversarial Examples.pdf}
}

@article{chen_sequential_2019,
  title = {Sequential {{VAE-LSTM}} for {{Anomaly Detection}} on {{Time Series}}},
  author = {Chen, Run-Qing and Shi, Guang-Hui and Zhao, Wan-Lei and Liang, Chang-Hui},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.03818 [cs, stat]},
  eprint = {1910.03818},
  primaryclass = {cs, stat},
  urldate = {2020-02-20},
  abstract = {In order to support stable web-based applications and services, anomalies on the IT performance status have to be detected timely. Moreover, the performance trend across the time series should be predicted. In this paper, we propose SeqVL (Sequential VAE-LSTM), a neural network model based on both VAE (Variational Auto-Encoder) and LSTM (Long Short-Term Memory). This work is the first attempt to integrate unsupervised anomaly detection and trend prediction under one framework. Moreover, this model performs considerably better on detection and prediction than VAE and LSTM work alone. On unsupervised anomaly detection, SeqVL achieves competitive experimental results compared with other state-of-the-art methods on public datasets. On trend prediction, SeqVL outperforms several classic time series prediction models in the experiments of the public dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\QHW5LTE3\\Chen et al. - 2019 - Sequential VAE-LSTM for Anomaly Detection on Time .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MK6SGD8J\\1910.html}
}

@article{chen_slide_2020,
  title = {{{SLIDE}} : {{In Defense}} of {{Smart Algorithms}} over {{Hardware Acceleration}} for {{Large-Scale Deep Learning Systems}}},
  shorttitle = {{{SLIDE}}},
  author = {Chen, Beidi and Medini, Tharun and Farwell, James and Gobriel, Sameh and Tai, Charlie and Shrivastava, Anshumali},
  year = {2020},
  month = feb,
  journal = {arXiv:1903.03129 [cs]},
  eprint = {1903.03129},
  primaryclass = {cs},
  urldate = {2020-03-06},
  abstract = {Deep Learning (DL) algorithms are the central focus of modern machine learning systems. As data volumes keep growing, it has become customary to train large neural networks with hundreds of millions of parameters to maintain enough capacity to memorize these volumes and obtain state-of-the-art accuracy. To get around the costly computations associated with large models and data, the community is increasingly investing in specialized hardware for model training. However, specialized hardware is expensive and hard to generalize to a multitude of tasks. The progress on the algorithmic front has failed to demonstrate a direct advantage over powerful hardware such as NVIDIA-V100 GPUs. This paper provides an exception. We propose SLIDE (Sub-LInear Deep learning Engine) that uniquely blends smart randomized algorithms, with multi-core parallelism and workload optimization. Using just a CPU, SLIDE drastically reduces the computations during both training and inference outperforming an optimized implementation of Tensorflow (TF) on the best available GPU. Our evaluations on industry-scale recommendation datasets, with large fully connected architectures, show that training with SLIDE on a 44 core CPU is more than 3.5 times (1 hour vs. 3.5 hours) faster than the same network trained using TF on Tesla V100 at any given accuracy level. On the same CPU hardware, SLIDE is over 10x faster than TF. We provide codes and scripts for reproducibility.},
  archiveprefix = {arxiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\H9GW4BMI\\Chen et al. - 2020 - SLIDE  In Defense of Smart Algorithms over Hardwa.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BXLJCCX9\\1903.html}
}

@inproceedings{chen_tada_2018,
  title = {{{TADA}}: {{Trend Alignment}} with {{Dual-Attention Multi-task Recurrent Neural Networks}} for {{Sales Prediction}}},
  shorttitle = {{{TADA}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Chen, Tong and Yin, Hongzhi and Chen, Hongxu and Wu, Lin and Wang, Hao and Zhou, Xiaofang and Li, Xue},
  year = {2018},
  month = nov,
  pages = {49--58},
  publisher = {{IEEE}},
  address = {{Singapore}},
  doi = {10.1109/ICDM.2018.00020},
  urldate = {2019-04-03},
  abstract = {As a common strategy in sales-supply chains, the prediction of sales volume offers precious information for companies to achieve a healthy balance between supply and demand. In practice, the sales prediction task is formulated as a time series prediction problem which aims to predict the future sales volume for different products with the observation of various influential factors (e.g., brand, season, discount, etc.) and corresponding historical sales records. However, with the development of contemporary commercial markets, the dynamic interaction between influential factors with different semantic meanings becomes more subtle, causing challenges in fully capturing dependencies among these variables. Besides, though seeking similar trends from the history benefits the accuracy for the prediction of upcoming sales, existing methods hardly suit sales prediction tasks because the trends in sales time series are more irregular and complex. Hence, we gain insights from the encoder-decoder recurrent neural network (RNN) structure, and propose a novel framework named TADA to carry out trend alignment with dualattention, multi-task RNNs for sales prediction. In TADA, we innovatively divide the influential factors into internal feature and external feature, which are jointly modelled by a multi-task RNN encoder. In the decoding stage, TADA utilizes two attention mechanisms to compensate for the unknown states of influential factors in the future and adaptively align the upcoming trend with relevant historical trends to ensure precise sales prediction. Experimental results on two real-world datasets comprehensively show the superiority of TADA in sales prediction tasks against other state-of-the-art competitors.},
  isbn = {978-1-5386-9159-5},
  langid = {english},
  keywords = {read - in related work},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\8829W6LP\Chen et al. - 2018 - TADA Trend Alignment with Dual-Attention Multi-ta.pdf}
}

@inproceedings{chen_xgboost_2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  pages = {785--794},
  publisher = {{ACM}},
  address = {{San Francisco California USA}},
  doi = {10.1145/2939672.2939785},
  urldate = {2020-06-25},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\L5T57BFU\Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf}
}

@misc{chen_you_2022,
  title = {You {{Only Live Once}}: {{Single-Life Reinforcement Learning}}},
  shorttitle = {You {{Only Live Once}}},
  author = {Chen, Annie S. and Sharma, Archit and Levine, Sergey and Finn, Chelsea},
  year = {2022},
  month = oct,
  number = {arXiv:2210.08863},
  eprint = {2210.08863},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.08863},
  urldate = {2022-11-03},
  abstract = {Reinforcement learning algorithms are typically designed to learn a performant policy that can repeatedly and autonomously complete a task, usually starting from scratch. However, in many real-world situations, the goal might not be to learn a policy that can do the task repeatedly, but simply to perform a new task successfully once in a single trial. For example, imagine a disaster relief robot tasked with retrieving an item from a fallen building, where it cannot get direct supervision from humans. It must retrieve this object within one test-time trial, and must do so while tackling unknown obstacles, though it may leverage knowledge it has of the building before the disaster. We formalize this problem setting, which we call single-life reinforcement learning (SLRL), where an agent must complete a task within a single episode without interventions, utilizing its prior experience while contending with some form of novelty. SLRL provides a natural setting to study the challenge of autonomously adapting to unfamiliar situations, and we find that algorithms designed for standard episodic reinforcement learning often struggle to recover from out-of-distribution states in this setting. Motivated by this observation, we propose an algorithm, \$Q\$-weighted adversarial learning (QWALE), which employs a distribution matching strategy that leverages the agent's prior experience as guidance in novel situations. Our experiments on several single-life continuous control problems indicate that methods based on our distribution matching formulation are 20-60\% more successful because they can more quickly recover from novel states.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\AQ2AJW63\\Chen et al. - 2022 - You Only Live Once Single-Life Reinforcement Lear.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XINV6H55\\2210.html}
}

@misc{cheng_computationally_2022,
  title = {Computationally {{Efficient Learning}} of {{Statistical Manifolds}}},
  author = {Cheng, Fan and Panagiotelis, Anastasios and Hyndman, Rob J.},
  year = {2022},
  month = mar,
  number = {arXiv:2103.11773},
  eprint = {2103.11773},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-05-24},
  abstract = {Analyzing high-dimensional data with manifold learning algorithms often requires searching for the nearest neighbors of all observations. This presents a computational bottleneck in statistical manifold learning when observations of probability distributions rather than vector-valued variables are available or when data size is large. We resolve this problem by proposing a new method for approximation in statistical manifold learning. The novelty of our approximation is the strongly consistent distance estimators based on independent and identically distributed samples from probability distributions. By exploiting the connection between Hellinger/total variation distance for discrete distributions and the L2/L1 norm, we demonstrate that the proposed distance estimators, combined with approximate nearest neighbor searching, could largely improve the computational efficiency with little to no loss in the accuracy of manifold embedding. The result is robust to different manifold learning algorithms and different approximate nearest neighbor algorithms. The proposed method is applied to learning statistical manifolds of electricity usage. This application demonstrates how underlying structures in high dimensional data, including anomalies, can be visualized and identified, in a way that is scalable to large datasets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Applications,Statistics - Computation},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MM7QQ2ND\Cheng et al. - 2022 - Computationally Efficient Learning of Statistical .pdf}
}

@article{child_generating_2019,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.10509 [cs, stat]},
  eprint = {1904.10509},
  primaryclass = {cs, stat},
  urldate = {2019-11-12},
  abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n \textbackslash sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00032},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\E3EPIQ3M\Child et al. - 2019 - Generating Long Sequences with Sparse Transformers.pdf}
}

@article{chipman_bart_2010,
  title = {{{BART}}: {{Bayesian}} Additive Regression Trees},
  shorttitle = {{{BART}}},
  author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
  year = {2010},
  month = mar,
  journal = {The Annals of Applied Statistics},
  volume = {4},
  number = {1},
  pages = {266--298},
  issn = {1932-6157},
  doi = {10.1214/09-AOAS285},
  urldate = {2020-08-24},
  abstract = {We develop a Bayesian ``sum-of-trees'' model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm that generates samples from a posterior. Effectively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood. This approach enables full posterior inference including point and interval estimates of the unknown regression function as well as the marginal effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model free variable selection. BART's many features are illustrated with a bake-off against competing methods on 42 different data sets, with a simulation experiment and on a drug discovery classification problem.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MSE4V48J\Chipman et al. - 2010 - BART Bayesian additive regression trees.pdf}
}

@article{chipman_bayesian_,
  title = {Bayesian {{Treed Models}}},
  author = {Chipman, Hugh A},
  pages = {22},
  abstract = {When simple parametric models such as linear regression fail to adequately approximate a relationship across an entire set of data, an alternative may be to consider a partition of the data, and then use a separate simple model within each subset of the partition. Such an alternative is provided by a treed model which uses a binary tree to identify such a partition. However, treed models go further than conventional trees (e.g. CART, C4.5) by fitting models rather than a simple mean or proportion within each subset. In this paper, we propose a Bayesian approach for finding and fitting parametric treed models, in particular focusing on Bayesian treed regression. The potential of this approach is illustrated by a cross-validation comparison of predictive performance with neural nets, MARS, and conventional trees on simulated and real data sets.},
  langid = {english},
  annotation = {00213},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\SI9N9VIX\Chipman - Bayesian Treed Models.pdf}
}

@article{cho_learning_2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  month = jun,
  journal = {arXiv:1406.1078 [cs, stat]},
  eprint = {1406.1078},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder\textendash Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\textendash Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\22A3USIS\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf}
}

@incollection{choi_retain_2016,
  title = {{{RETAIN}}: {{An Interpretable Predictive Model}} for {{Healthcare}} Using {{Reverse Time Attention Mechanism}}},
  shorttitle = {{{RETAIN}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Choi, Edward and Bahadori, Mohammad Taha and Sun, Jimeng and Kulas, Joshua and Schuetz, Andy and Stewart, Walter},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {3504--3512},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-29},
  keywords = {read - not in related work},
  annotation = {00142},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RM865ZIQ\\Choi et al. - 2016 - RETAIN An Interpretable Predictive Model for Heal.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PZZDWPM3\\6321-retain-an-interpretable-predictive-model-for-healthcare-using-reverse-time-attention-mecha.html}
}

@article{christ_distributed_2016,
  title = {Distributed and Parallel Time Series Feature Extraction for Industrial Big Data Applications},
  author = {Christ, Maximilian and {Kempa-Liehr}, Andreas W. and Feindt, Michael},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.07717 [cs]},
  eprint = {1610.07717},
  primaryclass = {cs},
  urldate = {2019-08-07},
  abstract = {The all-relevant problem of feature selection is the identification of all strongly and weakly relevant attributes. This problem is especially hard to solve for time series classification and regression in industrial applications such as predictive maintenance or production line optimization, for which each label or regression target is associated with several time series and meta-information simultaneously. Here, we are proposing an efficient, scalable feature extraction algorithm for time series, which filters the available features in an early stage of the machine learning pipeline with respect to their significance for the classification or regression task, while controlling the expected percentage of selected but irrelevant features. The proposed algorithm combines established feature extraction methods with a feature importance filter. It has a low computational complexity, allows to start on a problem with only limited domain knowledge available, can be trivially parallelized, is highly scalable and based on well studied non-parametric hypothesis tests. We benchmark our proposed algorithm on all binary classification problems of the UCR time series classification archive as well as time series from a production line optimization project and simulated stochastic processes with underlying qualitative change of dynamics.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {62M10,Computer Science - Machine Learning,I.2.11},
  annotation = {00058},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3USFHSX4\Christ et al. - 2016 - Distributed and parallel time series feature extra.pdf}
}

@incollection{chung_recurrent_2015,
  title = {A {{Recurrent Latent Variable Model}} for {{Sequential Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {2980--2988},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-05-13},
  annotation = {00327},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BPPLT9YI\\Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential D.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\R7K7W6WC\\5653-a-recurrent-latent-variable-model-for-sequential-data.html}
}

@misc{chung_scaling_2022,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  year = {2022},
  month = oct,
  number = {arXiv:2210.11416},
  eprint = {2210.11416},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.11416},
  urldate = {2022-11-03},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XGXZWCUG\\Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LIXEWUJG\\2210.html}
}

@inproceedings{cirstea_correlated_2018,
  title = {Correlated {{Time Series Forecasting}} Using {{Multi-Task Deep Neural Networks}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}  - {{CIKM}} '18},
  author = {Cirstea, Razvan-Gabriel and Micu, Darius-Valer and Muresan, Gabriel-Marcel and Guo, Chenjuan and Yang, Bin},
  year = {2018},
  pages = {1527--1530},
  publisher = {{ACM Press}},
  address = {{Torino, Italy}},
  doi = {10.1145/3269206.3269310},
  urldate = {2019-04-02},
  abstract = {Cyber-physical systems often consist of entities that interact with each other over time. Meanwhile, as part of the continued digitization of industrial processes, various sensor technologies are deployed that enable us to record time-varying attributes (a.k.a., time series) of such entities, thus producing correlated time series. To enable accurate forecasting on such correlated time series, this paper proposes two models that combine convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The first model employs a CNN on each individual time series, combines the convoluted features, and then applies an RNN on top of the convoluted features in the end to enable forecasting. The second model adds additional auto-encoders into the individual CNNs, making the second model a multi-task learning model, which provides accurate and robust forecasting. Experiments on a large real-world correlated time series data set suggest that the proposed two models are effective and outperform baselines in most settings.},
  isbn = {978-1-4503-6014-2},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00003},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\Z5EIYUYK\Cirstea et al. - 2018 - Correlated Time Series Forecasting using Multi-Tas.pdf}
}

@article{clark_unsupervised_2019,
  title = {Unsupervised {{Discovery}} of {{Temporal Structure}} in {{Noisy Data}} with {{Dynamical Components Analysis}}},
  author = {Clark, David G. and Livezey, Jesse A. and Bouchard, Kristofer E.},
  year = {2019},
  month = oct,
  journal = {arXiv:1905.09944 [cs, math]},
  eprint = {1905.09944},
  primaryclass = {cs, math},
  urldate = {2020-02-17},
  abstract = {Linear dimensionality reduction methods are commonly used to extract low-dimensional structure from high-dimensional data. However, popular methods disregard temporal structure, rendering them prone to extracting noise rather than meaningful dynamics when applied to time series data. At the same time, many successful unsupervised learning methods for temporal, sequential and spatial data extract features which are predictive of their surrounding context. Combining these approaches, we introduce Dynamical Components Analysis (DCA), a linear dimensionality reduction method which discovers a subspace of high-dimensional time series data with maximal predictive information, defined as the mutual information between the past and future. We test DCA on synthetic examples and demonstrate its superior ability to extract dynamical structure compared to commonly used linear methods. We also apply DCA to several real-world datasets, showing that the dimensions extracted by DCA are more useful than those extracted by other methods for predicting future states and decoding auxiliary variables. Overall, DCA robustly extracts dynamical structure in noisy, high-dimensional data while retaining the computational efficiency and geometric interpretability of linear dimensionality reduction methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PDXQIF29\\Clark et al. - 2019 - Unsupervised Discovery of Temporal Structure in No.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MSVVGN7W\\1905.html}
}

@incollection{corani_reconciling_2021,
  title = {Reconciling {{Hierarchical Forecasts}} via {{Bayes}}' {{Rule}}},
  author = {Corani, Giorgio and Azzimonti, Dario and Augusto, Jo{\~a}o P. S. C. and Zaffalon, Marco},
  year = {2021},
  volume = {12459},
  eprint = {1906.03105},
  primaryclass = {stat},
  pages = {211--226},
  doi = {10.1007/978-3-030-67664-3_13},
  urldate = {2022-05-25},
  abstract = {We present a novel approach for reconciling hierarchical forecasts, based on Bayes rule. We define a prior distribution for the bottom time series of the hierarchy, based on the bottom base forecasts. Then we update their distribution via Bayes rule, based on the base forecasts for the upper time series. Under the Gaussian assumption, we derive the updating in closed-form. We derive two algorithms, which differ as for the assumed independencies. We discuss their relation with the MinT reconciliation algorithm and with the Kalman filter, and we compare them experimentally.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\933ULMJB\\Corani et al. - 2021 - Reconciling Hierarchical Forecasts via Bayes' Rule.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4RHM8SCC\\1906.html}
}

@article{courbariaux_binarized_2016,
  title = {Binarized {{Neural Networks}}: {{Training Deep Neural Networks}} with {{Weights}} and {{Activations Constrained}} to +1 or -1},
  shorttitle = {Binarized {{Neural Networks}}},
  author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and {El-Yaniv}, Ran and Bengio, Yoshua},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.02830 [cs]},
  eprint = {1602.02830},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\AJK8ZNSF\Courbariaux et al. - 2016 - Binarized Neural Networks Training Deep Neural Ne.pdf}
}

@incollection{courty_joint_2017,
  title = {Joint Distribution Optimal Transportation for Domain Adaptation},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Courty, Nicolas and Flamary, R{\'e}mi and Habrard, Amaury and Rakotomamonjy, Alain},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {3730--3739},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-10-02},
  annotation = {00055},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\R882KED8\\Courty et al. - 2017 - Joint distribution optimal transportation for doma.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\J2RM288T\\6963-joint-distribution-optimal-transportation-for-domain-adaptation.html}
}

@article{courty_optimal_2015,
  title = {Optimal {{Transport}} for {{Domain Adaptation}}},
  author = {Courty, Nicolas and Flamary, R{\'e}mi and Tuia, Devis and Rakotomamonjy, Alain},
  year = {2015},
  month = jul,
  journal = {arXiv:1507.00504 [cs]},
  eprint = {1507.00504},
  primaryclass = {cs},
  urldate = {2019-10-02},
  abstract = {Domain adaptation is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data representation become more robust when confronted to data depicting the same classes, but described by another observation system. Among the many strategies proposed, finding domain-invariant representations has shown excellent properties, in particular since it allows to train a unique classifier effective in all domains. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labeled samples of the same class in the source domain to remain close during transport. This way, we exploit at the same time the labeled samples in the source and the distributions observed in both domains. Experiments on toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches. In addition, numerical experiments show that our approach leads to better performances on domain invariant deep learning features and can be easily adapted to the semi-supervised case where few labeled samples are available in the target domain.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  annotation = {00182},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\N4IAS5JZ\Courty et al. - 2015 - Optimal Transport for Domain Adaptation.pdf}
}

@article{croston_forecasting_1972,
  title = {Forecasting and {{Stock Control}} for {{Intermittent Demands}}},
  author = {Croston, J. D.},
  year = {1972},
  journal = {Operational Research Quarterly (1970-1977)},
  volume = {23},
  number = {3},
  eprint = {3007885},
  eprinttype = {jstor},
  pages = {289--303},
  issn = {0030-3623},
  doi = {10.2307/3007885},
  urldate = {2019-04-17},
  abstract = {[Exponential smoothing is frequently used for the forecasts in stock control systems. The analysis given shows that intermittent demands almost always produce inappropriate stock levels. Demand for constant quantities at fixed intervals may generate stock levels of up to double the quantity really needed. A method of overcoming these difficulties is described, using separate estimates of the size of demand, and of the demand frequency. The rules for setting the safety stock levels have also to be adjusted before consistent protection can be obtained against being out of stock.]},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HUGZFZBD\Croston - Forecasting and Stock Control for Intermittent Dem.pdf}
}

@article{cui_deep_2018,
  title = {Deep {{Bidirectional}} and {{Unidirectional LSTM Recurrent Neural Network}} for {{Network-wide Traffic Speed Prediction}}},
  author = {Cui, Zhiyong and Ke, Ruimin and Wang, Yinhai},
  year = {2018},
  month = jan,
  journal = {arXiv:1801.02143 [cs]},
  eprint = {1801.02143},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Short-term traffic forecasting based on deep learning methods, especially long short-term memory (LSTM) neural networks, has received much attention in recent years. However, the potential of deep learning methods in traffic forecasting has not yet fully been exploited in terms of the depth of the model architecture, the spatial scale of the prediction area, and the predictive power of spatial-temporal data. In this paper, a deep stacked bidirectional and unidirectional LSTM (SBU- LSTM) neural network architecture is proposed, which considers both forward and backward dependencies in time series data, to predict network-wide traffic speed. A bidirectional LSTM (BDLSM) layer is exploited to capture spatial features and bidirectional temporal dependencies from historical data. To the best of our knowledge, this is the first time that BDLSTMs have been applied as building blocks for a deep architecture model to measure the backward dependency of traffic data for prediction. The proposed model can handle missing values in input data by using a masking mechanism. Further, this scalable model can predict traffic speed for both freeway and complex urban traffic networks. Comparisons with other classical and state-of-the-art models indicate that the proposed SBU-LSTM neural network achieves superior prediction performance for the whole traffic network in both accuracy and robustness.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZRVSQ7R3\\Cui et al. - 2018 - Deep Bidirectional and Unidirectional LSTM Recurre.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Y3DV35DU\\1801.html}
}

@article{dabney_distributional_2017,
  title = {Distributional {{Reinforcement Learning}} with {{Quantile Regression}}},
  author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, R{\'e}mi},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.10044 [cs, stat]},
  eprint = {1710.10044},
  primaryclass = {cs, stat},
  urldate = {2020-01-29},
  abstract = {In reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TR6WC5LE\Dabney et al. - 2017 - Distributional Reinforcement Learning with Quantil.pdf}
}

@article{dabney_implicit_2018,
  title = {Implicit {{Quantile Networks}} for {{Distributional Reinforcement Learning}}},
  author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.06923 [cs, stat]},
  eprint = {1806.06923},
  primaryclass = {cs, stat},
  urldate = {2019-12-14},
  abstract = {In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\G7TALVL5\Dabney et al. - 2018 - Implicit Quantile Networks for Distributional Rein.pdf}
}

@misc{dabney_implicit_2018a,
  title = {Implicit {{Quantile Networks}} for {{Distributional Reinforcement Learning}}},
  author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  year = {2018},
  month = jun,
  number = {arXiv:1806.06923},
  eprint = {1806.06923},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.06923},
  urldate = {2022-11-03},
  abstract = {In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4SD5KZ88\\Dabney et al. - 2018 - Implicit Quantile Networks for Distributional Rein.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\G9WBMCIS\\1806.html}
}

@article{dabrowski_forecastnet_2020,
  title = {{{ForecastNet}}: {{A Time-Variant Deep Feed-Forward Neural Network Architecture}} for {{Multi-Step-Ahead Time-Series Forecasting}}},
  shorttitle = {{{ForecastNet}}},
  author = {Dabrowski, Joel Janek and Zhang, YiFan and Rahman, Ashfaqur},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.04155 [cs, stat]},
  eprint = {2002.04155},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {Recurrent and convolutional neural networks are the most common architectures used for time series forecasting in deep learning literature. These networks use parameter sharing by repeating a set of fixed architectures with fixed parameters over time or space. The result is that the overall architecture is time-invariant (shift-invariant in the spatial domain) or stationary. We argue that timeinvariance can reduce the capacity to perform multi-step-ahead forecasting, where modelling the dynamics at a range of scales and resolutions is required. We propose ForecastNet which uses a deep feed-forward architecture to provide a timevariant model. An additional novelty of ForecastNet is interleaved outputs, which we show assist in mitigating vanishing gradients. ForecastNet is demonstrated to outperform statistical and deep learning benchmark models on several datasets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WLWXRYQM\Dabrowski et al. - 2020 - ForecastNet A Time-Variant Deep Feed-Forward Neur.pdf}
}

@inproceedings{dabrowski_state_2018,
  title = {State {{Space Models}} for {{Forecasting Water Quality Variables}}: {{An Application}} in {{Aquaculture Prawn Farming}}},
  shorttitle = {State {{Space Models}} for {{Forecasting Water Quality Variables}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}  - {{KDD}} '18},
  author = {Dabrowski, Joel Janek and Rahman, Ashfaqur and George, Andrew and Arnold, Stuart and McCulloch, John},
  year = {2018},
  pages = {177--185},
  publisher = {{ACM Press}},
  address = {{London, United Kingdom}},
  doi = {10.1145/3219819.3219841},
  urldate = {2019-04-17},
  abstract = {A novel approach to deterministic modelling of diurnal water quality parameters in aquaculture prawn ponds is presented. The purpose is to provide assistance to prawn pond farmers in monitoring pond water quality with limited data. Obtaining sufficient water quality data is generally a challenge in commercial prawn farming applications. Farmers can sustain large losses in their crop if water quality is not well managed. The model presented provides a means for modelling and forecasting various water quality parameters. It is inspired by data dynamics and does not rely on physical ecosystem modelling. The model is constructed within the Bayesian filtering framework. The Kalman filter and the unscented Kalman filer are applied for inference. The results demonstrate generalisability to both variables and environments. The ability for short term forecasting with mean absolute percentage errors between 0.5\% and 11\% is demonstrated.},
  isbn = {978-1-4503-5552-0},
  langid = {english},
  keywords = {read - not in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2XTREYDJ\Dabrowski et al. - 2018 - State Space Models for Forecasting Water Quality V.pdf}
}

@article{dai_transformerxl_2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  journal = {arXiv:1901.02860 [cs, stat]},
  eprint = {1901.02860},
  primaryclass = {cs, stat},
  urldate = {2020-04-16},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4JXRQXYI\\Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TUKQ6PP8\\1901.html}
}

@misc{damianou_variational_2011,
  title = {Variational {{Gaussian Process Dynamical Systems}}},
  author = {Damianou, Andreas C. and Titsias, Michalis K. and Lawrence, Neil D.},
  year = {2011},
  journal = {undefined},
  urldate = {2019-04-17},
  abstract = {High dimensional time series are endemic in applications of machine learning such as robotics (sensor data), computational biology (gene expression data), vision (video sequences) and graphics (motion capture data). Practical nonlinear probabilistic approaches to this data are required. In this paper we introduce the variational Gaussian process dynamical system. Our work builds on recent variational approximations for Gaussian process latent variable models to allow for nonlinear dimensionality reduction simultaneously with learning a dynamical prior in the latent space. The approach also allows for the appropriate dimensionality of the latent space to be automatically determined. We demonstrate the model on a human motion capture data set and a series of high resolution video sequences.},
  howpublished = {/paper/Variational-Gaussian-Process-Dynamical-Systems-Damianou-Titsias/74014f17c54eddf72f601cbe642ab904d3aeccf4},
  langid = {english},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\28JL24FF\\Damianou et al. - 2011 - Variational Gaussian Process Dynamical Systems.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZGW6JG22\\74014f17c54eddf72f601cbe642ab904d3aeccf4.html}
}

@misc{das_topdown_2022,
  title = {A {{Top-Down Approach}} to {{Hierarchically Coherent Probabilistic Forecasting}}},
  author = {Das, Abhimanyu and Kong, Weihao and Paria, Biswajit and Sen, Rajat},
  year = {2022},
  month = apr,
  number = {arXiv:2204.10414},
  eprint = {2204.10414},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2204.10414},
  urldate = {2022-05-25},
  abstract = {Hierarchical forecasting is a key problem in many practical multivariate forecasting applications - the goal is to obtain coherent predictions for a large number of correlated time series that are arranged in a pre-specified tree hierarchy. In this paper, we present a probabilistic top-down approach to hierarchical forecasting that uses a novel attention-based RNN model to learn the distribution of the proportions according to which each parent prediction is split among its children nodes at any point in time. These probabilistic proportions are then coupled with an independent univariate probabilistic forecasting model (such as Prophet or STS) for the root time series. The resulting forecasts are computed in a top-down fashion and are naturally coherent, and also support probabilistic predictions over all time series in the hierarchy. We provide theoretical justification for the superiority of our top-down approach compared to traditional bottom-up hierarchical modeling. Finally, we experiment on three public datasets and demonstrate significantly improved probabilistic forecasts, compared to state-of-the-art probabilistic hierarchical models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TM4KNI9U\\Das et al. - 2022 - A Top-Down Approach to Hierarchically Coherent Pro.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\96F2CZ8R\\2204.html}
}

@article{dau_ucr_,
  title = {{{UCR}} Time Series Archive 2018},
  author = {Dau, Hoang Anh},
  pages = {34},
  abstract = {The UCR Time Series Archive - introduced in 2002, has become an important resource in the time series data mining community, with at least one thousand published papers making use of at least one dataset from the archive. The original incarnation of the archive had sixteen datasets but since that time, it has gone through periodic expansions. The last expansion took place in the summer of 2015 when the archive grew from 45 datasets to 85 datasets. This paper introduces and will focus on the new data expansion from 85 to 128 datasets. Beyond expanding this valuable resource, this paper offers pragmatic advice to anyone who may wish to evaluate a new algorithm on the archive. Finally, this paper makes a novel and yet actionable claim: of the hundreds of papers that show an improvement over the standard baseline (1-Nearest Neighbor classification), a large fraction may be misattributing the reasons for their improvement. Moreover, they may have been able to achieve the same improvement with a much simpler modification, requiring just a single line of code.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\XTAAGYAQ\Dau - UCR time series archive 2018.pdf}
}

@article{daume_searchbased_2009,
  title = {Search-Based Structured Prediction},
  author = {Daum{\'e}, Hal and Langford, John and Marcu, Daniel},
  year = {2009},
  month = jun,
  journal = {Machine Learning},
  volume = {75},
  number = {3},
  pages = {297--325},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-009-5106-x},
  urldate = {2019-08-07},
  abstract = {We present SEARN, an algorithm for integrating SEARch and lEARNing to solve complex structured prediction problems such as those that occur in natural language, speech, computational biology, and vision. SEARN is a meta-algorithm that transforms these complex problems into simple classification problems to which any binary classifier may be applied. Unlike current algorithms for structured learning that require decomposition of both the loss function and the feature functions over the predicted structure, SEARN is able to learn prediction functions for any loss function and any class of features. Moreover, SEARN comes with a strong, natural theoretical guarantee: good performance on the derived classification problems implies good performance on the structured prediction problem.},
  langid = {english},
  annotation = {00435},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MYNZLKRC\DaumÃ© et al. - 2009 - Search-based structured prediction.pdf}
}

@inproceedings{debezenac_normalizing_2020,
  title = {Normalizing {{Kalman Filters}} for {{Multivariate Time Series Analysis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{de B{\'e}zenac}, Emmanuel and Rangapuram, Syama Sundar and Benidis, Konstantinos and {Bohlke-Schneider}, Michael and Kurle, Richard and Stella, Lorenzo and Hasson, Hilaf and Gallinari, Patrick and Januschowski, Tim},
  year = {2020},
  volume = {33},
  pages = {2995--3007},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-06},
  abstract = {This paper tackles the modelling of large, complex and multivariate time series panels in a probabilistic setting. To this extent, we present a novel approach reconciling classical state space models with deep learning methods. By augmenting state space models with normalizing flows, we mitigate imprecisions stemming from idealized assumptions in state space models. The resulting model is highly flexible while still retaining many of the attractive properties of state space models, e.g., uncertainty and observation errors are properly accounted for, inference is tractable, sampling is efficient, good generalization performance is observed, even in low data regimes. We demonstrate competitiveness against state-of-the-art deep learning methods on the tasks of forecasting real world data and handling varying levels of missing data.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\U8X7BBV2\de BÃ©zenac et al. - 2020 - Normalizing Kalman Filters for Multivariate Time S.pdf}
}

@inproceedings{debezenac_normalizing_2020a,
  title = {Normalizing {{Kalman Filters}} for {{Multivariate Time Series Analysis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{de B{\'e}zenac}, Emmanuel and Rangapuram, Syama Sundar and Benidis, Konstantinos and {Bohlke-Schneider}, Michael and Kurle, Richard and Stella, Lorenzo and Hasson, Hilaf and Gallinari, Patrick and Januschowski, Tim},
  year = {2020},
  volume = {33},
  pages = {2995--3007},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-05-31},
  abstract = {This paper tackles the modelling of large, complex and multivariate time series panels in a probabilistic setting. To this extent, we present a novel approach reconciling classical state space models with deep learning methods. By augmenting state space models with normalizing flows, we mitigate imprecisions stemming from idealized assumptions in state space models. The resulting model is highly flexible while still retaining many of the attractive properties of state space models, e.g., uncertainty and observation errors are properly accounted for, inference is tractable, sampling is efficient, good generalization performance is observed, even in low data regimes. We demonstrate competitiveness against state-of-the-art deep learning methods on the tasks of forecasting real world data and handling varying levels of missing data.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JLUPI8AD\de BÃ©zenac et al. - 2020 - Normalizing Kalman Filters for Multivariate Time S.pdf}
}

@article{dehghani_universal_2018,
  title = {Universal {{Transformers}}},
  author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.03819 [cs, stat]},
  eprint = {1807.03819},
  primaryclass = {cs, stat},
  urldate = {2019-05-09},
  abstract = {Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00024},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5RXPPPVR\\Dehghani et al. - 2018 - Universal Transformers.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3R7E4AXZ\\1807.html}
}

@article{demarta_copula_2007,
  title = {The t {{Copula}} and {{Related Copulas}}},
  author = {Demarta, Stefano and McNeil, Alexander J.},
  year = {2007},
  month = jan,
  journal = {International Statistical Review},
  volume = {73},
  number = {1},
  pages = {111--129},
  issn = {03067734},
  doi = {10.1111/j.1751-5823.2005.tb00254.x},
  urldate = {2020-03-03},
  abstract = {The t copula and its properties are described with a focus on issues related to the dependence of extreme values. The Gaussian mixture representation of a multivariate t distribution is used as a starting point to construct two new copulas, the skewed t copula and the grouped t copula, which allow more heterogeneity in the modelling of dependent observations. Extreme value considerations are used to derive two further new copulas: the t extreme value copula is the limiting copula of componentwise maxima of t distributed random vectors; the t lower tail copula is the limiting copula of bivariate observations from a t distribution that are conditioned to lie below some joint threshold that is progressively lowered. Both these copulas may be approximated for practical purposes by simpler, better-known copulas, these being the Gumbel and Clayton copulas respectively.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TYTNI87J\Demarta and McNeil - 2007 - The t Copula and Related Copulas.pdf}
}

@inproceedings{deng_deep_2019,
  title = {Deep {{Learning}} for {{Knowledge-Driven Ontology Stream Prediction}}},
  booktitle = {Knowledge {{Graph}} and {{Semantic Computing}}. {{Knowledge Computing}} and {{Language Understanding}}},
  author = {Deng, Shumin and Pan, Jeff Z. and Chen, Jiaoyan and Chen, Huajun},
  editor = {Zhao, Jun and van Harmelen, Frank and Tang, Jie and Han, Xianpei and Wang, Quan and Li, Xianyong},
  year = {2019},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {52--64},
  publisher = {{Springer Singapore}},
  abstract = {Time series prediction with data stream has been widely studied. Current deep learning methods e.g., Long Short-Term Memory (LSTM) perform well in learning feature representations from raw data. However, most of these models can narrowly learn semantic information behind the data. In this paper, we revisit LSTM from the perspective of Semantic Web, where streaming data are represented as ontology sequences. We propose a novel semantic-based neural network (STBNet) that (i) enriches the semantics of data stream with external text, and (ii) exploits the underlying semantics with background knowledge for time series prediction. Previous models mainly rely on numerical representation of values in raw data, while the proposed STBNet model creatively integrates semantic embedding into a hybrid neural network. We develop a new attention mechanism based on similarity among semantic embedding of ontology stream, and then we combine ontology stream and numerical analysis in the deep learning model. Furthermore, we also enrich ontology stream in STBNet, where Convolutional Neural Networks (CNNs) are incorporated in learning lexical representations of words in the text. The experiments show that STBNet outperforms state-of-the-art methods on stock price prediction.},
  isbn = {9789811331466},
  langid = {english},
  keywords = {Deep learning,Ontology stream,read - not in related work,Time series prediction},
  annotation = {00000}
}

@misc{deng_efficient_2022,
  title = {Efficient {{Automated Deep Learning}} for {{Time Series Forecasting}}},
  author = {Deng, Difan and Karl, Florian and Hutter, Frank and Bischl, Bernd and Lindauer, Marius},
  year = {2022},
  month = may,
  number = {arXiv:2205.05511},
  eprint = {2205.05511},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.05511},
  urldate = {2022-05-25},
  abstract = {Recent years have witnessed tremendously improved efficiency of Automated Machine Learning (AutoML), especially Automated Deep Learning (AutoDL) systems, but recent work focuses on tabular, image, or NLP tasks. So far, little attention has been paid to general AutoDL frameworks for time series forecasting, despite the enormous success in applying different novel architectures to such tasks. In this paper, we propose an efficient approach for the joint optimization of neural architecture and hyperparameters of the entire data processing pipeline for time series forecasting. In contrast to common NAS search spaces, we designed a novel neural architecture search space covering various state-of-the-art architectures, allowing for an efficient macro-search over different DL approaches. To efficiently search in such a large configuration space, we use Bayesian optimization with multi-fidelity optimization. We empirically study several different budget types enabling efficient multi-fidelity optimization on different forecasting datasets. Furthermore, we compared our resulting system, dubbed Auto-PyTorch-TS, against several established baselines and show that it significantly outperforms all of them across several datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\W7J8TUPV\\Deng et al. - 2022 - Efficient Automated Deep Learning for Time Series .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BVWEYDTB\\2205.html}
}

@article{derijke_parameter_2021,
  title = {Parameter {{Efficient Deep Probabilistic Forecasting}}},
  author = {{de Rijke}, Olivier Sprangers Sebastian Schelter Maarten},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.02905 [cs]},
  eprint = {2112.02905},
  primaryclass = {cs},
  urldate = {2021-12-07},
  abstract = {Probabilistic time series forecasting is crucial in many application domains such as retail, ecommerce, finance, or biology. With the increasing availability of large volumes of data, a number of neural architectures have been proposed for this problem. In particular, Transformer-based methods achieve state-of-the-art performance on real-world benchmarks. However, these methods require a large number of parameters to be learned, which imposes high memory requirements on the computational resources for training such models. To address this problem, we introduce a novel Bidirectional Temporal Convolutional Network (BiTCN), which requires an order of magnitude less parameters than a common Transformer-based approach. Our model combines two Temporal Convolutional Networks (TCNs): the first network encodes future covariates of the time series, whereas the second network encodes past observations and covariates. We jointly estimate the parameters of an output distribution via these two networks. Experiments on four real-world datasets show that our method performs on par with four state-of-the-art probabilistic forecasting methods, including a Transformer-based approach and WaveNet, on two point metrics (sMAPE, NRMSE) as well as on a set of range metrics (quantile loss percentiles) in the majority of cases. Secondly, we demonstrate that our method requires significantly less parameters than Transformer-based methods, which means the model can be trained faster with significantly lower memory requirements, which as a consequence reduces the infrastructure cost for deploying these models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\G9TJ4I2D\\de Rijke - 2021 - Parameter Efficient Deep Probabilistic Forecasting.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5F793PUX\\2112.html}
}

@article{deshpande_long_2021,
  title = {Long {{Range Probabilistic Forecasting}} in {{Time-Series}} Using {{High Order Statistics}}},
  author = {Deshpande, Prathamesh and Sarawagi, Sunita},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.03394 [cs, stat]},
  eprint = {2111.03394},
  primaryclass = {cs, stat},
  urldate = {2022-04-08},
  abstract = {Long range forecasts are the starting point of many decision support systems that need to draw inference from high-level aggregate patterns on forecasted values. State of the art time-series forecasting methods are either subject to concept drift on long-horizon forecasts, or fail to accurately predict coherent and accurate high-level aggregates. In this work, we present a novel probabilistic forecasting method that produces forecasts that are coherent in terms of base level and predicted aggregate statistics. We achieve the coherency between predicted base-level and aggregate statistics using a novel inference method. Our inference method is based on KL-divergence and can be solved efficiently in closed form. We show that our method improves forecast performance across both base level and unseen aggregates post inference on real datasets ranging three diverse domains.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5V4PQ6TP\\Deshpande and Sarawagi - 2021 - Long Range Probabilistic Forecasting in Time-Serie.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Q3H7QSD5\\2111.html}
}

@misc{detommaso_fortuna_2023,
  title = {Fortuna: {{A Library}} for {{Uncertainty Quantification}} in {{Deep Learning}}},
  shorttitle = {Fortuna},
  author = {Detommaso, Gianluca and Gasparin, Alberto and Donini, Michele and Seeger, Matthias and Wilson, Andrew Gordon and Archambeau, Cedric},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04019},
  eprint = {2302.04019},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.04019},
  urldate = {2023-03-07},
  abstract = {We present Fortuna, an open-source library for uncertainty quantification in deep learning. Fortuna supports a range of calibration techniques, such as conformal prediction that can be applied to any trained neural network to generate reliable uncertainty estimates, and scalable Bayesian inference methods that can be applied to Flax-based deep neural networks trained from scratch for improved uncertainty quantification and accuracy. By providing a coherent framework for advanced uncertainty quantification methods, Fortuna simplifies the process of benchmarking and helps practitioners build robust AI systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FQ9HD6GA\\Detommaso et al. - 2023 - Fortuna A Library for Uncertainty Quantification .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CDWCWJ7Q\\2302.html}
}

@misc{detommaso_fortuna_2023a,
  title = {Fortuna: {{A Library}} for {{Uncertainty Quantification}} in {{Deep Learning}}},
  shorttitle = {Fortuna},
  author = {Detommaso, Gianluca and Gasparin, Alberto and Donini, Michele and Seeger, Matthias and Wilson, Andrew Gordon and Archambeau, Cedric},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04019},
  eprint = {2302.04019},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-31},
  abstract = {We present Fortuna, an open-source library for uncertainty quantification in deep learning. Fortuna supports a range of calibration techniques, such as conformal prediction that can be applied to any trained neural network to generate reliable uncertainty estimates, and scalable Bayesian inference methods that can be applied to Flax-based deep neural networks trained from scratch for improved uncertainty quantification and accuracy. By providing a coherent framework for advanced uncertainty quantification methods, Fortuna simplifies the process of benchmarking and helps practitioners build robust AI systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IEYBW44E\\Detommaso et al. - 2023 - Fortuna A Library for Uncertainty Quantification .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\K775K8UZ\\2302.html}
}

@article{devries_improved_2017,
  title = {Improved {{Regularization}} of {{Convolutional Neural Networks}} with {{Cutout}}},
  author = {DeVries, Terrance and Taylor, Graham W.},
  year = {2017},
  month = nov,
  journal = {arXiv:1708.04552 [cs]},
  eprint = {1708.04552},
  primaryclass = {cs},
  urldate = {2019-11-20},
  abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {00237},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TYNMQLJV\DeVries and Taylor - 2017 - Improved Regularization of Convolutional Neural Ne.pdf}
}

@article{dewolff_gaussian_2020,
  title = {Gaussian Process Imputation of Multiple Financial Series},
  author = {{de Wolff}, Taco and Cuevas, Alejandro and Tobar, Felipe},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.05789 [cs, eess, q-fin, stat]},
  eprint = {2002.05789},
  primaryclass = {cs, eess, q-fin, stat},
  urldate = {2020-02-17},
  abstract = {In Financial Signal Processing, multiple time series such as financial indicators, stock prices and exchange rates are strongly coupled due to their dependence on the latent state of the market and therefore they are required to be jointly analysed. We focus on learning the relationships among financial time series by modelling them through a multi-output Gaussian process (MOGP) with expressive covariance functions. Learning these market dependencies among financial series is crucial for the imputation and prediction of financial observations. The proposed model is validated experimentally on two realworld financial datasets for which their correlations across channels are analysed. We compare our model against other MOGPs and the independent Gaussian process on real financial data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Quantitative Finance - Statistical Finance,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MY479Y43\de Wolff et al. - 2020 - Gaussian process imputation of multiple financial .pdf}
}

@article{di_fonzo_forecast_2022,
  title = {Forecast Combination-Based Forecast Reconciliation: {{Insights}} and Extensions},
  shorttitle = {Forecast Combination-Based Forecast Reconciliation},
  author = {Di Fonzo, Tommaso and Girolimetto, Daniele},
  year = {2022},
  month = aug,
  journal = {International Journal of Forecasting},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2022.07.001},
  urldate = {2023-08-01},
  abstract = {In this paper, we build upon a recently proposed forecast combination-based approach to the reconciliation of a simple hierarchy (Hollyman R., Petropoulos F., Tipping M.E., Understanding forecast reconciliation, European Journal of Operational Research, 2021, 294, 149\textendash 160) and extend it in some new directions. In particular, we provide insights into the nature and mathematical derivation of the level-l conditional coherent (LlCC) point forecast reconciliation procedure for an elementary two-level hierarchy. We show that: (i) the LlCC procedure is the result of a linearly constrained minimization of a quadratic loss function, with an exogenous constraint given by the base forecast of the top level series of the hierarchy, which is not revised; and (ii) endogenous constraints may also be considered in the same framework, thereby resulting in level conditional reconciled forecasts where both the top and the bottom level time series forecasts are coherently revised. In addition, we show that the LlCC procedure (i.e., with exogenous constraints but the result also holds in the endogenous case) does not guarantee the non-negativity of the reconciled forecasts, which can be an issue in cases when non-negativity is a natural attribute of the variables that need to be forecast (e.g., sales and tourism flows). Finally, we consider two forecasting experiments to evaluate the performance of various cross-sectional forecast combination-based point forecast reconciliation procedures (vis-\`a-vis the state-of-the-art procedures) in a fair setting. In this framework, due to the crucial role played by the (possibly different) models used to compute the base forecasts, we re-interpret the combined conditional coherent reconciliation procedure (CCCH) of Hollyman et~al. (2021) as a forecast pooling approach, and show that accuracy improvements may be obtained by adopting a simple forecast averaging strategy.},
  langid = {english},
  keywords = {Cross-sectional forecast reconciliation,Forecast averaging,Forecast combination,Forecast pooling,Level conditional coherent forecast reconciliation},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PG6BPA3C\\Di Fonzo and Girolimetto - 2022 - Forecast combination-based forecast reconciliation.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LBB6EAJV\\S0169207022000991.html}
}

@article{diebold_comparing_2015,
  title = {Comparing {{Predictive Accuracy}}, {{Twenty Years Later}}: {{A Personal Perspective}} on the {{Use}} and {{Abuse}} of {{Diebold}}\textendash{{Mariano Tests}}},
  shorttitle = {Comparing {{Predictive Accuracy}}, {{Twenty Years Later}}},
  author = {Diebold, Francis X.},
  year = {2015},
  month = jan,
  journal = {Journal of Business \& Economic Statistics},
  volume = {33},
  number = {1},
  pages = {1--1},
  publisher = {{Taylor \& Francis}},
  issn = {0735-0015},
  doi = {10.1080/07350015.2014.983236},
  urldate = {2023-06-08},
  abstract = {The Diebold\textendash Mariano (DM) test was intended for comparing forecasts; it has been, and remains, useful in that regard. The DM test was not intended for comparing models. Much of the large ensuing literature, however, uses DM-type tests for comparing models, in pseudo-out-of-sample environments. In that case, simpler yet more compelling full-sample model comparison procedures exist; they have been, and should continue to be, widely used. The hunch that pseudo-out-of-sample analysis is somehow the ``only,'' or ``best,'' or even necessarily a ``good'' way to provide insurance against in-sample overfitting in model comparisons proves largely false. On the other hand, pseudo-out-of-sample analysis remains useful for certain tasks, perhaps most notably for providing information about comparative predictive performance during particular historical episodes.},
  keywords = {Forecasting,Model comparison,Model selection,Out-of-sample tests},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PVU9PYFK\Diebold - 2015 - Comparing Predictive Accuracy, Twenty Years Later.pdf}
}

@inproceedings{ding_deep_2015,
  title = {Deep {{Learning}} for {{Event-driven Stock Prediction}}},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Artificial Intelligence}}},
  author = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
  year = {2015},
  series = {{{IJCAI}}'15},
  pages = {2327--2333},
  publisher = {{AAAI Press}},
  address = {Buenos Aires, Argentina},
  urldate = {2019-04-17},
  abstract = {We propose a deep learning method for event-driven stock market prediction. First, events are extracted from news text, and represented as dense vectors, trained using a novel neural tensor network. Second, a deep convolutional neural network is used to model both short-term and long-term influences of events on stock price movements. Experimental results show that our model can achieve nearly 6\% improvements on S\&P 500 index prediction and individual stock prediction, respectively, compared to state-of-the-art baseline methods. In addition, market simulation results show that our system is more capable of making profits than previously reported systems trained on S\&P 500 stock historical data.},
  isbn = {978-1-57735-738-4},
  keywords = {read - in related work},
  annotation = {00176},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WHCL4TXP\Ding et al. - 2015 - Deep Learning for Event-driven Stock Prediction.pdf}
}

@article{ding_querying_2008,
  title = {Querying and {{Mining}} of {{Time Series Data}}: {{Experimental Comparison}} of {{Representations}} and {{Distance Measures}}},
  shorttitle = {Querying and {{Mining}} of {{Time Series Data}}},
  author = {Ding, Hui and Trajcevski, Goce and Scheuermann, Peter and Wang, Xiaoyue and Keogh, Eamonn},
  year = {2008},
  month = aug,
  journal = {Proc. VLDB Endow.},
  volume = {1},
  number = {2},
  pages = {1542--1552},
  issn = {2150-8097},
  doi = {10.14778/1454159.1454226},
  urldate = {2019-11-21},
  abstract = {The last decade has witnessed a tremendous growths of interests in applications that deal with querying and mining of time series data. Numerous representation methods for dimensionality reduction and similarity measures geared towards time series have been introduced. Each individual work introducing a particular method has made specific claims and, aside from the occasional theoretical justifications, provided quantitative experimental observations. However, for the most part, the comparative aspects of these experiments were too narrowly focused on demonstrating the benefits of the proposed methods over some of the previously introduced ones. In order to provide a comprehensive validation, we conducted an extensive set of time series experiments re-implementing 8 different representation methods and 9 similarity measures and their variants, and testing their effectiveness on 38 time series data sets from a wide variety of application domains. In this paper, we give an overview of these different techniques and present our comparative experimental findings regarding their effectiveness. Our experiments have provided both a unified validation of some of the existing achievements, and in some cases, suggested that certain claims in the literature may be unduly optimistic.},
  annotation = {01218}
}

@article{diodato_winning_2019,
  title = {Winning the {{ICCV}} 2019 {{Learning}} to {{Drive Challenge}}},
  author = {Diodato, Michael and Li, Yu and Goyal, Manik and Drori, Iddo},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.10318 [cs]},
  eprint = {1910.10318},
  primaryclass = {cs},
  urldate = {2020-02-19},
  abstract = {Autonomous driving has a significant impact on society. Predicting vehicle trajectories, specifically, angle and speed, is important for safe and comfortable driving. This work focuses on fusing inputs from camera sensors and visual map data which lead to significant improvement in performance and plays a key role in winning the challenge. We use pre-trained CNN's for processing image frames, a neural network for fusing the image representation with visual map data, and train a sequence model for time series prediction. We demonstrate the best performing MSE angle and best performance overall, to win the ICCV 2019 Learning to Drive challenge. We make our models and code publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LNYB7FQG\\Diodato et al. - 2019 - Winning the ICCV 2019 Learning to Drive Challenge.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\428KDKWK\\1910.html}
}

@inproceedings{doan_generating_2018,
  title = {Generating {{Realistic Sequences}} of {{Customer-Level Transactions}} for {{Retail Datasets}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  author = {Doan, Thang and Veira, Neil and Keng, Brian},
  year = {2018},
  month = nov,
  pages = {820--827},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2018.00122},
  abstract = {In order to better engage with customers, retailers rely on extensive customer and product databases which allows them to better understand customer behaviour and purchasing patterns. This has long been a challenging task as customer modelling is a multi-faceted, noisy and time-dependent problem. The most common way to tackle this problem is indirectly through task-specific supervised learning prediction problems, with relatively little literature on modelling a customer by directly simulating their future transactions. In this paper we propose a method for generating realistic sequences of baskets that a given customer is likely to purchase over a period of time. Customer embedding representations are learned using a Recurrent Neural Network (RNN) which takes into account the entire sequence of transaction data. Given the customer state at a specific point in time, a Generative Adversarial Network (GAN) is trained to generate a plausible basket of products for the following week. The newly generated basket is then fed back into the RNN to update the customer's state. The GAN is thus used in tandem with the RNN module in a pipeline alternating between basket generation and customer state updating steps. This allows for sampling over a distribution of a customer's future sequence of baskets, which then can be used to gain insight into how to service the customer more effectively. The methodology is empirically shown to produce baskets that appear similar to real baskets and enjoy many common properties, including frequencies of different product types, brands, and prices. Furthermore, the generated data is able to replicate most of the strongest sequential patterns that exist between product types in the real data.},
  keywords = {basket generation,consumer behaviour,customer behaviour,customer embedding representations,customer modelling,customer state,customer-level transactions,Data mining,Data models,Gallium nitride,generative adversarial network,Generative adversarial networks,{Generative Adversarial Networks, Customer embedding, Basket Generation, Retail},Generators,learning (artificial intelligence),purchasing,purchasing patterns,recurrent neural nets,recurrent neural network,Recurrent neural networks,retail data processing,retail datasets,RNN,Task analysis,task-specific supervised learning prediction problems,transaction data},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6GB525F5\\Doan et al. - 2018 - Generating Realistic Sequences of Customer-Level T.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\QJGZCFMQ\\8637567.html}
}

@article{dobra_secret_,
  title = {{{SECRET}}: {{A Scalable Linear Regression Tree Algorithm}}},
  author = {Dobra, Alin and Gehrke, Johannes},
  pages = {11},
  abstract = {Recently there has been an increasing interest in developing regression models for large datasets that are both accurate and easy to interpret. Regressors that have these properties are regression trees with linear models in the leaves, but so far, the algorithms proposed for constructing them are not scalable. In this paper we propose a novel regression tree construction algorithm that is both accurate and can truly scale to very large datasets. The main idea is, for every intermediate node, to use the EM algorithm for Gaussian mixtures to find two clusters in the data and to locally transform the regression problem into a classification problem based on closeness to these clusters. Goodness of split measures, like the gini gain, can then be used to determine the split variable and the split point much like in classification tree construction. Scalability of the algorithm can be enhanced by employing scalable versions of the EM and the classification tree construction algorithms. Tests on real and artificial data show that the proposed algorithm has accuracy comparable to other linear regression tree algorithms but requires orders of magnitude less computation time for large datasets.},
  langid = {english},
  annotation = {00109},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7NZUIMMD\Dobra and Gehrke - SECRET A Scalable Linear Regression Tree Algorith.pdf}
}

@article{doerr_probabilistic_2018,
  title = {Probabilistic {{Recurrent State-Space Models}}},
  author = {Doerr, Andreas and Daniel, Christian and Schiegg, Martin and {Nguyen-Tuong}, Duy and Schaal, Stefan and Toussaint, Marc and Trimpe, Sebastian},
  year = {2018},
  month = jan,
  journal = {arXiv:1801.10395 [stat]},
  eprint = {1801.10395},
  primaryclass = {stat},
  urldate = {2019-05-10},
  abstract = {State-space models (SSMs) are a highly expressive model class for learning patterns in time series data and for system identification. Deterministic versions of SSMs (e.g. LSTMs) proved extremely successful in modeling complex time series data. Fully probabilistic SSMs, however, are often found hard to train, even for smaller problems. To overcome this limitation, we propose a novel model formulation and a scalable training algorithm based on doubly stochastic variational inference and Gaussian processes. In contrast to existing work, the proposed variational approximation allows one to fully capture the latent state temporal correlations. These correlations are the key to robust training. The effectiveness of the proposed PR-SSM is evaluated on a set of real-world benchmark datasets in comparison to state-of-the-art probabilistic model learning methods. Scalability and robustness are demonstrated on a high dimensional problem.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Machine Learning},
  annotation = {00011},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EFY6LZUK\\Doerr et al. - 2018 - Probabilistic Recurrent State-Space Models.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\78SNKE67\\1801.html}
}

@article{dorogush_catboost_,
  title = {{{CatBoost}}: Gradient Boosting with Categorical Features Support},
  author = {Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey},
  pages = {7},
  abstract = {In this paper we present CatBoost, a new open-sourced gradient boosting library that successfully handles categorical features and outperforms existing publicly available implementations of gradient boosting in terms of quality on a set of popular publicly available datasets. The library has a GPU implementation of learning algorithm and a CPU implementation of scoring algorithm, which are significantly faster than other gradient boosting libraries on ensembles of similar sizes.},
  langid = {english},
  annotation = {00042},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RXBI3XUS\Dorogush et al. - CatBoost gradient boosting with categorical featu.pdf}
}

@incollection{dougherty_supervised_1995,
  title = {Supervised and {{Unsupervised Discretization}} of {{Continuous Features}}},
  booktitle = {Machine {{Learning Proceedings}} 1995},
  author = {Dougherty, James and Kohavi, Ron and Sahami, Mehran},
  year = {1995},
  pages = {194--202},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-55860-377-6.50032-3},
  urldate = {2019-09-09},
  abstract = {Many supervised machine learning algorithms require a discrete feature space. In this paper, we review previous work on continuous feature discretization, identify de ning characteristics of the methods, and conduct an empirical evaluation of several methods. We compare binning, an unsupervised discretization method, to entropy-based and purity-based methods, which are supervised algorithms. We found that the performance of the Naive-Bayes algorithm signi cantly improved when features were discretized using an entropy-based method. In fact, over the 16 tested datasets, the discretized version of Naive-Bayes slightly outperformed C4.5 on average. We also show that in some cases, the performance of the C4.5 induction algorithm signi cantly improved if features were discretized in advance in our experiments, the performance never signi cantly degraded, an interesting phenomenon considering the fact that C4.5 is capable of locally discretizing features.},
  isbn = {978-1-55860-377-6},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PYZCLRW8\Dougherty et al. - 1995 - Supervised and Unsupervised Discretization of Cont.pdf}
}

@article{dror_hitchhiker_,
  title = {The {{Hitchhiker}}'s {{Guide}} to {{Testing Statistical Significance}} in {{Natural Language Processing}}},
  author = {Dror, Rotem and Baumer, Gili and Shlomov, Segev and Reichart, Roi},
  pages = {10},
  abstract = {Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion, we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied in NLP research in a statistically sound manner1.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\S3Q4QVMV\Dror et al. - The Hitchhikerâ€™s Guide to Testing Statistical Sign.pdf}
}

@article{du_deep_,
  title = {Deep {{Air Quality Forecasting Using Hybrid Deep Learning Framework}}},
  author = {Du, Shengdong and Li, Tianrui},
  pages = {11},
  abstract = {Air quality forecasting has been regarded as the key problem of air pollution early warning and control management. In this paper, we propose a novel deep learning model for air quality (mainly PM2.5) forecasting, which learns the spatialtemporal correlation features and interdependence of multivariate air quality related time series data by hybrid deep learning architecture. Due to the nonlinear and dynamic characteristics of multivariate air quality time series data, the base modules of our model include one-dimensional Convolutional Neural Networks (CNN) and Bi-directional Long Short-term Memory networks (Bi-LSTM). The former is to extract the local trend features and the latter is to learn long temporal dependencies. Then we design a jointly hybrid deep learning framework which based on one-dimensional CNN and Bi-LSTM for shared representation features learning of multivariate air quality related time series data. The experiment results show that our model is capable of dealing with PM2.5 air pollution forecasting with satisfied accuracy.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\IW4WBBJG\Du and Li - Deep Air Quality Forecasting Using Hybrid Deep Lea.pdf}
}

@inproceedings{du_modeling_2017,
  title = {Modeling Approaches for Time Series Forecasting and Anomaly Detection},
  author = {Du, Shuyang and Pandey, Madhulima and Xing, Cuiqun},
  year = {2017},
  abstract = {Accurate time series forecasting is critical for business operations for optimal resource allocation, budget planning, anomaly detection and tasks such as predicting customer growth, or understanding stock market trends. This project focuses on applying machine learning techniques for forecasting on time series data. The dataset chosen is web traffic time series for Wikipedia webpages. We explore three different approaches including K-Nearest Neighbors (KNN), LSTM-based recurrent networks (LSTM), and Sequence to Sequence with Causal CNN (SeqtoSeq CNN). These approaches will be verified through error analyses using SMAPE and time series plots.},
  keywords = {Anomaly detection,HL7PublishingSubSection {$<$}operations{$>$},K-nearest neighbors algorithm,Long short-term memory,Machine learning,Projections and Predictions,read - not in related work,Resource Allocation,Silo (dataset),Time series,Web traffic,Wikipedia},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JZKCQJXM\Du et al. - 2017 - Modeling approaches for time series forecasting an.pdf}
}

@article{du_multivariate_2020,
  title = {Multivariate Time Series Forecasting via Attention-Based Encoder\textendash Decoder Framework},
  author = {Du, Shengdong and Li, Tianrui and Yang, Yan and Horng, Shi-Jinn},
  year = {2020},
  month = may,
  journal = {Neurocomputing},
  volume = {388},
  pages = {269--279},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.12.118},
  urldate = {2021-02-04},
  abstract = {Time series forecasting is an important technique to study the behavior of temporal data and forecast future values, which is widely applied in many fields, e.g. air quality forecasting, power load forecasting, medical monitoring, and intrusion detection. In this paper, we firstly propose a novel temporal attention encoder\textendash decoder model to deal with the multivariate time series forecasting problem. It is an end-to-end deep learning structure that integrates the traditional encode context vector and temporal attention vector for jointly temporal representation learning, which is based on bi-directional long short-term memory networks (Bi-LSTM) layers with temporal attention mechanism as the encoder network to adaptively learning long-term dependency and hidden correlation features of multivariate temporal data. Extensive experimental results on five typical multivariate time series datasets showed that our model has the best forecasting performance compared with baseline methods.},
  langid = {english},
  keywords = {Deep learning,Encoder\textendash decoder,Long short-term memory networks,Multi-step forecasting,Multivariate time series,Temporal attention},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\T3C7DDWW\\Du et al. - 2020 - Multivariate time series forecasting via attention.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BVVZICXT\\S0925231220300606.html}
}

@article{duan_ngboost_2020,
  title = {{{NGBoost}}: {{Natural Gradient Boosting}} for {{Probabilistic Prediction}}},
  shorttitle = {{{NGBoost}}},
  author = {Duan, Tony and Avati, Anand and Ding, Daisy and Thai, Khanh K. and Basu, Sanjay and Ng, Andrew and Schuler, Alejandro},
  year = {2020},
  journal = {ICML},
  urldate = {2020-08-31},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9H7MTHV8\\Duan et al. - 2020 - NGBoost Natural Gradient Boosting for Probabilist.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FCYPKBG2\\a4fa7175d4757e45eac71a8487751f63.html}
}

@article{duda_adaptive_2020,
  title = {Adaptive Exponential Power Distribution with Moving Estimator for Nonstationary Time Series},
  author = {Duda, Jarek},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.02149 [q-fin, stat]},
  eprint = {2003.02149},
  primaryclass = {q-fin, stat},
  urldate = {2020-03-11},
  abstract = {While standard estimation assumes that all datapoints are from probability distribution of the same fixed parameters \$\textbackslash theta\$, we will focus on maximum likelihood (ML) adaptive estimation for nonstationary time series: separately estimating parameters \$\textbackslash theta\_T\$ for each time \$T\$ based on the earlier values \$(x\_t)\_\{t\vphantom\}},
  archiveprefix = {arxiv},
  keywords = {Quantitative Finance - Statistical Finance,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CAZM5ZBQ\\Duda - 2020 - Adaptive exponential power distribution with movin.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YDGPZ7G4\\2003.html}
}

@article{dulac-arnold_challenges_2019,
  title = {Challenges of {{Real-World Reinforcement Learning}}},
  author = {{Dulac-Arnold}, Gabriel and Mankowitz, Daniel and Hester, Todd},
  year = {2019},
  month = apr,
  urldate = {2019-05-01},
  abstract = {Reinforcement learning (RL) has proven its worthin a series of artificial domains, and is beginningto show some successes in real-world scenarios.However, much of the research advances in RLare...},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EWBHD78I\\Dulac-Arnold et al. - 2019 - Challenges of Real-World Reinforcement Learning.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4B3P9I5Z\\forum.html}
}

@incollection{durante_locally_2013,
  title = {Locally {{Adaptive Bayesian Multivariate Time Series}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Durante, Daniele and Scarpa, Bruno and Dunson, David B},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  year = {2013},
  pages = {1664--1672},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\S4DG5YC9\\Durante et al. - 2013 - Locally Adaptive Bayesian Multivariate Time Series.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Q2KIKTIQ\\5115-locally-adaptive-bayesian-multivariate-time-series.html}
}

@article{dvurechensky_computational_2018,
  title = {Computational {{Optimal Transport}}: {{Complexity}} by {{Accelerated Gradient Descent Is Better Than}} by {{Sinkhorn}}'s {{Algorithm}}},
  shorttitle = {Computational {{Optimal Transport}}},
  author = {Dvurechensky, Pavel and Gasnikov, Alexander and Kroshnin, Alexey},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.04367 [cs, math]},
  eprint = {1802.04367},
  primaryclass = {cs, math},
  urldate = {2019-10-02},
  abstract = {We analyze two algorithms for approximating the general optimal transport (OT) distance between two discrete distributions of size \$n\$, up to accuracy \$\textbackslash varepsilon\$. For the first algorithm, which is based on the celebrated Sinkhorn's algorithm, we prove the complexity bound \$\textbackslash widetilde\{O\}\textbackslash left(\{n\^2/\textbackslash varepsilon\^2\}\textbackslash right)\$ arithmetic operations. For the second one, which is based on our novel Adaptive Primal-Dual Accelerated Gradient Descent (APDAGD) algorithm, we prove the complexity bound \$\textbackslash widetilde\{O\}\textbackslash left(\textbackslash min\textbackslash left\textbackslash\{n\^\{9/4\}/\textbackslash varepsilon, n\^\{2\}/\textbackslash varepsilon\^2 \textbackslash right\textbackslash\}\textbackslash right)\$ arithmetic operations. Both bounds have better dependence on \$\textbackslash varepsilon\$ than the state-of-the-art result given by \$\textbackslash widetilde\{O\}\textbackslash left(\{n\^2/\textbackslash varepsilon\^3\}\textbackslash right)\$. Our second algorithm not only has better dependence on \$\textbackslash varepsilon\$ in the complexity bound, but also is not specific to entropic regularization and can solve the OT problem with different regularizers.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Optimization and Control},
  annotation = {00030},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZDL42JDA\Dvurechensky et al. - 2018 - Computational Optimal Transport Complexity by Acc.pdf}
}

@article{eckert_forecasting_2021,
  title = {Forecasting {{Swiss}} Exports Using {{Bayesian}} Forecast Reconciliation},
  author = {Eckert, Florian and Hyndman, Rob J. and Panagiotelis, Anastasios},
  year = {2021},
  month = jun,
  journal = {European Journal of Operational Research},
  volume = {291},
  number = {2},
  pages = {693--710},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2020.09.046},
  urldate = {2021-10-18},
  abstract = {This paper proposes a novel forecast reconciliation framework using Bayesian state-space methods. It allows for the joint reconciliation at all forecast horizons and uses predictive distributions rather than past variation of forecast errors. Informative priors are used to assign weights to specific predictions, which makes it possible to reconcile forecasts such that they accommodate specific judgmental predictions or managerial decisions. The reconciled forecasts adhere to hierarchical constraints, which facilitates communication and supports aligned decision-making at all levels of complex hierarchical structures. An extensive forecasting study is conducted on a large collection of 13,118 time series that measure Swiss merchandise exports, grouped hierarchically by export destination and product category. We find strong evidence that in addition to producing coherent forecasts, reconciliation also leads to substantial improvements in forecast accuracy. The use of state-space methods is particularly promising for optimal decision-making under conditions with increased model uncertainty and data volatility.},
  langid = {english},
  keywords = {Decision-making,Forecasting,Hierarchical reconciliation,Optimal combination},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TXGL25DC\\Eckert et al. - 2021 - Forecasting Swiss exports using Bayesian forecast .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NM3WNEVT\\S037722172030850X.html}
}

@article{ehm_quantiles_2015,
  title = {Of {{Quantiles}} and {{Expectiles}}: {{Consistent Scoring Functions}}, {{Choquet Representations}}, and {{Forecast Rankings}}},
  shorttitle = {Of {{Quantiles}} and {{Expectiles}}},
  author = {Ehm, Werner and Gneiting, Tilmann and Jordan, Alexander and Kr{\"u}ger, Fabian},
  year = {2015},
  month = apr,
  journal = {arXiv:1503.08195 [math, stat]},
  eprint = {1503.08195},
  primaryclass = {math, stat},
  urldate = {2021-11-09},
  abstract = {In the practice of point prediction, it is desirable that forecasters receive a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. When evaluating and comparing competing forecasts, it is then critical that the scoring function used for these purposes be consistent for the functional at hand, in the sense that the expected score is minimized when following the directive.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TU996P88\Ehm et al. - 2015 - Of Quantiles and Expectiles Consistent Scoring Fu.pdf}
}

@incollection{elidan_copula_2010,
  title = {Copula {{Bayesian Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 23},
  author = {Elidan, Gal},
  editor = {Lafferty, J. D. and Williams, C. K. I. and {Shawe-Taylor}, J. and Zemel, R. S. and Culotta, A.},
  year = {2010},
  pages = {559--567},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-02-24},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\STMTV5RS\\Elidan - 2010 - Copula Bayesian Networks.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5P3QU7DK\\3956-copula-bayesian-networks.html}
}

@article{eroglu_multiplex_2018,
  title = {Multiplex {{Recurrence Networks}}},
  author = {Eroglu, Deniz and Marwan, Norbert and Stebich, Martina and Kurths, J{\"u}rgen},
  year = {2018},
  month = jan,
  journal = {Physical Review E},
  volume = {97},
  number = {1},
  eprint = {2003.03309},
  pages = {012312},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.97.012312},
  urldate = {2020-03-11},
  abstract = {We have introduced a novel multiplex recurrence network (MRN) approach by combining recurrence networks with the multiplex network approach in order to investigate multivariate time series. The potential use of this approach is demonstrated on coupled map lattices and a typical example from palaeobotany research. In both examples, topological changes in the multiplex recurrence networks allow for the detection of regime changes in their dynamics. The method goes beyond classical interpretation of pollen records by considering the vegetation as a whole and using the intrinsic similarity in the dynamics of the different regional vegetation elements. We find that the different vegetation types behave more similar when one environmental factor acts as the dominant driving force.},
  archiveprefix = {arxiv},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Nonlinear Sciences - Chaotic Dynamics,{Physics - Data Analysis, Statistics and Probability}},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4X9RASHK\\Eroglu et al. - 2018 - Multiplex Recurrence Networks.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XG6RK7YH\\2003.html}
}

@book{ester_proceedings_2018,
  title = {Proceedings of the 2018 {{SIAM International Conference}} on {{Data Mining}}},
  editor = {Ester, Martin and Pedreschi, Dino},
  year = {2018},
  month = may,
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia, PA}},
  doi = {10.1137/1.9781611975321},
  urldate = {2019-04-03},
  abstract = {Time series classification maps time series to labels. The nearest neighbour algorithm (NN) using the Dynamic Time Warping (DTW) similarity measure is a leading algorithm for this task and a component of the current best ensemble classifiers for time series. However, NN-DTW is only a winning combination when its meta-parameter \textendash{} its warping window \textendash{} is learned from the training data. The warping window (WW) intuitively controls the amount of distortion allowed when comparing a pair of time series. With a training database of N time series of lengths L, a naive approach to learning the WW requires {$\Theta$}(N 2{$\cdot$}L3) operations. This often translates in NN-DTW requiring days for training on datasets containing a few thousand time series only. In this paper, we introduce FastWWSearch: an efficient and exact method to learn WW. We show on 86 datasets that our method is always faster than the state of the art, with at least one order of magnitude and up to 1000x speed-up.},
  isbn = {978-1-61197-532-1},
  langid = {english},
  keywords = {read - not in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\48Q79B6M\Ester and Pedreschi - 2018 - Proceedings of the 2018 SIAM International Confere.pdf}
}

@article{evchenko_frugal_2021,
  title = {Frugal {{Machine Learning}}},
  author = {Evchenko, Mikhail and Vanschoren, Joaquin and Hoos, Holger H. and Schoenauer, Marc and Sebag, Mich{\`e}le},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.03731 [cs, eess]},
  eprint = {2111.03731},
  primaryclass = {cs, eess},
  urldate = {2021-12-06},
  abstract = {Machine learning, already at the core of increasingly many systems and applications, is set to become even more ubiquitous with the rapid rise of wearable devices and the Internet of Things. In most machine learning applications, the main focus is on the quality of the results achieved (e.g., prediction accuracy), and hence vast amounts of data are being collected, requiring significant computational resources to build models. In many scenarios, however, it is infeasible or impractical to set up large centralized data repositories. In personal health, for instance, privacy issues may inhibit the sharing of detailed personal data. In such cases, machine learning should ideally be performed on wearable devices themselves, which raises major computational limitations such as the battery capacity of smartwatches. This paper thus investigates frugal learning, aimed to build the most accurate possible models using the least amount of resources. A wide range of learning algorithms is examined through a frugal lens, analyzing their accuracy/runtime performance on a wide range of data sets. The most promising algorithms are thereafter assessed in a real-world scenario by implementing them in a smartwatch and letting them learn activity recognition models on the watch itself.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\B3DC8QVJ\\Evchenko et al. - 2021 - Frugal Machine Learning.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8N3S5BUP\\2111.html}
}

@article{faloutsos_forecasting_2018,
  title = {Forecasting Big Time Series: Old and New},
  shorttitle = {Forecasting Big Time Series},
  author = {Faloutsos, Christos and Gasthaus, Jan and Januschowski, Tim and Wang, Yuyang},
  year = {2018},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {11},
  number = {12},
  pages = {2102--2105},
  issn = {21508097},
  doi = {10.14778/3229863.3229878},
  urldate = {2019-10-02},
  abstract = {Time series forecasting is a key ingredient in the automation and optimization of business processes: in retail, deciding which products to order and where to store them depends on the forecasts of future demand in different regions; in cloud computing, the estimated future usage of services and infrastructure components guides capacity planning; and workforce scheduling in warehouses, call centers, factories requires forecasts of the future workload. Recent years have witnessed a paradigm shift in forecasting techniques and applications, from computer-assisted model- and assumptionbased to data-driven and fully-automated. This shift can be attributed to the availability of large, rich, and diverse time series data sources, posing unprecedented challenges to traditional time series forecasting methods. As such, how can we build statistical models to efficiently and effectively learn to forecast from large and diverse data sources? How can we leverage the statistical power of ``similar'' time series to improve forecasts in the case of limited observations? What are the implications for building forecasting systems that can handle large data volumes? The objective of this tutorial is to provide a concise and intuitive overview of the most important methods and tools available for solving large-scale forecasting problems. We review the state of the art in three related fields: (1) classical modeling of time series, (2) scalable tensor methods, and (3) deep learning for forecasting. Further, we share lessons learned from building scalable forecasting systems. While our focus is on providing an intuitive overview of the methods and practical issues, we also present technical details underlying these powerful tools.},
  langid = {english},
  annotation = {00008},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\A4D2AUAQ\Faloutsos et al. - 2018 - Forecasting big time series old and new.pdf}
}

@article{fan_overview_2015,
  title = {An {{Overview}} on the {{Estimation}} of {{Large Covariance}} and {{Precision Matrices}}},
  author = {Fan, Jianqing and Liao, Yuan and Liu, Han},
  year = {2015},
  month = apr,
  journal = {arXiv:1504.02995 [stat]},
  eprint = {1504.02995},
  primaryclass = {stat},
  urldate = {2020-03-13},
  abstract = {Estimating large covariance and precision matrices are fundamental in modern multivariate analysis. The problems arise from statistical analysis of large panel economics and finance data. The covariance matrix reveals marginal correlations between variables, while the precision matrix encodes conditional correlations between pairs of variables given the remaining variables. In this paper, we provide a selective review of several recent developments on estimating large covariance and precision matrices. We focus on two general approaches: rank based method and factor model based method. Theories and applications of both approaches are presented. These methods are expected to be widely applicable to analysis of economic and financial data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QBHTEMLM\Fan et al. - 2015 - An Overview on the Estimation of Large Covariance .pdf}
}

@article{fang_performance_2019,
  title = {Performance Enhancing Techniques for Deep Learning Models in Time Series Forecasting},
  author = {Fang, Xing and Yuan, Zhuoning},
  year = {2019},
  month = oct,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {85},
  pages = {533--542},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2019.07.011},
  urldate = {2021-02-04},
  abstract = {Time series forecasting uses deterministic algorithms to capture past temporal information or dependencies that can be used to predict future patterns. Studies have shown that traditional forecasting techniques are outperformed by deep learning models. Since then research work has been much focused on proposing different network models, little attention has been paid to improve the performance of existing models. In this paper, we compare the performance of several existing deep learning models used in both single and multiple time series forecasting tasks. We then propose two different approaches to improve the models' performance. Specifically, we present a fine-grained attention mechanism that achieves a much better performance for multi-step forecasting tasks. An ensemble technique is then proposed to further improve the performance of all the models.},
  langid = {english},
  keywords = {Deep learning,Ensemble,Fine-grained attention,Time series forecasting},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\GJY7IJND\Fang and Yuan - 2019 - Performance enhancing techniques for deep learning.pdf}
}

@article{fawaz_adversarial_2019,
  title = {Adversarial {{Attacks}} on {{Deep Neural Networks}} for {{Time Series Classification}}},
  author = {Fawaz, H. Ismail and Forestier, G. and Weber, J. and Idoumghar, L. and Muller, P.},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.07054 [cs, stat]},
  eprint = {1903.07054},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {Time Series Classification (TSC) problems are encountered in many real life data mining tasks ranging from medicine and security to human activity recognition and food safety. With the recent success of deep neural networks in various domains such as computer vision and natural language processing, researchers started adopting these techniques for solving time series data mining problems. However, to the best of our knowledge, no previous work has considered the vulnerability of deep learning models to adversarial time series examples, which could potentially make them unreliable in situations where the decision taken by the classifier is crucial such as in medicine and security. For computer vision problems, such attacks have been shown to be very easy to perform by altering the image and adding an imperceptible amount of noise to trick the network into wrongly classifying the input image. Following this line of work, we propose to leverage existing adversarial attack mechanisms to add a special noise to the input time series in order to decrease the network's confidence when classifying instances at test time. Our results reveal that current state-of-the-art deep learning time series classifiers are vulnerable to adversarial attacks which can have major consequences in multiple domains such as food safety and quality assurance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,read - not in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FFX8LRCR\\Fawaz et al. - 2019 - Adversarial Attacks on Deep Neural Networks for Ti.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YWZ9U6C5\\1903.html}
}

@misc{feldman_achieving_2023,
  title = {Achieving {{Risk Control}} in {{Online Learning Settings}}},
  author = {Feldman, Shai and Ringel, Liran and Bates, Stephen and Romano, Yaniv},
  year = {2023},
  month = jan,
  number = {arXiv:2205.09095},
  eprint = {2205.09095},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.09095},
  urldate = {2023-03-07},
  abstract = {To provide rigorous uncertainty quantification for online learning models, we develop a framework for constructing uncertainty sets that provably control risk -- such as coverage of confidence intervals, false negative rate, or F1 score -- in the online setting. This extends conformal prediction to apply to a larger class of online learning problems. Our method guarantees risk control at any user-specified level even when the underlying data distribution shifts drastically, even adversarially, over time in an unknown fashion. The technique we propose is highly flexible as it can be applied with any base online learning algorithm (e.g., a deep neural network trained online), requiring minimal implementation effort and essentially zero additional computational cost. We further extend our approach to control multiple risks simultaneously, so the prediction sets we generate are valid for all given risks. To demonstrate the utility of our method, we conduct experiments on real-world tabular time-series data sets showing that the proposed method rigorously controls various natural risks. Furthermore, we show how to construct valid intervals for an online image-depth estimation problem that previous sequential calibration schemes cannot handle.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9NGVEUUT\\Feldman et al. - 2023 - Achieving Risk Control in Online Learning Settings.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\QYPT8BBK\\2205.html}
}

@inproceedings{feldman_improving_2021,
  title = {Improving {{Conditional Coverage}} via {{Orthogonal Quantile Regression}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Feldman, Shai and Bates, Stephen and Romano, Yaniv},
  year = {2021},
  volume = {34},
  pages = {2060--2071},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-07},
  abstract = {We develop a method to generate prediction intervals that have a user-specified coverage level across all regions of feature-space, a property called conditional coverage. A typical approach to this task is to estimate the conditional quantiles with quantile regression---it is well-known that this leads to correct coverage in the large-sample limit, although it may not be accurate in finite samples. We find in experiments that traditional quantile regression can have poor conditional coverage. To remedy this, we modify the loss function to promote independence between the size of the intervals and the indicator of a miscoverage event. For the true conditional quantiles, these two quantities are independent (orthogonal), so the modified loss function continues to be valid. Moreover, we empirically show that the modified loss function leads to improved conditional coverage, as evaluated by several metrics. We also introduce two new metrics that check conditional coverage by looking at the strength of the dependence between the interval size and the indicator of miscoverage.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MH8JFZJ6\Feldman et al. - 2021 - Improving Conditional Coverage via Orthogonal Quan.pdf}
}

@article{feng_autoencoder_,
  title = {{{AutoEncoder}} by {{Forest}}},
  author = {Feng, Ji and Zhou, Zhi-Hua},
  pages = {7},
  abstract = {Auto-encoding is an important task which is typically realized by deep neural networks (DNNs) such as convolutional neural networks (CNN). In this paper, we propose EncoderForest (abbrv. eForest), the first tree ensemble based auto-encoder. We present a procedure for enabling forests to do backward reconstruction by utilizing the MaximalCompatible Rule (MCR) defined by the decision paths of the trees, and demonstrate its usage in both supervised and unsupervised setting. Experiments show that, compared with DNN based auto-encoders, eForest is able to obtain lower reconstruction error with fast training speed, while the model itself is reusable and damage-tolerable.},
  langid = {english},
  annotation = {00014},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7CW5ITF4\Feng and Zhou - AutoEncoder by Forest.pdf}
}

@inproceedings{feng_convolutional_2018,
  title = {A {{Convolutional Sequential Model}} for {{Network Load Forecasting}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Internet Multimedia Computing}} and {{Service}}},
  author = {Feng, Yan and Wu, Zhou and Wu, Huizhe},
  year = {2018},
  series = {{{ICIMCS}} '18},
  pages = {25:1--25:6},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3240876.3240897},
  urldate = {2019-04-03},
  abstract = {With the development of cloud computing and big data, more and more applications are deployed in the cloud. Data center, as a management unit of computing and storage resources, consists of quantities of computing clusters, and the scale of clusters will be increased as demand grows. Even the same application will have a totally different impact on the network load when it is under clusters with different sizes, for the topology of the clusters is changed. Therefore, it is meaningful to do research on how to make use of the load of network from an application under a small cluster to predict the network load under a larger cluster, which will guide the data center on its infrastructure. In this paper, first of all, we address a new network load forecasting problem and collect network load generated by the operations of a designated computing intensive task (Terasort and GroupBy). Then we analyze the data and put forward the difficulties of the new problem. Finally, with a given set of data, we test the traditional machine learning models and the deep learning models respectively and provide a simple but effective model.},
  isbn = {978-1-4503-6520-8},
  keywords = {deep learning,load forecasting,read - not in related work,sequence analysis},
  annotation = {00000}
}

@article{feng_improving_2018,
  title = {Improving {{Stock Movement Prediction}} with {{Adversarial Training}}},
  author = {Feng, Fuli and Chen, Huimin and He, Xiangnan and Ding, Ji and Sun, Maosong and Chua, Tat-Seng},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.09936 [cs, q-fin]},
  eprint = {1810.09936},
  primaryclass = {cs, q-fin},
  urldate = {2019-04-03},
  abstract = {This paper contributes a new machine learning solution for stock movement prediction, which aims to predict whether the price of a stock will be up or down in the near future. The key novelty is that we propose to employ adversarial training to improve the generalization of a recurrent neural network model. The rationality of adversarial training here is that the input features to stock prediction are typically based on stock price, which is essentially a stochastic variable and continuously changed with time by nature. As such, normal training with stationary price-based features (e.g., the closing price) can easily overfit the data, being insufficient to obtain reliable models. To address this problem, we propose to add perturbations to simulate the stochasticity of continuous price variable, and train the model to work well under small yet intentional perturbations. Extensive experiments on two real-world stock data show that our method outperforms the state-of-the-art solution (Xu and Cohen 2018) with 3.11\% relative improvements on average w.r.t. accuracy, verifying the usefulness of adversarial training for stock prediction task. Codes will be made available upon acceptance1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {{Computer Science - Computational Engineering, Finance, and Science},Computer Science - Machine Learning,Quantitative Finance - Trading and Market Microstructure,read - in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5MJ7H7QW\Feng et al. - 2018 - Improving Stock Movement Prediction with Adversari.pdf}
}

@incollection{feng_multilayered_2018,
  title = {Multi-{{Layered Gradient Boosting Decision Trees}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Feng, Ji and Yu, Yang and Zhou, Zhi-Hua},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {3551--3561},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-08-30},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XIEJXVHJ\\Feng et al. - 2018 - Multi-Layered Gradient Boosting Decision Trees.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UMHQXPDQ\\7614-multi-layered-gradient-boosting-decision-trees.html}
}

@article{fischer_deep_2018,
  title = {Deep Learning with Long Short-Term Memory Networks for Financial Market Predictions},
  author = {Fischer, Thomas and Krauss, Christopher},
  year = {2018},
  month = oct,
  journal = {European Journal of Operational Research},
  volume = {270},
  number = {2},
  pages = {654--669},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2017.11.054},
  urldate = {2019-04-26},
  abstract = {Long short-term memory (LSTM) networks are a state-of-the-art technique for sequence learning. They are less commonly applied to financial time series predictions, yet inherently suitable for this domain. We deploy LSTM networks for predicting out-of-sample directional movements for the constituent stocks of the S\&P 500 from 1992 until 2015. With daily returns of 0.46 percent and a Sharpe ratio of 5.8 prior to transaction costs, we find LSTM networks to outperform memory-free classification methods, i.e., a random forest (RAF), a deep neural net (DNN), and a logistic regression classifier (LOG). The outperformance relative to the general market is very clear from 1992 to 2009, but as of 2010, excess returns seem to have been arbitraged away with LSTM profitability fluctuating around zero after transaction costs. We further unveil sources of profitability, thereby shedding light into the black box of artificial neural networks. Specifically, we find one common pattern among the stocks selected for trading \textendash{} they exhibit high volatility and a short-term reversal return profile. Leveraging these findings, we are able to formalize a rules-based short-term reversal strategy that yields 0.23 percent prior to transaction costs. Further regression analysis unveils low exposure of the LSTM returns to common sources of systematic risk \textendash{} also compared to the three benchmark models.},
  keywords = {Deep learning,Finance,LSTM,Machine learning,read - in related work,Statistical arbitrage},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MNASKXFA\\Fischer and Krauss - 2018 - Deep learning with long short-term memory networks.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DGWTZ6XG\\S0377221717310652.html}
}

@inproceedings{fischer_samplebased_2012,
  title = {Sample-Based Forecasting Exploiting Hierarchical Time Series},
  booktitle = {Proceedings of the 16th {{International Database Engineering}} \& {{Applications Sysmposium}}},
  author = {Fischer, Ulrike and Rosenthal, Frank and Lehner, Wolfgang},
  year = {2012},
  month = aug,
  series = {{{IDEAS}} '12},
  pages = {120--129},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2351476.2351490},
  urldate = {2021-02-04},
  abstract = {Time series forecasting is challenging as sophisticated forecast models are computationally expensive to build. Recent research has addressed the integration of forecasting inside a DBMS. One main benefit is that models can be created once and then repeatedly used to answer forecast queries. Often forecast queries are submitted on higher aggregation levels, e. g., forecasts of sales over all locations. To answer such a forecast query, we have two possibilities. First, we can aggregate all base time series (sales in Austria, sales in Belgium...) and create only one model for the aggregate time series. Second, we can create models for all base time series and aggregate the base forecast values. The second possibility might lead to a higher accuracy but it is usually too expensive due to a high number of base time series. However, we actually do not need all base models to achieve a high accuracy, a sample of base models is enough. With this approach, we still achieve a better accuracy than an aggregate model, very similar to using all models, but we need less models to create and maintain in the database. We further improve this approach if new actual values of the base time series arrive at different points in time. With each new actual value we can refine the aggregate forecast and eventually converge towards the real actual value. Our experimental evaluation using several real-world data sets, shows a high accuracy of our approaches and a fast convergence towards the optimal value with increasing sample sizes and increasing number of actual values respectively.},
  isbn = {978-1-4503-1234-9},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BH4TSVGV\Fischer et al. - 2012 - Sample-based forecasting exploiting hierarchical t.pdf}
}

@article{fisher_improved_2011,
  title = {Improved {{Stein-type}} Shrinkage Estimators for the High-Dimensional Multivariate Normal Covariance Matrix},
  author = {Fisher, Thomas J. and Sun, Xiaoqian},
  year = {2011},
  month = may,
  journal = {Computational Statistics \& Data Analysis},
  volume = {55},
  number = {5},
  pages = {1909--1918},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2010.12.006},
  urldate = {2022-08-10},
  abstract = {Many applications require an estimate for the covariance matrix that is non-singular and well-conditioned. As the dimensionality increases, the sample covariance matrix becomes ill-conditioned or even singular. A common approach to estimating the covariance matrix when the dimensionality is large is that of Stein-type shrinkage estimation. A convex combination of the sample covariance matrix and a well-conditioned target matrix is used to estimate the covariance matrix. Recent work in the literature has shown that an optimal combination exists under mean-squared loss, however it must be estimated from the data. In this paper, we introduce a new set of estimators for the optimal convex combination for three commonly used target matrices. A simulation study shows an improvement over those in the literature in cases of extreme high-dimensionality of the data. A data analysis shows the estimators are effective in a discriminant and classification analysis.},
  langid = {english},
  keywords = {Covariance matrix,High-dimensional data analysis,Shrinkage estimation},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HP2SCPG7\\Fisher and Sun - 2011 - Improved Stein-type shrinkage estimators for the h.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RQ6BYM2U\\S0167947310004743.html}
}

@inproceedings{fong_conformal_2021,
  title = {Conformal {{Bayesian Computation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fong, Edwin and Holmes, Chris C},
  year = {2021},
  volume = {34},
  pages = {18268--18279},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-07},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZCPVYYSV\Fong and Holmes - 2021 - Conformal Bayesian Computation.pdf}
}

@article{fontana_conformal_2023,
  title = {Conformal Prediction: {{A}} Unified Review of Theory and New Challenges},
  shorttitle = {Conformal Prediction},
  author = {Fontana, Matteo and Zeni, Gianluca and Vantini, Simone},
  year = {2023},
  month = feb,
  journal = {Bernoulli},
  volume = {29},
  number = {1},
  pages = {1--23},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  doi = {10.3150/21-BEJ1447},
  urldate = {2023-03-07},
  abstract = {In this work we provide a review of basic ideas and novel developments about Conformal Prediction \textemdash{} an innovative distribution-free, non-parametric forecasting method, based on minimal assumptions \textemdash{} that is able to yield in a very straightforward way prediction sets that are valid in a statistical sense also in the finite sample case. The discussion provided in the paper covers the theoretical underpinnings of Conformal Prediction, and then proceeds to list the more advanced developments and adaptations of the original idea.},
  keywords = {conformal prediction,nonparametric statistics,prediction intervals,review},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2SNWG7IY\Fontana et al. - 2023 - Conformal prediction A unified review of theory a.pdf}
}

@article{fortuin_gpvae_2019,
  title = {{{GP-VAE}}: {{Deep Probabilistic Time Series Imputation}}},
  shorttitle = {{{GP-VAE}}},
  author = {Fortuin, Vincent and Baranchuk, Dmitry and R{\"a}tsch, Gunnar and Mandt, Stephan},
  year = {2019},
  month = oct,
  journal = {arXiv:1907.04155 [cs, stat]},
  eprint = {1907.04155},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, na\"ive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TJ3QVJ9W\Fortuin et al. - 2019 - GP-VAE Deep Probabilistic Time Series Imputation.pdf}
}

@article{fortuin_gpvae_2019a,
  title = {{{GP-VAE}}: {{Deep Probabilistic Time Series Imputation}}},
  shorttitle = {{{GP-VAE}}},
  author = {Fortuin, Vincent and Baranchuk, Dmitry and R{\"a}tsch, Gunnar and Mandt, Stephan},
  year = {2019},
  month = oct,
  journal = {arXiv:1907.04155 [cs, stat]},
  eprint = {1907.04155},
  primaryclass = {cs, stat},
  urldate = {2019-10-28},
  abstract = {Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, na\"ive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\9568GPAS\Fortuin et al. - 2019 - GP-VAE Deep Probabilistic Time Series Imputation.pdf}
}

@article{fox_deep_2018,
  title = {Deep {{Multi-Output Forecasting}}: {{Learning}} to {{Accurately Predict Blood Glucose Trajectories}}},
  shorttitle = {Deep {{Multi-Output Forecasting}}},
  author = {Fox, Ian and Ang, Lynn and Jaiswal, Mamta and {Pop-Busui}, Rodica and Wiens, Jenna},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.05357 [cs, stat]},
  eprint = {1806.05357},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {In many forecasting applications, it is valuable to predict not only the value of a signal at a certain time point in the future, but also the values leading up to that point. This is especially true in clinical applications, where the future state of the patient can be less important than the patient's overall trajectory. This requires multi-step forecasting, a forecasting variant where one aims to predict multiple values in the future simultaneously. Standard methods to accomplish this can propagate error from prediction to prediction, reducing quality over the long term. In light of these challenges, we propose multi-output deep architectures for multi-step forecasting in which we explicitly model the distribution of future values of the signal over a prediction horizon. We apply these techniques to the challenging and clinically relevant task of blood glucose forecasting. Through a series of experiments on a real-world dataset consisting of 550K blood glucose measurements, we demonstrate the effectiveness of our proposed approaches in capturing the underlying signal dynamics. Compared to existing shallow and deep methods, we find that our proposed approaches improve performance individually and capture complementary information, leading to a large improvement over the baseline when combined (4.87 vs. 5.31 absolute percentage error (APE)). Overall, the results suggest the efficacy of our proposed approach in predicting blood glucose level and multi-step forecasting more generally.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00004},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DWIL4HDV\\Fox et al. - 2018 - Deep Multi-Output Forecasting Learning to Accurat.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\K5978LEZ\\1806.html}
}

@incollection{fraccaro_disentangled_2017,
  title = {A {{Disentangled Recognition}} and {{Nonlinear Dynamics Model}} for {{Unsupervised Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Fraccaro, Marco and Kamronn, Simon and Paquet, Ulrich and Winther, Ole},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {3601--3610},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-05-10},
  annotation = {00035},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\F6U9M95D\\Fraccaro et al. - 2017 - A Disentangled Recognition and Nonlinear Dynamics .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Y9TWS2ET\\6951-a-disentangled-recognition-and-nonlinear-dynamics-model-for-unsupervised-learning.html}
}

@article{frazier_tutorial_2018,
  title = {A {{Tutorial}} on {{Bayesian Optimization}}},
  author = {Frazier, Peter I.},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.02811 [cs, math, stat]},
  eprint = {1807.02811},
  primaryclass = {cs, math, stat},
  urldate = {2019-05-20},
  abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  annotation = {00017},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\AXKJ47X9\Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf}
}

@article{friedberg_local_2018,
  title = {Local {{Linear Forests}}},
  author = {Friedberg, Rina and Tibshirani, Julie and Athey, Susan and Wager, Stefan},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.11408 [cs, econ, math, stat]},
  eprint = {1807.11408},
  primaryclass = {cs, econ, math, stat},
  urldate = {2019-09-06},
  abstract = {Random forests are a powerful method for non-parametric regression, but are limited in their ability to fit smooth signals, and can show poor predictive performance in the presence of strong, smooth effects. Taking the perspective of random forests as an adaptive kernel method, we pair the forest kernel with a local linear regression adjustment to better capture smoothness. The resulting procedure, local linear forests, enables us to improve on asymptotic rates of convergence for random forests with smooth signals, and provides substantial gains in accuracy on both real and simulated data. We prove a central limit theorem valid under regularity conditions on the forest and smoothness constraints, and propose a computationally efficient construction for confidence intervals. Moving to a causal inference application, we discuss the merits of local regression adjustments for heterogeneous treatment effect estimation, and give an example on a dataset exploring the effect word choice has on attitudes to the social safety net. Last, we include simulation results on real and generated data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,Mathematics - Statistics Theory,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PLCH5479\Friedberg et al. - 2018 - Local Linear Forests.pdf}
}

@article{friedman_greedy_2001,
  title = {Greedy {{Function Approximation}}: {{A Gradient Boosting Machine}}},
  shorttitle = {Greedy {{Function Approximation}}},
  author = {Friedman, Jerome H.},
  year = {2001},
  journal = {The Annals of Statistics},
  volume = {29},
  number = {5},
  eprint = {2699986},
  eprinttype = {jstor},
  pages = {1189--1232},
  issn = {0090-5364},
  urldate = {2019-04-15},
  abstract = {[Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.]},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HBHM3HT5\trebst.pdf}
}

@inproceedings{frigola_variational_2014,
  title = {Variational {{Gaussian Process State-space Models}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Frigola, Roger and Chen, Yutian and Rasmussen, Carl E.},
  year = {2014},
  series = {{{NIPS}}'14},
  pages = {3680--3688},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  urldate = {2019-04-17},
  abstract = {State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.},
  keywords = {read - in related work},
  annotation = {00070},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\A5DBEKGP\Frigola et al. - Variational Gaussian Process State-Space Models.pdf}
}

@article{frosst_distilling_2017,
  title = {Distilling a {{Neural Network Into}} a {{Soft Decision Tree}}},
  author = {Frosst, Nicholas and Hinton, Geoffrey},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.09784 [cs, stat]},
  eprint = {1711.09784},
  primaryclass = {cs, stat},
  urldate = {2019-09-05},
  abstract = {Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00089},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\S5XEG5DN\\Frosst and Hinton - 2017 - Distilling a Neural Network Into a Soft Decision T.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5M7MVGTV\\1711.html}
}

@article{frye_asymmetric_2019,
  title = {Asymmetric {{Shapley}} Values: Incorporating Causal Knowledge into Model-Agnostic Explainability},
  shorttitle = {Asymmetric {{Shapley}} Values},
  author = {Frye, Christopher and Feige, Ilya and Rowat, Colin},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.06358 [cs, stat]},
  eprint = {1910.06358},
  primaryclass = {cs, stat},
  urldate = {2020-02-19},
  abstract = {Explaining AI systems is fundamental both to the development of high performing models and to the trust placed in them by their users. A general framework for explaining any AI model is provided by the Shapley values that attribute the prediction output to the various model inputs ("features") in a principled and model-agnostic way. The outstanding strength of Shapley values is their combined generality and rigorous foundation: they can be used to explain any AI system, and one always understands their values as the unique attribution method satisfying a set of mathematical axioms. However, as a framework, Shapley values are too restrictive in one significant regard: they ignore all causal structure in the data. We introduce a less-restrictive framework for model-agnostic explainability: "Asymmetric" Shapley values. Asymmetric Shapley values (ASVs) are rigorously founded on a set of axioms, applicable to any AI system, and can flexibly incorporate any causal knowledge known a-priori to be respected by the data. We show through explicit, realistic examples that the ASV framework can be used to (i) improve model explanations by incorporating causal information, (ii) provide an unambiguous test for unfair discrimination based on simple policy articulations, (iii) enable sequentially incremental explanations in time-series models, and (iv) support feature-selection studies without the need for model retraining.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\JV4JXG3T\\Frye et al. - 2019 - Asymmetric Shapley values incorporating causal kn.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GEKXT677\\1910.html}
}

@inproceedings{fu_spatiotemporal_2019,
  title = {Spatiotemporal {{Attention Networks}} for {{Wind Power Forecasting}}},
  booktitle = {2019 {{International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  author = {Fu, Xingbo and Gao, Feng and Wu, Jiang and Wei, Xinyu and Duan, Fangwei},
  year = {2019},
  month = nov,
  pages = {149--154},
  publisher = {{IEEE}},
  address = {{Beijing, China}},
  doi = {10.1109/ICDMW.2019.00032},
  urldate = {2020-02-17},
  abstract = {Wind power is one of the most important renewable energy sources and accurate wind power forecasting is very significant for reliable and economic power system operation and control strategies. This paper proposes a novel framework with spatiotemporal attention networks (STAN) for wind power forecasting. This model captures spatial correlations among wind farms and temporal dependencies of wind power time series. First of all, we employ a multi-head self-attention mechanism to extract spatial correlations among wind farms. Then, temporal dependencies are captured by the Sequence-to-Sequence (Seq2Seq) model with a global attention mechanism. Finally, experimental results demonstrate that our model achieves better performance than other baseline approaches. Our work provides useful insights to capture non-Euclidean spatial correlations.},
  isbn = {978-1-72814-896-0},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PR8BIL8V\Fu et al. - 2019 - Spatiotemporal Attention Networks for Wind Power F.pdf}
}

@article{gallicchio_comparison_2019,
  title = {Comparison between {{DeepESNs}} and Gated {{RNNs}} on Multivariate Time-Series Prediction},
  author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
  year = {2019},
  month = nov,
  journal = {arXiv:1812.11527 [cs, stat]},
  eprint = {1812.11527},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {We propose an experimental comparison between Deep Echo State Networks (DeepESNs) and gated Recurrent Neural Networks (RNNs) on multivariate time-series prediction tasks. In particular, we compare reservoir and fully-trained RNNs able to represent signals featured by multiple time-scales dynamics. The analysis is performed in terms of efficiency and prediction accuracy on 4 polyphonic music tasks. Our results show that DeepESN is able to outperform ESN in terms of prediction accuracy and efficiency. Whereas, between fully-trained approaches, Gated Recurrent Units (GRU) outperforms Long Short-Term Memory (LSTM) and simple RNN models in most cases. Overall, DeepESN turned out to be extremely more efficient than others RNN approaches and the best solution in terms of prediction accuracy on 3 out of 4 tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\X8EQZJVZ\\Gallicchio et al. - 2019 - Comparison between DeepESNs and gated RNNs on mult.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3WPRRCAV\\1812.html}
}

@article{gamakumara_probabilistic_,
  title = {Probabilistic {{Forecasts}} in {{Hierarchical Time Series}}},
  author = {Gamakumara, Puwasala and Panagiotelis, Anastasios and Athanasopoulos, George and Hyndman, Rob J},
  pages = {28},
  abstract = {Forecast reconciliation involves adjusting forecasts to ensure coherence with aggregation constraints. We extend this concept from point forecasts to probabilistic forecasts by redefining forecast reconciliation in terms of linear functions in general, and projections more specifically. New theorems establish that the true predictive distribution can be recovered in the elliptical case by linear reconciliation, and general conditions are derived for when this is a projection. A geometric interpretation is also used to prove two new theoretical results for point forecasting; that reconciliation via projection both preserves unbiasedness and dominates unreconciled forecasts in a mean squared error sense. Strategies for forecast evaluation based on scoring rules are discussed, and it is shown that the popular log score is an improper scoring rule with respect to the class of unreconciled forecasts when the true predictive distribution coheres with aggregation constraints. Finally, evidence from a simulation study shows that reconciliation based on an oblique projection, derived from the MinT method of Wickramasuriya, Athanasopoulos \& Hyndman (2018) for point forecasting, outperforms both reconciled and unreconciled alternatives.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\KESUJBBF\Gamakumara et al. - Probabilistic Forecasts in Hierarchical Time Serie.pdf}
}

@article{gamboa_deep_2017,
  title = {Deep {{Learning}} for {{Time-Series Analysis}}},
  author = {Gamboa, John Cristian Borges},
  year = {2017},
  month = jan,
  journal = {arXiv:1701.01887 [cs]},
  eprint = {1701.01887},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {In many real-world application, e.g., speech recognition or sleep stage classification, data are captured over the course of time, constituting a Time-Series. Time-Series often contain temporal dependencies that cause two otherwise identical points of time to belong to different classes or predict different behavior. This characteristic generally increases the difficulty of analysing them. Existing techniques often depended on hand-crafted features that were expensive to create and required expert knowledge of the field. With the advent of Deep Learning new models of unsupervised learning of features for Time-series analysis and forecast have been developed. Such new developments are the topic of this paper: a review of the main Deep Learning techniques is presented, and some applications on Time-Series analysis are summaried. The results make it clear that Deep Learning has a lot to contribute to the field.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\K83RI733\\Gamboa - 2017 - Deep Learning for Time-Series Analysis.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8DN38SJ4\\1701.html}
}

@inproceedings{gan_deep_2015,
  title = {Deep {{Temporal Sigmoid Belief Networks}} for {{Sequence Modeling}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Gan, Zhe and Li, Chunyuan and Henao, Ricardo and Carlson, David and Carin, Lawrence},
  year = {2015},
  series = {{{NIPS}}'15},
  pages = {2467--2475},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  urldate = {2019-04-17},
  abstract = {Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences.},
  keywords = {read - not in related work},
  annotation = {00047},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\S7PEDJLY\Gan et al. - Deep Temporal Sigmoid Belief Networks for Sequence.pdf}
}

@article{ganea_hyperbolic_,
  title = {Hyperbolic {{Neural Networks}}},
  author = {Ganea, Octavian-Eugen and B{\'e}cigneul, Gary and Hofmann, Thomas},
  pages = {11},
  abstract = {Hyperbolic spaces have recently gained momentum in the context of machine learning due to their high capacity and tree-likeliness properties. However, the representational power of hyperbolic geometry is not yet on par with Euclidean geometry, mostly because of the absence of corresponding hyperbolic neural network layers. This makes it hard to use hyperbolic embeddings in downstream tasks. Here, we bridge this gap in a principled manner by combining the formalism of M\"obius gyrovector spaces with the Riemannian geometry of the Poincar\'e model of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep learning tools: multinomial logistic regression, feed-forward and recurrent neural networks such as gated recurrent units. This allows to embed sequential data and perform classification in the hyperbolic space. Empirically, we show that, even if hyperbolic optimization tools are limited, hyperbolic sentence embeddings either outperform or are on par with their Euclidean variants on textual entailment and noisy-prefix recognition tasks.},
  langid = {english},
  annotation = {00021},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\8QZ52KTW\Ganea et al. - Hyperbolic Neural Networks.pdf}
}

@article{gao_deep_2018,
  title = {Deep Reinforcement Learning for Time Series: Playing Idealized Trading Games},
  shorttitle = {Deep Reinforcement Learning for Time Series},
  author = {Gao, Xiang},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.03916 [cs, stat]},
  eprint = {1803.03916},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {Deep Q-learning is investigated as an end-to-end solution to estimate the optimal strategies for acting on time series input. Experiments are conducted on two idealized trading games. 1) Univariate: the only input is a wave-like price time series, and 2) Bivariate: the input includes a random stepwise price time series and a noisy signal time series, which is positively correlated with future price changes. The Univariate game tests whether the agent can capture the underlying dynamics, and the Bivariate game tests whether the agent can utilize the hidden relation among the inputs. Stacked Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM) units, Convolutional Neural Network (CNN), and multi-layer perceptron (MLP) are used to model Q values. For both games, all agents successfully find a profitable strategy. The GRU-based agents show best overall performance in the Univariate game, while the MLP-based agents outperform others in the Bivariate game.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\H6AERHTV\\Gao - 2018 - Deep reinforcement learning for time series playi.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7MFKKH7V\\1803.html}
}

@article{gashler_modeling_2016,
  title = {Modeling Time Series Data with Deep {{Fourier}} Neural Networks},
  author = {Gashler, Michael S. and Ashmore, Stephen C.},
  year = {2016},
  month = may,
  journal = {Neurocomputing},
  series = {Advanced {{Intelligent Computing Methodologies}} and {{Applications}}},
  volume = {188},
  pages = {3--11},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2015.01.108},
  urldate = {2021-01-21},
  abstract = {We present a method for training a deep neural network containing sinusoidal activation functions to fit to time-series data. Weights are initialized using a fast Fourier transform, then trained with regularization to improve generalization. A simple dynamic parameter tuning method is employed to adjust both the learning rate and the regularization term, such that both stability and efficient training are achieved. We show how deeper layers can be utilized to model the observed sequence using a sparser set of sinusoid units, and how non-uniform regularization can improve generalization by promoting the shifting of weight toward simpler units. The method is demonstrated with time-series problems to show that it leads to effective extrapolation of nonlinear trends.},
  langid = {english},
  keywords = {Curve fitting,Extrapolation,Fourier decomposition,Neural networks,Time-series},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YXU9QAS6\\Gashler and Ashmore - 2016 - Modeling time series data with deep Fourier neural.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MRQIXCAK\\S0925231215017774.html}
}

@article{gashler_training_2014,
  title = {Training {{Deep Fourier Neural Networks To Fit Time-Series Data}}},
  author = {Gashler, Michael S. and Ashmore, Stephen C.},
  year = {2014},
  month = may,
  journal = {arXiv:1405.2262 [cs]},
  eprint = {1405.2262},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {We present a method for training a deep neural network containing sinusoidal activation functions to fit to time-series data. Weights are initialized using a fast Fourier transform, then trained with regularization to improve generalization. A simple dynamic parameter tuning method is employed to adjust both the learning rate and regularization term, such that stability and efficient training are both achieved. We show how deeper layers can be utilized to model the observed sequence using a sparser set of sinusoid units, and how non-uniform regularization can improve generalization by promoting the shifting of weight toward simpler units. The method is demonstrated with time-series problems to show that it leads to effective extrapolation of nonlinear trends.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ACT55RP4\\Gashler and Ashmore - 2014 - Training Deep Fourier Neural Networks To Fit Time-.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\S2W2EZ88\\1405.html}
}

@inproceedings{gasthaus_probabilistic_2019,
  title = {Probabilistic {{Forecasting}} with {{Spline Quantile Function RNNs}}},
  booktitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gasthaus, Jan and Benidis, Konstantinos and Wang, Yuyang and Rangapuram, Syama Sundar and Salinas, David and Flunkert, Valentin and Januschowski, Tim},
  year = {2019},
  month = apr,
  pages = {1901--1910},
  urldate = {2020-01-17},
  abstract = {In this paper, we propose a flexible method for probabilistic modeling with conditional quantile functions using monotonic regression splines. The shape of the spline is parameterized by a neural n...},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\VS28Y9NE\\Gasthaus et al. - 2019 - Probabilistic Forecasting with Spline Quantile Fun.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9NI8QVV9\\gasthaus19a.html}
}

@article{gastinger_study_2021,
  title = {A Study on {{Ensemble Learning}} for {{Time Series Forecasting}} and the Need for {{Meta-Learning}}},
  author = {Gastinger, Julia and Nicolas, S{\'e}bastien and Stepi{\'c}, Du{\v s}ica and Schmidt, Mischa and Sch{\"u}lke, Anett},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.11475 [cs]},
  eprint = {2104.11475},
  primaryclass = {cs},
  urldate = {2021-10-18},
  abstract = {The contribution of this work is twofold: (1) We introduce a collection of ensemble methods for time series forecasting to combine predictions from base models. We demonstrate insights on the power of ensemble learning for forecasting, showing experiment results on about 16000 openly available datasets, from M4, M5, M3 competitions, as well as FRED (Federal Reserve Economic Data) datasets. Whereas experiments show that ensembles provide a benefit on forecasting results, there is no clear winning ensemble strategy (plus hyperparameter configuration). Thus, in addition, (2), we propose a meta-learning step to choose, for each dataset, the most appropriate ensemble method and their hyperparameter configuration to run based on dataset meta-features.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\A28HDSUH\\Gastinger et al. - 2021 - A study on Ensemble Learning for Time Series Forec.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ESZ6X26K\\2104.html}
}

@article{gee_explaining_2019,
  title = {Explaining {{Deep Classification}} of {{Time-Series Data}} with {{Learned Prototypes}}},
  author = {Gee, Alan H. and {Garcia-Olano}, Diego and Ghosh, Joydeep and Paydarfar, David},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.08935 [cs, stat]},
  eprint = {1904.08935},
  primaryclass = {cs, stat},
  urldate = {2019-04-23},
  abstract = {The emergence of deep learning networks raises a need for algorithms to explain their decisions so that users and domain experts can be confident using algorithmic recommendations for high-risk decisions. In this paper we leverage the information-rich latent space induced by such models to learn data representations or prototypes within such networks to elucidate their internal decision-making process. We introduce a novel application of case-based reasoning using prototypes to understand the decisions leading to the classification of time-series data, specifically investigating electrocardiogram (ECG) waveforms for classification of bradycardia, a slowing of heart rate, in infants. We improve upon existing models by explicitly optimizing for increased prototype diversity which in turn improves model accuracy by learning regions of the latent space that highlight features for distinguishing classes. We evaluate the hyperparameter space of our model to show robustness in diversity prototype generation and additionally, explore the resultant latent space of a deep classification network on ECG waveforms via an interactive tool to visualize the learned prototypical waveforms therein. We show that the prototypes are capable of learning real-world features - in our case-study ECG morphology related to bradycardia - as well as features within sub-classes. Our novel work leverages learned prototypical framework on two dimensional time-series data to produce explainable insights during classification tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read - not in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\QIYKM7BD\\Gee et al. - 2019 - Explaining Deep Classification of Time-Series Data.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DGCFUVWZ\\1904.html}
}

@inproceedings{gendler_adversarially_2022,
  title = {Adversarially {{Robust Conformal Prediction}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Gendler, Asaf and Weng, Tsui-Wei and Daniel, Luca and Romano, Yaniv},
  year = {2022},
  month = jan,
  urldate = {2023-03-07},
  abstract = {Conformal prediction is a model-agnostic tool for constructing prediction sets that are valid under the common i.i.d. assumption, which has been applied to quantify the prediction uncertainty of deep net classifiers. In this paper, we generalize this framework to the case where adversaries exist during inference time, under which the i.i.d. assumption is grossly violated. By combining conformal prediction with randomized smoothing, our proposed method forms a prediction set with finite-sample coverage guarantee that holds for any data distribution with \$\textbackslash ell\_2\$-norm bounded adversarial noise, generated by any adversarial attack algorithm. The core idea is to bound the Lipschitz constant of the non-conformity score by smoothing it with Gaussian noise and leverage this knowledge to account for the effect of the unknown adversarial perturbation. We demonstrate the necessity of our method in the adversarial setting and the validity of our theoretical guarantee on three widely used benchmark data sets: CIFAR10, CIFAR100, and ImageNet.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\IGF2RK7X\Gendler et al. - 2022 - Adversarially Robust Conformal Prediction.pdf}
}

@article{genevay_sample_2018,
  title = {Sample {{Complexity}} of {{Sinkhorn}} Divergences},
  author = {Genevay, Aude and Chizat, L{\'e}naic and Bach, Francis and Cuturi, Marco and Peyr{\'e}, Gabriel},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.02733 [math, stat]},
  eprint = {1810.02733},
  primaryclass = {math, stat},
  urldate = {2019-10-03},
  abstract = {Optimal transport (OT) and maximum mean discrepancies (MMD) are now routinely used in machine learning to compare probability measures. We focus in this paper on \textbackslash emph\{Sinkhorn divergences\} (SDs), a regularized variant of OT distances which can interpolate, depending on the regularization strength \$\textbackslash varepsilon\$, between OT (\$\textbackslash varepsilon=0\$) and MMD (\$\textbackslash varepsilon=\textbackslash infty\$). Although the tradeoff induced by that regularization is now well understood computationally (OT, SDs and MMD require respectively \$O(n\^3\textbackslash log n)\$, \$O(n\^2)\$ and \$n\^2\$ operations given a sample size \$n\$), much less is known in terms of their \textbackslash emph\{sample complexity\}, namely the gap between these quantities, when evaluated using finite samples \textbackslash emph\{vs.\} their respective densities. Indeed, while the sample complexity of OT and MMD stand at two extremes, \$1/n\^\{1/d\}\$ for OT in dimension \$d\$ and \$1/\textbackslash sqrt\{n\}\$ for MMD, that for SDs has only been studied empirically. In this paper, we \textbackslash emph\{(i)\} derive a bound on the approximation error made with SDs when approximating OT as a function of the regularizer \$\textbackslash varepsilon\$, \textbackslash emph\{(ii)\} prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and \textbackslash emph\{(iii)\} provide the first sample complexity bound for SDs, obtained,by reformulating SDs as a maximization problem in a RKHS. We thus obtain a scaling in \$1/\textbackslash sqrt\{n\}\$ (as in MMD), with a constant that depends however on \$\textbackslash varepsilon\$, making the bridge between OT and MMD complete.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory},
  annotation = {00011},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\Y6XF8Z3Z\Genevay et al. - 2018 - Sample Complexity of Sinkhorn divergences.pdf}
}

@article{geurts_extremely_2006,
  title = {Extremely Randomized Trees},
  author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
  year = {2006},
  month = apr,
  journal = {Machine Learning},
  volume = {63},
  number = {1},
  pages = {3--42},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-006-6226-1},
  urldate = {2019-09-06},
  abstract = {This paper proposes a new tree-based ensemble method for supervised classification and regression problems. It essentially consists of randomizing strongly both attribute and cut-point choice while splitting a tree node. In the extreme case, it builds totally randomized trees whose structures are independent of the output values of the learning sample. The strength of the randomization can be tuned to problem specifics by the appropriate choice of a parameter. We evaluate the robustness of the default choice of this parameter, and we also provide insight on how to adjust it in particular situations. Besides accuracy, the main strength of the resulting algorithm is computational efficiency. A bias/variance analysis of the Extra-Trees algorithm is also provided as well as a geometrical and a kernel characterization of the models induced.},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YV9F52N5\Geurts et al. - 2006 - Extremely randomized trees.pdf}
}

@article{ghaderi_deep_2017,
  title = {Deep {{Forecast}}: {{Deep Learning-based Spatio-Temporal Forecasting}}},
  shorttitle = {Deep {{Forecast}}},
  author = {Ghaderi, Amir and Sanandaji, Borhan M. and Ghaderi, Faezeh},
  year = {2017},
  month = jul,
  journal = {arXiv:1707.08110 [cs]},
  eprint = {1707.08110},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {The paper presents a spatio-temporal wind speed forecasting algorithm using Deep Learning (DL)and in particular, Recurrent Neural Networks(RNNs). Motivated by recent advances in renewable energy integration and smart grids, we apply our proposed algorithm for wind speed forecasting. Renewable energy resources (wind and solar)are random in nature and, thus, their integration is facilitated with accurate short-term forecasts. In our proposed framework, we model the spatiotemporal information by a graph whose nodes are data generating entities and its edges basically model how these nodes are interacting with each other. One of the main contributions of our work is the fact that we obtain forecasts of all nodes of the graph at the same time based on one framework. Results of a case study on recorded time series data from a collection of wind mills in the north-east of the U.S. show that the proposed DL-based forecasting algorithm significantly improves the short-term forecasts compared to a set of widely-used benchmarks models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NY23IGCH\\Ghaderi et al. - 2017 - Deep Forecast Deep Learning-based Spatio-Temporal.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RL4N7WV3\\1707.html}
}

@article{ghadimi_reinforcement_2020,
  title = {Reinforcement {{Learning}} via {{Parametric Cost Function Approximation}} for {{Multistage Stochastic Programming}}},
  author = {Ghadimi, Saeed and Perkins, Raymond T. and Powell, Warren B.},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.00831 [math]},
  eprint = {2001.00831},
  primaryclass = {math},
  urldate = {2020-05-25},
  abstract = {The most common approaches for solving stochastic resource allocation problems in the research literature is to either use value functions ("dynamic programming") or scenario trees ("stochastic programming") to approximate the impact of a decision now on the future. By contrast, common industry practice is to use a deterministic approximation of the future which is easier to understand and solve, but which is criticized for ignoring uncertainty. We show that a parameterized version of a deterministic lookahead can be an effective way of handling uncertainty, while enjoying the computational simplicity of a deterministic lookahead. We present the parameterized lookahead model as a form of policy for solving a stochastic base model, which is used as the basis for optimizing the parameterized policy. This approach can handle complex, high-dimensional state variables, and avoids the usual approximations associated with scenario trees. We formalize this approach and demonstrate its use in the context of a complex, nonstationary energy storage problem.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Optimization and Control},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4TL6DE9R\\Ghadimi et al. - 2020 - Reinforcement Learning via Parametric Cost Functio.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8E79W6AS\\2001.html}
}

@inproceedings{gibbs_adaptive_2021,
  title = {Adaptive {{Conformal Inference Under Distribution Shift}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gibbs, Isaac and Candes, Emmanuel},
  year = {2021},
  volume = {34},
  pages = {1660--1672},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-07},
  abstract = {We develop methods for forming prediction sets in an online setting where the data generating distribution is allowed to vary over time in an unknown fashion. Our framework builds on ideas from conformal inference to provide a general wrapper that can be combined with any black box method that produces point predictions of the unseen label or estimated quantiles of its distribution. While previous conformal inference methods rely on the assumption that the data are exchangeable, our adaptive approach provably achieves the desired coverage frequency over long-time intervals irrespective of the true data generating process. We accomplish this by modelling the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated. We test our method, adaptive conformal inference, on two real world datasets and find that its predictions are robust to visible and significant distribution shifts.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FZFEEELY\Gibbs and Candes - 2021 - Adaptive Conformal Inference Under Distribution Sh.pdf}
}

@incollection{girard_gaussian_2003,
  title = {Gaussian {{Process Priors}} with {{Uncertain Inputs Application}} to {{Multiple-Step Ahead Time Series Forecasting}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 15},
  author = {Girard, Agathe and Rasmussen, Carl Edward and Candela, Joaquin Qui{\~n}onero and {Murray-Smith}, Roderick},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  year = {2003},
  pages = {545--552},
  publisher = {{MIT Press}},
  urldate = {2019-04-17},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DK8ZDEDP\\Girard et al. - 2003 - Gaussian Process Priors with Uncertain Inputs Appl.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\X2LKWZEL\\2313-gaussian-process-priors-with-uncertain-inputs-application-to-multiple-step-ahead-time-seri.html}
}

@misc{girolimetto_point_2023,
  title = {Point and Probabilistic Forecast Reconciliation for General Linearly Constrained Multiple Time Series},
  author = {Girolimetto, Daniele and Di Fonzo, Tommaso},
  year = {2023},
  month = may,
  number = {arXiv:2305.05330},
  eprint = {2305.05330},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.05330},
  urldate = {2023-08-01},
  abstract = {Forecast reconciliation is the post-forecasting process aimed to revise a set of incoherent base forecasts into coherent forecasts in line with given data structures. Most of the point and probabilistic regression-based forecast reconciliation results ground on the so called "structural representation" and on the related unconstrained generalized least squares reconciliation formula. However, the structural representation naturally applies to genuine hierarchical/grouped time series, where the top- and bottom-level variables are uniquely identified. When a general linearly constrained multiple time series is considered, the forecast reconciliation is naturally expressed according to a projection approach. While it is well known that the classic structural reconciliation formula is equivalent to its projection approach counterpart, so far it is not completely understood if and how a structural-like reconciliation formula may be derived for a general linearly constrained multiple time series. Such an expression would permit to extend reconciliation definitions, theorems and results in a straightforward manner. In this paper, we show that for general linearly constrained multiple time series it is possible to express the reconciliation formula according to a "structural-like" approach that keeps distinct free and constrained, instead of bottom and upper (aggregated), variables, establish the probabilistic forecast reconciliation framework, and apply these findings to obtain fully reconciled point and probabilistic forecasts for the aggregates of the Australian GDP from income and expenditure sides, and for the European Area GDP disaggregated by income, expenditure and output sides and by 19 countries.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HF2CIMK3\\Girolimetto and Di Fonzo - 2023 - Point and probabilistic forecast reconciliation fo.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CP5BSC5Q\\2305.html}
}

@article{gneiting_assessing_,
  title = {Assessing Probabilistic Forecasts of Multivariate Quantities, with an Application to Ensemble Predictions of Surface Winds},
  author = {Gneiting, Tilmann and Stanberry, Larissa I and Grimit, Eric P and Held, Leonhard and Johnson, Nicholas A},
  pages = {26},
  abstract = {We discuss methods for the evaluation of probabilistic predictions of vector-valued quantities, that can take the form of a discrete forecast ensemble or a density forecast. In particular, we propose a multivariate version of the univariate verification rank histogram or Talagrand diagram that can be used to check the calibration of ensemble forecasts. In the case of density forecasts, Box's density ordinate transform provides an attractive alternative. The multivariate energy score generalizes the continuous ranked probability score. It addresses both calibration and sharpness, and can be used to compare deterministic forecasts, ensemble forecasts and density forecasts, using a single loss function that is proper. An application to the University of Washington mesoscale ensemble points at strengths and deficiencies of probabilistic short-range forecasts of surface wind vectors over the North American Pacific Northwest.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\I6BKX5SF\Gneiting et al. - Assessing probabilistic forecasts of multivariate .pdf}
}

@article{gneiting_probabilistic_,
  title = {Probabilistic Forecasts, Calibration and Sharpness},
  author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E},
  pages = {26},
  abstract = {Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\DHELRLGM\Gneiting et al. - Probabilistic forecasts, calibration and sharpness.pdf}
}

@article{gobbi_statedependent_2020,
  title = {State-{{Dependent Autoregressive Models}}: {{Properties}}, {{Estimation}} and {{Forecasting}}},
  shorttitle = {State-{{Dependent Autoregressive Models}}},
  author = {Gobbi, Fabio and Mulinacci, Sabrina},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.03134 [math, stat]},
  eprint = {2002.03134},
  primaryclass = {math, stat},
  urldate = {2020-02-17},
  abstract = {This paper studies some temporal dependence properties and addresses the issue of parametric estimation for a class of state-dependent autoregressive models for nonlinear time series in which we assume a stochastic autoregressive coefficient depending on the first lagged value of the process itself. We call such a model state-dependent first-order autoregressive process, (SDAR). We introduce some assumptions under which this class of models is strictly stationary and uniformly ergodic and we establish consistency and asymptotic normality of the quasi-maximum likelihood estimator of the parameters. In order to capture the potentiality of the model, we present an empirical application to nonlinear time series provided by the weekly realized volatility extracted from returns of some European financial indices. The comparison of forecasting accuracy is made considering an alternative approach provided by a two-regime SETAR model},
  archiveprefix = {arxiv},
  keywords = {{60G10, 62M10, 91B84},Mathematics - Statistics Theory},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\44HTU47Y\\Gobbi and Mulinacci - 2020 - State-Dependent Autoregressive Models Properties,.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\36AINZSC\\2002.html}
}

@article{goel_r2n2_2017,
  title = {{{R2N2}}: {{Residual Recurrent Neural Networks}} for {{Multivariate Time Series Forecasting}}},
  shorttitle = {{{R2N2}}},
  author = {Goel, Hardik and Melnyk, Igor and Banerjee, Arindam},
  year = {2017},
  month = sep,
  journal = {arXiv:1709.03159 [cs, stat]},
  eprint = {1709.03159},
  primaryclass = {cs, stat},
  urldate = {2019-04-26},
  abstract = {Multivariate time-series modeling and forecasting is an important problem with numerous applications. Traditional approaches such as VAR (vector auto-regressive) models and more recent approaches such as RNNs (recurrent neural networks) are indispensable tools in modeling time-series data. In many multivariate time series modeling problems, there is usually a significant linear dependency component, for which VARs are suitable, and a nonlinear component, for which RNNs are suitable. Modeling such times series with only VAR or only RNNs can lead to poor predictive performance or complex models with large training times. In this work, we propose a hybrid model called R2N2 (Residual RNN), which first models the time series with a simple linear model (like VAR) and then models its residual errors using RNNs. R2N2s can be trained using existing algorithms for VARs and RNNs. Through an extensive empirical evaluation on two real world datasets (aviation and climate domains), we show that R2N2 is competitive, usually better than VAR or RNN, used alone. We also show that R2N2 is faster to train as compared to an RNN, while requiring less number of hidden units.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00007},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\8X8DS7PS\Goel et al. - 2017 - R2N2 Residual Recurrent Neural Networks for Multi.pdf}
}

@incollection{gomez_reversible_2017,
  title = {The {{Reversible Residual Network}}: {{Backpropagation Without Storing Activations}}},
  shorttitle = {The {{Reversible Residual Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Gomez, Aidan N and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {2214--2224},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-01-27},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\A7WE48D9\\Gomez et al. - 2017 - The Reversible Residual Network Backpropagation W.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LW76FYCR\\6816-the-reversible-residual-network-backpropagation-without-storing-activations.html}
}

@article{gonzalez_panel_,
  title = {Panel {{Smooth Transition Regression Models}}},
  author = {Gonzalez, Andres and Terasvirta, Timo and {van Dijk}, Dick and Yang, Yukai},
  pages = {49},
  abstract = {We introduce the panel smooth transition regression model. This new model is intended for characterizing heterogeneous panels, allowing the regression coefficients to vary both across individuals and over time. Specifically, heterogeneity is allowed for by assuming that these coefficients are bounded continuous functions of an observable variable and fluctuate between a limited number of ``extreme regimes''. The model can be viewed as a generalization of the threshold panel model of Hansen (1999). We extend the modelling strategy originally designed for univariate smooth transition regression models to the panel context. The strategy consists of model specification based on homogeneity tests, parameter estimation, and model evaluation, including tests of parameter constancy and no remaining heterogeneity. The model is applied to describing firms' investment decisions in the presence of capital market imperfections.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MZ6BCVMV\Gonzalez et al. - Panel Smooth Transition Regression Models.pdf}
}

@article{goodfellow_generative_,
  title = {Generative {{Adversarial Nets}}},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  pages = {9},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\27MATGQB\Goodfellow et al. - Generative Adversarial Nets.pdf}
}

@inproceedings{gouk_stochastic_2019,
  title = {Stochastic {{Gradient Trees}}},
  booktitle = {Asian {{Conference}} on {{Machine Learning}}},
  author = {Gouk, Henry and Pfahringer, Bernhard and Frank, Eibe},
  year = {2019},
  month = oct,
  pages = {1094--1109},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2020-11-18},
  abstract = {We present an algorithm for learning decision trees using stochastic gradient information as the source of supervision. In contrast to previous approaches to gradient-based tree learning, our metho...},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DTDBU8EH\\Gouk et al. - 2019 - Stochastic Gradient Trees.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LSET4D9D\\gouk19a.html}
}

@misc{gouttes_probabilistic_2021,
  title = {Probabilistic {{Time Series Forecasting}} with {{Implicit Quantile Networks}}},
  author = {Gouttes, Ad{\`e}le and Rasul, Kashif and Koren, Mateusz and Stephan, Johannes and Naghibi, Tofigh},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03743},
  eprint = {2107.03743},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-03-06},
  abstract = {Here, we propose a general method for probabilistic time series forecasting. We combine an autoregressive recurrent neural network to model temporal dynamics with Implicit Quantile Networks to learn a large class of distributions over a time-series target. When compared to other probabilistic neural forecasting models on real- and simulated data, our approach is favorable in terms of point-wise prediction accuracy as well as on estimating the underlying temporal distribution.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NKLHU7AA\\Gouttes et al. - 2021 - Probabilistic Time Series Forecasting with Implici.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TXPTUSPA\\2107.html}
}

@article{graves_generating_2013,
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2013},
  month = aug,
  journal = {arXiv:1308.0850 [cs]},
  eprint = {1308.0850},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\Z4W36XCF\Graves - 2013 - Generating Sequences With Recurrent Neural Network.pdf}
}

@article{graves_neural_2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = oct,
  journal = {arXiv:1410.5401 [cs]},
  eprint = {1410.5401},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\65TDFYZB\Graves et al. - 2014 - Neural Turing Machines.pdf}
}

@misc{grazzi_metaforecasting_2021,
  title = {Meta-{{Forecasting}} by Combining {{Global Deep Representations}} with {{Local Adaptation}}},
  author = {Grazzi, Riccardo and Flunkert, Valentin and Salinas, David and Januschowski, Tim and Seeger, Matthias and Archambeau, Cedric},
  year = {2021},
  month = nov,
  number = {arXiv:2111.03418},
  eprint = {2111.03418},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-31},
  abstract = {While classical time series forecasting considers individual time series in isolation, recent advances based on deep learning showed that jointly learning from a large pool of related time series can boost the forecasting accuracy. However, the accuracy of these methods suffers greatly when modeling out-of-sample time series, significantly limiting their applicability compared to classical forecasting methods. To bridge this gap, we adopt a meta-learning view of the time series forecasting problem. We introduce a novel forecasting method, called Meta Global-Local Auto-Regression (Meta-GLAR), that adapts to each time series by learning in closed-form the mapping from the representations produced by a recurrent neural network (RNN) to one-step-ahead forecasts. Crucially, the parameters ofthe RNN are learned across multiple time series by backpropagating through the closed-form adaptation mechanism. In our extensive empirical evaluation we show that our method is competitive with the state-of-the-art in out-of-sample forecasting accuracy reported in earlier work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LNVYACSV\\Grazzi et al. - 2021 - Meta-Forecasting by combining Global Deep Represen.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZHRQBF35\\2111.html}
}

@article{green_simple_2015,
  title = {Simple versus Complex Forecasting: {{The}} Evidence},
  shorttitle = {Simple versus Complex Forecasting},
  author = {Green, Kesten C. and Armstrong, J. Scott},
  year = {2015},
  month = aug,
  journal = {Journal of Business Research},
  series = {Special {{Issue}} on {{Simple Versus Complex Forecasting}}},
  volume = {68},
  number = {8},
  pages = {1678--1685},
  issn = {0148-2963},
  doi = {10.1016/j.jbusres.2015.03.026},
  urldate = {2020-03-13},
  abstract = {This article introduces this JBR Special Issue on simple versus complex methods in forecasting. Simplicity in forecasting requires that (1) method, (2) representation of cumulative knowledge, (3) relationships in models, and (4) relationships among models, forecasts, and decisions are all sufficiently uncomplicated as to be easily understood by decision-makers. Our review of studies comparing simple and complex methods \textendash{} including those in this special issue \textendash{} found 97 comparisons in 32 papers. None of the papers provide a balance of evidence that complexity improves forecast accuracy. Complexity increases forecast error by 27 percent on average in the 25 papers with quantitative comparisons. The finding is consistent with prior research to identify valid forecasting methods: all 22 previously identified evidence-based forecasting procedures are simple. Nevertheless, complexity remains popular among researchers, forecasters, and clients. Some evidence suggests that the popularity of complexity may be due to incentives: (1) researchers are rewarded for publishing in highly ranked journals, which favor complexity; (2) forecasters can use complex methods to provide forecasts that support decision-makers' plans; and (3) forecasters' clients may be reassured by incomprehensibility. Clients who prefer accuracy should accept forecasts only from simple evidence-based procedures. They can rate the simplicity of forecasters' procedures using the questionnaire at simple-forecasting.com.},
  langid = {english},
  keywords = {Analytics,Big data,Decision-making,Decomposition,Econometrics,Occam's razor},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\K74LEDJF\\Green and Armstrong - 2015 - Simple versus complex forecasting The evidence.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\49J5XME6\\S014829631500140X.html}
}

@article{greff_multiobject_2019,
  title = {Multi-{{Object Representation Learning}} with {{Iterative Variational Inference}}},
  author = {Greff, Klaus and Kaufmann, Rapha{\"e}l Lopez and Kabra, Rishab and Watters, Nick and Burgess, Chris and Zoran, Daniel and Matthey, Loic and Botvinick, Matthew and Lerchner, Alexander},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.00450 [cs, stat]},
  eprint = {1903.00450},
  primaryclass = {cs, stat},
  urldate = {2019-04-11},
  abstract = {Human perception is structured around objects which form the basis for our higher-level cognition and impressive systematic generalization abilities. Yet most work on representation learning focuses on feature learning without even considering multiple objects, or treats segmentation as an (often supervised) preprocessing step. Instead, we argue for the importance of learning to segment and represent objects jointly. We demonstrate that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations. Our method learns \textendash{} without supervision \textendash{} to inpaint occluded parts, and extrapolates to scenes with more objects and to unseen objects with novel feature combinations. We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal posteriors for ambiguous inputs and extends naturally to sequences.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6ZGQH22G\Greff et al. - 2019 - Multi-Object Representation Learning with Iterativ.pdf}
}

@article{gregor_draw_,
  title = {{{DRAW}}: {{A Recurrent Neural Network For Image Generation}}},
  author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  pages = {10},
  abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FEMK4YL4\Gregor et al. - DRAW A Recurrent Neural Network For Image Generat.pdf}
}

@inproceedings{grover_deep_2015,
  title = {A {{Deep Hybrid Model}} for {{Weather Forecasting}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '15},
  author = {Grover, Aditya and Kapoor, Ashish and Horvitz, Eric},
  year = {2015},
  pages = {379--386},
  publisher = {{ACM Press}},
  address = {{Sydney, NSW, Australia}},
  doi = {10.1145/2783258.2783275},
  urldate = {2019-04-17},
  abstract = {Weather forecasting is a canonical predictive challenge that has depended primarily on model-based methods. We explore new directions with forecasting weather as a dataintensive challenge that involves inferences across space and time. We study specifically the power of making predictions via a hybrid approach that combines discriminatively trained predictive models with a deep neural network that models the joint statistics of a set of weather-related variables. We show how the base model can be enhanced with spatial interpolation that uses learned long-range spatial dependencies. We also derive an efficient learning and inference procedure that allows for large scale optimization of the model parameters. We evaluate the methods with experiments on real-world meteorological data that highlight the promise of the approach.},
  isbn = {978-1-4503-3664-2},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00071},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\J7KETTEN\Grover et al. - 2015 - A Deep Hybrid Model for Weather Forecasting.pdf}
}

@article{gu_continuous_2016,
  title = {Continuous {{Deep Q-Learning}} with {{Model-based Acceleration}}},
  author = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
  year = {2016},
  month = mar,
  journal = {arXiv:1603.00748 [cs]},
  eprint = {1603.00748},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of modelfree algorithms, particularly when using highdimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LLYTIB5J\Gu et al. - 2016 - Continuous Deep Q-Learning with Model-based Accele.pdf}
}

@article{gu_generalized_2019,
  title = {Generalized Probabilistic Principal Component Analysis of Correlated Data},
  author = {Gu, Mengyang and Shen, Weining},
  year = {2019},
  month = oct,
  journal = {arXiv:1808.10868 [stat]},
  eprint = {1808.10868},
  primaryclass = {stat},
  urldate = {2020-02-17},
  abstract = {Principal component analysis (PCA) is a well-established tool in machine learning and data processing. The principal axes in PCA were shown to be equivalent to the maximum marginal likelihood estimator of the factor loading matrix in a latent factor model for the observed data, assuming that the latent factors are independently distributed as standard normal distributions. However, the independence assumption may be unrealistic for many scenarios such as modeling multiple time series, spatial processes, and functional data, where the outcomes are correlated. In this paper, we introduce the generalized probabilistic principal component analysis (GPPCA) to study the latent factor model for multiple correlated outcomes, where each factor is modeled by a Gaussian process. Our method generalizes the previous probabilistic formulation of PCA (PPCA) by providing the closedform maximum marginal likelihood estimator of the factor loadings and other parameters. Based on the explicit expression of the precision matrix in the marginal likelihood that we derived, the number of the computational operations is linear to the number of output variables. Furthermore, we also provide the closed-form expression of the marginal likelihood when other covariates are included in the mean structure. We highlight the advantage of GPPCA in terms of the practical relevance, estimation accuracy and computational convenience. Numerical studies of simulated and real data confirm the excellent finite-sample performance of the proposed approach.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2E396AGB\Gu and Shen - 2019 - Generalized probabilistic principal component anal.pdf}
}

@article{guen_shape_2019,
  title = {Shape and {{Time Distortion Loss}} for {{Training Deep Time Series Forecasting Models}}},
  author = {Guen, Vincent Le and Thome, Nicolas},
  year = {2019},
  month = oct,
  journal = {arXiv:1909.09020 [cs, stat]},
  eprint = {1909.09020},
  primaryclass = {cs, stat},
  urldate = {2019-10-28},
  abstract = {This paper addresses the problem of time series forecasting for non-stationary signals and multiple future steps prediction. To handle this challenging task, we introduce DILATE (DIstortion Loss including shApe and TimE), a new objective function for training deep neural networks. DILATE aims at accurately predicting sudden changes, and explicitly incorporates two terms supporting precise shape and temporal change detection. We introduce a differentiable loss function suitable for training deep neural nets, and provide a custom back-prop implementation for speeding up optimization. We also introduce a variant of DILATE, which provides a smooth generalization of temporally-constrained Dynamic Time Warping (DTW). Experiments carried out on various non-stationary datasets reveal the very good behaviour of DILATE compared to models trained with the standard Mean Squared Error (MSE) loss function, and also to DTW and variants. DILATE is also agnostic to the choice of the model, and we highlight its benefit for training fully connected networks as well as specialized recurrent architectures, showing its capacity to improve over state-of-the-art trajectory forecasting approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BVPX3J34\\Guen and Thome - 2019 - Shape and Time Distortion Loss for Training Deep T.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\QQ75WFA9\\1909.html}
}

@article{gunduz_transfer_2023,
  title = {Transfer Learning for Electricity Price Forecasting},
  author = {Gunduz, Salih and Ugurlu, Umut and Oksuz, Ilkay},
  year = {2023},
  month = jun,
  journal = {Sustainable Energy, Grids and Networks},
  volume = {34},
  pages = {100996},
  issn = {2352-4677},
  doi = {10.1016/j.segan.2023.100996},
  urldate = {2023-05-05},
  abstract = {Electricity price forecasting is an essential task in all the deregulated markets of the world. The accurate prediction of day-ahead electricity prices is an active research field and available data from various markets can be used as input for forecasting. A collection of models have been proposed for this task, but the fundamental question on how to use the available big data is often neglected. In this paper, we propose to use transfer learning as a tool for utilizing information from other electricity price markets for forecasting. We pre-train a neural network model on source markets and finally do a fine-tuning for the target market. Moreover, we test different ways to use the rich input data from various electricity price markets to forecast 24 steps ahead in hourly frequency. Our experiments on four different day-ahead markets indicate that transfer learning improves the electricity price forecasting performance in a statistically significant manner. Furthermore, we compare our results with state-of-the-art methods in a rolling window scheme to demonstrate the performance of the transfer learning approach. Our method improves the performance of the state-of-the-art algorithms by 7\% for the French market and 3\% for the German market.},
  langid = {english},
  keywords = {Artificial neural networks,Electricity price forecasting,Market integration,Transfer learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4CX9WCET\\Gunduz et al. - 2023 - Transfer learning for electricity price forecastin.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IJ38TLM9\\S2352467723000048.html}
}

@article{guo_bitcoin_2018,
  title = {Bitcoin {{Volatility Forecasting}} with a {{Glimpse}} into {{Buy}} and {{Sell Orders}}},
  author = {Guo, Tian and Bifet, Albert and {Antulov-Fantulin}, Nino},
  year = {2018},
  month = nov,
  journal = {2018 IEEE International Conference on Data Mining (ICDM)},
  eprint = {1802.04065},
  pages = {989--994},
  doi = {10.1109/ICDM.2018.00123},
  urldate = {2019-04-03},
  abstract = {Bitcoin is one of the most prominent decentralized digital cryptocurrencies, currently having the largest market capitalization among cryptocurrencies. Ability to understand which factors drive the fluctuations of the Bitcoin price and to what extent they are predictable is interesting both from theoretical and practical perspective. In this paper, we study the problem of the Bitcoin short-term volatility forecasting by exploiting volatility history and order book data. Order book, consisting of buy and sell orders over time, reflects the intention of the market and is closely related to the evolution of volatility. We propose temporal mixture models capable of adaptively exploiting both volatility history and order book features for short-term volatility forecasting. By leveraging rolling and incremental learning and evaluation procedures, we demonstrate the prediction performance of our model as well as studying the robustness, in comparison to a variety of statistical and machine learning baselines. Meanwhile, our temporal mixture model enables to decipher time-varying effect of order book features on the volatility. It demonstrates the prospect of our temporal mixture model as an interpretable forecasting framework over heterogeneous Bitcoin data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read - not in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\X59T337M\Guo et al. - 2018 - Bitcoin Volatility Forecasting with a Glimpse into.pdf}
}

@article{guo_deep_2018,
  title = {Deep {{Poisson}} Gamma Dynamical Systems},
  author = {Guo, Dandan and Chen, Bo and Zhang, Hao and Zhou, Mingyuan},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.11209 [cs, stat]},
  eprint = {1810.11209},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {We develop deep Poisson-gamma dynamical systems (DPGDS) to model sequentially observed multivariate count data, improving previously proposed models by not only mining deep hierarchical latent structure from the data, but also capturing both first-order and long-range temporal dependencies. Using sophisticated but simple-to-implement data augmentation techniques, we derived closed-form Gibbs sampling update equations by first backward and upward propagating auxiliary latent counts, and then forward and downward sampling latent variables. Moreover, we develop stochastic gradient MCMC inference that is scalable to very long multivariate count time series. Experiments on both synthetic and a variety of real-world data demonstrate that the proposed model not only has excellent predictive performance, but also provides highly interpretable multilayer latent structure to represent hierarchical and temporal information propagation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - in related work,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CP52KWT6\\Guo et al. - 2018 - Deep Poisson gamma dynamical systems.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5HY4CQFF\\1810.html}
}

@article{guo_exploring_2018,
  title = {Exploring the Interpretability of {{LSTM}} Neural Networks over Multi-Variable Data},
  author = {Guo, Tian and Lin, Tao},
  year = {2018},
  month = sep,
  urldate = {2019-04-17},
  abstract = {In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for...},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5R7X5WZE\\Guo and Lin - 2018 - Exploring the interpretability of LSTM neural netw.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\N7KU5LLS\\forum.html}
}

@article{guo_interpretable_2018,
  title = {An Interpretable {{LSTM}} Neural Network for Autoregressive Exogenous Model},
  author = {Guo, Tian and Lin, Tao and Lu, Yao},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.05251 [cs, stat]},
  eprint = {1804.05251},
  primaryclass = {cs, stat},
  urldate = {2019-04-03},
  abstract = {In this paper, we propose an interpretable LSTM recurrent neural network, i.e., multi-variable LSTM for time series with exogenous variables. Currently, widely used attention mechanism in recurrent neural networks mostly focuses on the temporal aspect of data and falls short of characterizing variable importance. To this end, our multi-variable LSTM equipped with tensorized hidden states is developed to learn variable specific representations, which give rise to both temporal and variable level attention. Preliminary experiments demonstrate comparable prediction performance of multi-variable LSTM w.r.t. encoder-decoder based baselines. More interestingly, variable importance in real datasets characterized by the variable attention is highly in line with that determined by statistical Granger causality test, which exhibits the prospect of multi-variable LSTM as a simple and uniform end-to-end framework for both forecasting and knowledge discovery.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LDD5HM8Q\Guo et al. - 2018 - An interpretable LSTM neural network for autoregre.pdf}
}

@article{guo_multivariable_2018,
  title = {Multi-Variable {{LSTM}} Neural Network for Autoregressive Exogenous Model},
  author = {Guo, Tian and Lin, Tao},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.06384 [cs, stat]},
  eprint = {1806.06384},
  primaryclass = {cs, stat},
  urldate = {2019-04-03},
  abstract = {In this paper, we propose multi-variable LSTM capable of accurate forecasting and variable importance interpretation for time series with exogenous variables. Current attention mechanism in recurrent neural networks mostly focuses on the temporal aspect of data and falls short of characterizing variable importance. To this end, the multi-variable LSTM equipped with tensorized hidden states is developed to learn hidden states for individual variables, which give rise to our mixture temporal and variable attention. Based on such attention mechanism, we infer and quantify variable importance. Extensive experiments using real datasets with Granger-causality test and the synthetic dataset with ground truth demonstrate the prediction performance and interpretability of multi-variable LSTM in comparison to a variety of baselines. It exhibits the prospect of multi-variable LSTM as an end-to-end framework for both forecasting and knowledge discovery.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6LQZK7RH\Guo and Lin - 2018 - Multi-variable LSTM neural network for autoregress.pdf}
}

@inproceedings{guo_robust_2016,
  title = {Robust {{Online Time Series Prediction}} with {{Recurrent Neural Networks}}},
  booktitle = {2016 {{IEEE International Conference}} on {{Data Science}} and {{Advanced Analytics}} ({{DSAA}})},
  author = {Guo, T. and Xu, Z. and Yao, X. and Chen, H. and Aberer, K. and Funaya, K.},
  year = {2016},
  month = oct,
  pages = {816--825},
  doi = {10.1109/DSAA.2016.92},
  abstract = {Time series forecasting for streaming data plays an important role in many real applications, ranging from IoT systems, cyber-networks, to industrial systems and healthcare. However the real data is often complicated with anomalies and change points, which can lead the learned models deviating from the underlying patterns of the time series, especially in the context of online learning mode. In this paper we present an adaptive gradient learning method for recurrent neural networks (RNN) to forecast streaming time series in the presence of anomalies and change points. We explore the local features of time series to automatically weight the gradients of the loss of the newly available observations with distributional properties of the data in real time. We perform extensive experimental analysis on both synthetic and real datasets to evaluate the performance of the proposed method.},
  keywords = {adaptive gradient learning method,data analysis,Data models,Forecasting,learning (artificial intelligence),Learning systems,online learning mode,Predictive models,read - in related work,recurrent neural nets,recurrent neural networks,Recurrent neural networks,RNN,robust online time series prediction,Robustness,streaming data,streaming time series forecasting,time series,Time series analysis},
  annotation = {00026},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FR7YLKM5\7796970.html}
}

@article{gupta_compression_2022,
  title = {Compression of {{Deep Learning Models}} for {{Text}}: {{A Survey}}},
  shorttitle = {Compression of {{Deep Learning Models}} for {{Text}}},
  author = {Gupta, Manish and Agrawal, Puneet},
  year = {2022},
  month = jan,
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {16},
  number = {4},
  pages = {61:1--61:55},
  issn = {1556-4681},
  doi = {10.1145/3487045},
  urldate = {2023-05-05},
  abstract = {In recent years, the fields of natural language processing (NLP) and information retrieval (IR) have made tremendous progress thanks to deep learning models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTMs) networks, and Transformer [121] based models like Bidirectional Encoder Representations from Transformers (BERT) [24], Generative Pre-training Transformer (GPT-2) [95], Multi-task Deep Neural Network (MT-DNN) [74], Extra-Long Network (XLNet) [135], Text-to-text transfer transformer (T5) [96], T-NLG [99], and GShard [64]. But these models are humongous in size. On the other hand, real-world applications demand small model size, low response times, and low computational power wattage. In this survey, we discuss six different types of methods (Pruning, Quantization, Knowledge Distillation (KD), Parameter Sharing, Tensor Decomposition, and Sub-quadratic Transformer-based methods) for compression of such models to enable their deployment in real industry NLP projects. Given the critical need of building applications with efficient and small models, and the large amount of recently published work in this area, we believe that this survey organizes the plethora of work done by the ``deep learning for NLP'' community in the past few years and presents it as a coherent story.},
  keywords = {deep learning,knowledge distillation,Model compression,parameter sharing,pruning,quantization,sub-quadratic transformers,tensor factorization},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\8UR6RQA9\Gupta and Agrawal - 2022 - Compression of Deep Learning Models for Text A Su.pdf}
}

@article{gupta_novel_2019,
  title = {A Novel High-Order Fuzzy Time Series Forecasting Method Based on Probabilistic Fuzzy Sets},
  author = {Gupta, Krishna Kumar and Kumar, Sanjay},
  year = {2019},
  month = oct,
  journal = {Granular Computing},
  volume = {4},
  number = {4},
  pages = {699--713},
  issn = {2364-4974},
  doi = {10.1007/s41066-019-00168-4},
  urldate = {2021-02-04},
  abstract = {Recently, the probabilistic fuzzy set has been applied by the researchers in various domains to model the uncertainties in the system due to both fuzziness and randomness. In this research paper, we propose a novel high-order probabilistic fuzzy set-based forecasting method in the environment of both non-probabilistic and probabilistic uncertainties. We have also proposed a novel probability-based discretization approach to model probabilistic uncertainty during partitioning of time series data. Gaussian probability distribution function is used in this research paper to associate probabilities to membership grades and probabilistic fuzzy elements are aggregated to a fuzzy row vector using an aggregation operator. Major advantages of the proposed method are that it includes both types of uncertainties in a single framework and enhances accuracy in forecast as well. To show its suitability and outperformance over other existing forecasting methods, the proposed method is implemented in University of Alabama enrolments and TAIFEX time series datasets. Various statistical parameters, e.g., coefficient of correlation, coefficient of determination, performance parameter, evaluation parameter and tracking signal are used to verify the validity of proposed PFS-based high-order time series forecasting method.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\D4X55RE4\Gupta and Kumar - 2019 - A novel high-order fuzzy time series forecasting m.pdf}
}

@misc{gustafsson_how_2023,
  title = {How {{Reliable}} Is {{Your Regression Model}}'s {{Uncertainty Under Real-World Distribution Shifts}}?},
  author = {Gustafsson, Fredrik K. and Danelljan, Martin and Sch{\"o}n, Thomas B.},
  year = {2023},
  month = feb,
  number = {arXiv:2302.03679},
  eprint = {2302.03679},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.03679},
  urldate = {2023-03-07},
  abstract = {Many important computer vision applications are naturally formulated as regression problems. Within medical imaging, accurate regression models have the potential to automate various tasks, helping to lower costs and improve patient outcomes. Such safety-critical deployment does however require reliable estimation of model uncertainty, also under the wide variety of distribution shifts that might be encountered in practice. Motivated by this, we set out to investigate the reliability of regression uncertainty estimation methods under various real-world distribution shifts. To that end, we propose an extensive benchmark of 8 image-based regression datasets with different types of challenging distribution shifts. We then employ our benchmark to evaluate many of the most common uncertainty estimation methods, as well as two state-of-the-art uncertainty scores from the task of out-of-distribution detection. We find that while methods are well calibrated when there is no distribution shift, they all become highly overconfident on many of the benchmark datasets. This uncovers important limitations of current uncertainty estimation methods, and the proposed benchmark therefore serves as a challenge to the research community. We hope that our benchmark will spur more work on how to develop truly reliable regression uncertainty estimation methods. Code is available at https://github.com/fregu856/regression\_uncertainty.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4SC5PPDR\\Gustafsson et al. - 2023 - How Reliable is Your Regression Model's Uncertaint.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LXCNR334\\2302.html}
}

@misc{hadjicosti_dataset_2018,
  title = {Dataset},
  author = {Hadjicosti, Nayia},
  year = {2018},
  month = sep,
  journal = {M-Competitions},
  urldate = {2019-05-24},
  abstract = {Visit the post for more.},
  howpublished = {https://www.mcompetitions.unic.ac.cy/the-dataset/},
  langid = {american},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\GRZUFCSD\the-dataset.html}
}

@inproceedings{hallac_toeplitz_2017,
  title = {Toeplitz {{Inverse Covariance-Based Clustering}} of {{Multivariate Time Series Data}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Hallac, David and Vare, Sagar and Boyd, Stephen and Leskovec, Jure},
  year = {2017},
  series = {{{KDD}} '17},
  pages = {215--223},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3097983.3098060},
  urldate = {2019-04-16},
  abstract = {Subsequence clustering of multivariate time series is a useful tool for discovering repeated patterns in temporal data. Once these patterns have been discovered, seemingly complicated datasets can be interpreted as a temporal sequence of only a small number of states, or clusters. For example, raw sensor data from a fitness-tracking application can be expressed as a timeline of a select few actions (i.e., walking, sitting, running). However, discovering these patterns is challenging because it requires simultaneous segmentation and clustering of the time series. Furthermore, interpreting the resulting clusters is difficult, especially when the data is high-dimensional. Here we propose a new method of model-based clustering, which we call Toeplitz Inverse Covariance-based Clustering (TICC). Each cluster in the TICC method is defined by a correlation network, or Markov random field (MRF), characterizing the interdependencies between different observations in a typical subsequence of that cluster. Based on this graphical representation, TICC simultaneously segments and clusters the time series data. We solve the TICC problem through alternating minimization, using a variation of the expectation maximization (EM) algorithm. We derive closed-form solutions to efficiently solve the two resulting subproblems in a scalable way, through dynamic programming and the alternating direction method of multipliers (ADMM), respectively. We validate our approach by comparing TICC to several state-of-the-art baselines in a series of synthetic experiments, and we then demonstrate on an automobile sensor dataset how TICC can be used to learn interpretable clusters in real-world scenarios.},
  isbn = {978-1-4503-4887-4},
  keywords = {alternating minimization,clustering,convex optimization,networks,read - in related work,time series analysis},
  annotation = {00023},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\UIDBV5U4\Hallac et al. - 2017 - Toeplitz Inverse Covariance-Based Clustering of Mu.pdf}
}

@article{han_deep_2015,
  title = {Deep {{Compression}}: {{Compressing Deep Neural Networks}} with {{Pruning}}, {{Trained Quantization}} and {{Huffman Coding}}},
  shorttitle = {Deep {{Compression}}},
  author = {Han, Song and Mao, Huizi and Dally, William J.},
  year = {2015},
  month = oct,
  journal = {arXiv:1510.00149 [cs]},
  eprint = {1510.00149},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce ``deep compression'', a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35\texttimes{} to 49\texttimes{} without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9\texttimes{} to 13\texttimes; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35\texttimes, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49\texttimes{} from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3\texttimes{} to 4\texttimes{} layerwise speedup and 3\texttimes{} to 7\texttimes{} better energy efficiency.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\4ZDUY3NV\Han et al. - 2015 - Deep Compression Compressing Deep Neural Networks.pdf}
}

@article{han_mecats_2021,
  title = {{{MECATS}}: {{Mixture-of-Experts}} for {{Probabilistic Forecasts}} of {{Aggregated Time Series}}},
  shorttitle = {{{MECATS}}},
  author = {Han, Xing and Hu, Jing and Ghosh, Joydeep},
  year = {2021},
  month = sep,
  urldate = {2022-04-08},
  abstract = {We introduce a mixture of heterogeneous experts framework called MECATS, which simultaneously forecasts the values of a set of time series that are related through an aggregation hierarchy....},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TITYJGDX\Han et al. - 2021 - MECATS Mixture-of-Experts for Probabilistic Forec.pdf}
}

@inproceedings{han_simultaneously_2021,
  title = {Simultaneously {{Reconciled Quantile Forecasting}} of {{Hierarchically Related Time Series}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Han, Xing and Dasgupta, Sambarta and Ghosh, Joydeep},
  year = {2021},
  month = mar,
  pages = {190--198},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-10-18},
  abstract = {Many real-life applications involve simultaneously forecasting multiple time series that are hierarchically related via aggregation or disaggregation operations. For instance, commercial organizations often want to forecast inventories simultaneously at store, city, and state levels for resource planning purposes. In such applications, it is important that the forecasts, in addition to being reasonably accurate, are also consistent w.r.t one another. Although forecasting such hierarchical time series has been pursued by economists and data scientists, the current state-of-the-art models use strong assumptions, e.g., all forecasts being unbiased estimates, noise distribution being Gaussian. Besides, state-of-the-art models have not harnessed the power of modern nonlinear models, especially ones based on deep learning. In this paper, we propose using a flexible nonlinear model that optimizes quantile regression loss coupled with suitable regularization terms to maintain the consistency of forecasts across hierarchies. The theoretical framework introduced herein can be applied to any forecasting model with an underlying differentiable loss function. A proof of optimality of our proposed method is also provided. Simulation studies over a range of datasets highlight the efficacy of our approach.},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IJP8H728\\Han et al. - 2021 - Simultaneously Reconciled Quantile Forecasting of .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\SUKYLRNQ\\Han et al. - 2021 - Simultaneously Reconciled Quantile Forecasting of .pdf}
}

@inproceedings{haque_exploiting_2017,
  title = {Exploiting {{Heterogeneity}} for {{Tail Latency}} and {{Energy Efficiency}}},
  booktitle = {2017 50th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Haque, Md E. and He, Yuxiong and Elnikety, Sameh and Nguyen, Thu D. and Bianchini, Ricardo and McKinley, Kathryn S.},
  year = {2017},
  month = oct,
  pages = {625--638},
  issn = {2379-3155},
  abstract = {Interactive service providers have strict requirements on high-percentile (tail) latency to meet user expectations. If providers meet tail latency targets with less energy, they increase profits, because energy is a significant operating expense. Unfortunately, optimizing tail latency and energy are typically conflicting goals. Our work resolves this conflict by exploiting servers with per-core Dynamic Voltage and Frequency Scaling (DVFS) and Asymmetric Multicore Processors (AMPs). We introduce the Adaptive Slow-to- Fast scheduling framework, which matches the heterogeneity of the workload-a mix of short and long requests-to the heterogeneity of the hardware- cores running at different speeds. The scheduler prioritizes long requests to faster cores by exploiting the insight that long requests reveal themselves. We use control theory to design threshold-based scheduling policies that use individual request progress, load, competition, and latency targets to optimize performance and energy. We configure our framework to optimize Energy Efficiency for a given Tail Latency (EETL) for both DVFS and AMP. In this framework, each request self-schedules, starting on a slow core and then migrating itself to faster cores. At high load, when a desired AMP core speed s is not available for a request but a faster core is, the longest request on an s core type migrates early to make room for the other request. Compared to per-core DVFS systems, EETL for AMPs delivers the same tail latency, reduces energy by 18\% to 50\%, and improves capacity (throughput) by 32\% to 82\%. We demonstrate that our framework effectively exploits dynamic DVFS and static AMP heterogeneity to reduce provisioning and operational costs for interactive services.},
  keywords = {adaptive slow-to-fast scheduling,Asymmetric Multicore Processors,dynamic DVFS,dynamic voltage and frequency scaling,EETL,Energy consumption,energy efficiency,Energy efficiency,Energy Efficiency,Hardware,Heterogeneous Processors,interactive services,interactive systems,microprocessor chips,Multicore processing,multiprocessing systems,optimisation,power aware computing,Program processors,Schedules,scheduling,Servers,static AMP heterogeneity,tail latency,Tail Latency,threshold-based scheduling policies},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BFEMW24Z\8686710.html}
}

@article{hariharan_lowshot_2016,
  title = {Low-Shot {{Visual Recognition}} by {{Shrinking}} and {{Hallucinating Features}}},
  author = {Hariharan, Bharath and Girshick, Ross},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.02819 [cs]},
  eprint = {1606.02819},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Low-shot visual learning\textemdash the ability to recognize novel object categories from very few examples\textemdash is a hallmark of human visual intelligence. Existing machine learning approaches fail to generalize in the same way. To make progress on this foundational problem, we present a lowshot learning benchmark on complex images that mimics challenges faced by recognition systems in the wild. We then propose (1) representation regularization techniques, and (2) techniques to hallucinate additional training examples for data-starved classes. Together, our methods improve the effectiveness of convolutional networks in low-shot learning, improving the one-shot accuracy on novel classes by 2.3\texttimes{} on the challenging ImageNet dataset.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\F6QDJL2I\Hariharan and Girshick - 2016 - Low-shot Visual Recognition by Shrinking and Hallu.pdf}
}

@article{hassler_forecasting_2019,
  title = {Forecasting under {{Long Memory}} and {{Nonstationarity}}},
  author = {Hassler, Uwe and Pohle, Marc-Oliver},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.08202 [econ]},
  eprint = {1910.08202},
  primaryclass = {econ},
  urldate = {2020-02-19},
  abstract = {Long memory in the sense of slowly decaying autocorrelations is a stylized fact in many time series from economics and finance. The fractionally integrated process is the workhorse model for the analysis of these time series. Nevertheless, there is mixed evidence in the literature concerning its usefulness for forecasting and how forecasting based on it should be implemented. Employing pseudo-out-of-sample forecasting on inflation and realized volatility time series and simulations we show that methods based on fractional integration clearly are superior to alternative methods not accounting for long memory, including autoregressions and exponential smoothing. Our proposal of choosing a fixed fractional integration parameter of \$d=0.5\$ a priori yields the best results overall, capturing long memory behavior, but overcoming the deficiencies of methods using an estimated parameter. Regarding the implementation of forecasting methods based on fractional integration, we use simulations to compare local and global semiparametric and parametric estimators of the long memory parameter from the Whittle family and provide asymptotic theory backed up by simulations to compare different mean estimators. Both of these analyses lead to new results, which are also of interest outside the realm of forecasting.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PV6QN658\\Hassler and Pohle - 2019 - Forecasting under Long Memory and Nonstationarity.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\JAI4ELBQ\\1910.html}
}

@inproceedings{hasson_probabilistic_2021,
  title = {Probabilistic {{Forecasting}}: {{A Level-Set Approach}}},
  shorttitle = {Probabilistic {{Forecasting}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hasson, Hilaf and Wang, Bernie and Januschowski, Tim and Gasthaus, Jan},
  year = {2021},
  volume = {34},
  pages = {6404--6416},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-07},
  abstract = {Large-scale time series panels have become ubiquitous over the last years in areas such as retail, operational metrics, IoT, and medical domain (to name only a few). This has resulted in a need for forecasting techniques that effectively leverage all available data by learning across all time series in each panel. Among the desirable properties of forecasting techniques, being able to generate probabilistic predictions ranks among the top. In this paper, we therefore present Level Set Forecaster (LSF), a simple yet effective general approach to transform a point estimator into a probabilistic one. By recognizing the connection of our algorithm to random forests (RFs) and quantile regression forests (QRFs), we are able to prove consistency guarantees of our approach under mild assumptions on the underlying point estimator. As a byproduct, we prove the first consistency results for QRFs under the CART-splitting criterion. Empirical experiments show that our approach, equipped with tree-based models as the point estimator, rivals state-of-the-art deep learning models in terms of forecasting accuracy.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TAVNLV99\Hasson et al. - 2021 - Probabilistic Forecasting A Level-Set Approach.pdf}
}

@article{hatalis_novel_2019,
  title = {A {{Novel Smoothed Loss}} and {{Penalty Function}} for {{Noncrossing Composite Quantile Estimation}} via {{Deep Neural Networks}}},
  author = {Hatalis, Kostas and Lamadrid, Alberto J. and Scheinberg, Katya and Kishore, Shalinee},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.12122 [cs, eess]},
  eprint = {1909.12122},
  primaryclass = {cs, eess},
  urldate = {2019-11-29},
  abstract = {Uncertainty analysis in the form of probabilistic forecasting can significantly improve decision making processes in the smart power grid when integrating renewable energy sources such as wind. Whereas point forecasting provides a single expected value, probabilistic forecasts provide more information in the form of quantiles, prediction intervals, or full predictive densities. Traditionally quantile regression is applied for such forecasting and recently quantile regression neural networks have become popular for weather and renewable energy forecasting. However, one major shortcoming of composite quantile estimation in neural networks is the quantile crossover problem. This paper analyzes the effectiveness of a novel smoothed loss and penalty function for neural network architectures to prevent the quantile crossover problem. It's efficacy is examined on the wind power forecasting problem. A numerical case study is conducted using publicly available wind data from the Global Energy Forecasting Competition 2014. Multiple quantiles are estimated to form 10\%, to 90\% prediction intervals which are evaluated using a quantile score and reliability measures. Benchmark models such as the persistence and climatology distributions, multiple quantile regression, and support vector quantile regression are used for comparison where results demonstrate the proposed approach leads to improved performance while preventing the problem of overlapping quantile estimates.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QTGC3B5K\Hatalis et al. - 2019 - A Novel Smoothed Loss and Penalty Function for Non.pdf}
}

@article{hatalis_smooth_2017,
  title = {Smooth {{Pinball Neural Network}} for {{Probabilistic Forecasting}} of {{Wind Power}}},
  author = {Hatalis, Kostas and Lamadrid, Alberto J. and Scheinberg, Katya and Kishore, Shalinee},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.01720 [stat]},
  eprint = {1710.01720},
  primaryclass = {stat},
  urldate = {2020-06-15},
  abstract = {Uncertainty analysis in the form of probabilistic forecasting can significantly improve decision making processes in the smart power grid for better integrating renewable energy sources such as wind. Whereas point forecasting provides a single expected value, probabilistic forecasts provide more information in the form of quantiles, prediction intervals, or full predictive densities. This paper analyzes the effectiveness of a novel approach for nonparametric probabilistic forecasting of wind power that combines a smooth approximation of the pinball loss function with a neural network architecture and a weighting initialization scheme to prevent the quantile cross over problem. A numerical case study is conducted using publicly available wind data from the Global Energy Forecasting Competition 2014. Multiple quantiles are estimated to form 10\%, to 90\% prediction intervals which are evaluated using a quantile score and reliability measures. Benchmark models such as the persistence and climatology distributions, multiple quantile regression, and support vector quantile regression are used for comparison where results demonstrate the proposed approach leads to improved performance while preventing the problem of overlapping quantile estimates.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PXLTKIPE\Hatalis et al. - 2017 - Smooth Pinball Neural Network for Probabilistic Fo.pdf}
}

@article{he_convolutional_2014,
  title = {Convolutional {{Neural Networks}} at {{Constrained Time Cost}}},
  author = {He, Kaiming and Sun, Jian},
  year = {2014},
  month = dec,
  journal = {arXiv:1412.1710 [cs]},
  eprint = {1412.1710},
  primaryclass = {cs},
  urldate = {2020-01-27},
  abstract = {Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and timeconsuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8\% top-5 error, 10-view test), yet is 20\% faster than ``AlexNet'' [14] (16.0\% top-5 error, 10-view test).},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TMR3UW8A\He and Sun - 2014 - Convolutional Neural Networks at Constrained Time .pdf}
}

@article{he_deep_2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.03385 [cs]},
  eprint = {1512.03385},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\textemdash 8\texttimes{} deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\KTUJQ65A\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf}
}

@article{hendrycks_gaussian_2018,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2018},
  month = nov,
  journal = {arXiv:1606.08415 [cs]},
  eprint = {1606.08415},
  primaryclass = {cs},
  urldate = {2019-12-19},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. The GELU nonlinearity weights inputs by their magnitude, rather than gates inputs by their sign as in ReLUs. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HYVPWPW9\Hendrycks and Gimpel - 2018 - Gaussian Error Linear Units (GELUs).pdf}
}

@incollection{hermans_training_2013,
  title = {Training and {{Analysing Deep Recurrent Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Hermans, Michiel and Schrauwen, Benjamin},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  year = {2013},
  pages = {190--198},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3GKM5MHG\\Hermans and Schrauwen - 2013 - Training and Analysing Deep Recurrent Neural Netwo.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\L3284Z8J\\5166-training-and-analysing-deep-recurrent-neural-networks.html}
}

@article{hersbach_decomposition_2000,
  title = {Decomposition of the {{Continuous Ranked Probability Score}} for {{Ensemble Prediction Systems}}},
  author = {Hersbach, Hans},
  year = {2000},
  month = oct,
  journal = {Weather and Forecasting},
  volume = {15},
  number = {5},
  pages = {559--570},
  publisher = {{American Meteorological Society}},
  issn = {1520-0434, 0882-8156},
  doi = {10.1175/1520-0434(2000)015<0559:DOTCRP>2.0.CO;2},
  urldate = {2023-10-09},
  abstract = {Abstract Some time ago, the continuous ranked probability score (CRPS) was proposed as a new verification tool for (probabilistic) forecast systems. Its focus is on the entire permissible range of a certain (weather) parameter. The CRPS can be seen as a ranked probability score with an infinite number of classes, each of zero width. Alternatively, it can be interpreted as the integral of the Brier score over all possible threshold values for the parameter under consideration. For a deterministic forecast system the CRPS reduces to the mean absolute error. In this paper it is shown that for an ensemble prediction system the CRPS can be decomposed into a reliability part and a resolution/uncertainty part, in a way that is similar to the decomposition of the Brier score. The reliability part of the CRPS is closely connected to the rank histogram of the ensemble, while the resolution/uncertainty part can be related to the average spread within the ensemble and the behavior of its outliers. The usefulness of such a decomposition is illustrated for the ensemble prediction system running at the European Centre for Medium-Range Weather Forecasts. The evaluation of the CRPS and its decomposition proposed in this paper can be extended to systems issuing continuous probability forecasts, by realizing that these can be interpreted as the limit of ensemble forecasts with an infinite number of members.},
  chapter = {Weather and Forecasting},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\DRBJ5EX7\Hersbach - 2000 - Decomposition of the Continuous Ranked Probability.pdf}
}

@misc{hewamalage_look_2021,
  title = {A {{Look}} at the {{Evaluation Setup}} of the {{M5 Forecasting Competition}}},
  author = {Hewamalage, Hansika and {Montero-Manso}, Pablo and Bergmeir, Christoph and Hyndman, Rob J.},
  year = {2021},
  month = aug,
  number = {arXiv:2108.03588},
  eprint = {2108.03588},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2108.03588},
  urldate = {2022-05-24},
  abstract = {Forecast evaluation plays a key role in how empirical evidence shapes the development of the discipline. Domain experts are interested in error measures relevant for their decision making needs. Such measures may produce unreliable results. Although reliability properties of several metrics have already been discussed, it has hardly been quantified in an objective way. We propose a measure named Rank Stability, which evaluates how much the rankings of an experiment differ in between similar datasets, when the models and errors are constant. We use this to study the evaluation setup of the M5. We find that the evaluation setup of the M5 is less reliable than other measures. The main drivers of instability are hierarchical aggregation and scaling. Price-weighting reduces the stability of all tested error measures. Scale normalization of the M5 error measure results in less stability than other scale-free errors. Hierarchical levels taken separately are less stable with more aggregation, and their combination is even less stable than individual levels. We also show positive tradeoffs of retaining aggregation importance without affecting stability. Aggregation and stability can be linked to the influence of much debated magic numbers. Many of our findings can be applied to general hierarchical forecast benchmarking.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UXH6L32Y\\Hewamalage et al. - 2021 - A Look at the Evaluation Setup of the M5 Forecasti.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\P9RBEL7S\\2108.html}
}

@article{hewamalage_recurrent_2021,
  title = {Recurrent {{Neural Networks}} for {{Time Series Forecasting}}: {{Current}} Status and Future Directions},
  shorttitle = {Recurrent {{Neural Networks}} for {{Time Series Forecasting}}},
  author = {Hewamalage, Hansika and Bergmeir, Christoph and Bandara, Kasun},
  year = {2021},
  month = jan,
  journal = {International Journal of Forecasting},
  volume = {37},
  number = {1},
  pages = {388--427},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2020.06.008},
  urldate = {2021-07-21},
  abstract = {Recurrent Neural Networks (RNNs) have become competitive forecasting methods, as most notably shown in the winning method of the recent M4 competition. However, established statistical models such as exponential smoothing (ETS) and the autoregressive integrated moving average (ARIMA) gain their popularity not only from their high accuracy, but also because they are suitable for non-expert users in that they are robust, efficient, and automatic. In these areas, RNNs have still a long way to go. We present an extensive empirical study and an open-source software framework of existing RNN architectures for forecasting, and we develop guidelines and best practices for their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series in the dataset possess homogeneous seasonal patterns; otherwise, we recommend a deseasonalisation step. Comparisons against ETS and ARIMA demonstrate that (semi-) automatic RNN models are not silver bullets, but they are nevertheless competitive alternatives in many situations.},
  langid = {english},
  keywords = {Best practices,Big data,Forecasting,Framework},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\AHKLIZJL\\Hewamalage et al. - 2021 - Recurrent Neural Networks for Time Series Forecast.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\SZ2Q2BJH\\S0169207020300996.html}
}

@article{highlander_very_2016,
  title = {Very {{Efficient Training}} of {{Convolutional Neural Networks}} Using {{Fast Fourier Transform}} and {{Overlap-and-Add}}},
  author = {Highlander, Tyler and Rodriguez, Andres},
  year = {2016},
  month = jan,
  journal = {arXiv:1601.06815 [cs]},
  eprint = {1601.06815},
  primaryclass = {cs},
  urldate = {2021-01-21},
  abstract = {Convolutional neural networks (CNNs) are currently state-of-the-art for various classification tasks, but are computationally expensive. Propagating through the convolutional layers is very slow, as each kernel in each layer must sequentially calculate many dot products for a single forward and backward propagation which equates to \$\textbackslash mathcal\{O\}(N\^\{2\}n\^\{2\})\$ per kernel per layer where the inputs are \$N \textbackslash times N\$ arrays and the kernels are \$n \textbackslash times n\$ arrays. Convolution can be efficiently performed as a Hadamard product in the frequency domain. The bottleneck is the transformation which has a cost of \$\textbackslash mathcal\{O\}(N\^\{2\}\textbackslash log\_2 N)\$ using the fast Fourier transform (FFT). However, the increase in efficiency is less significant when \$N\textbackslash gg n\$ as is the case in CNNs. We mitigate this by using the "overlap-and-add" technique reducing the computational complexity to \$\textbackslash mathcal\{O\}(N\^2\textbackslash log\_2 n)\$ per kernel. This method increases the algorithm's efficiency in both the forward and backward propagation, reducing the training and testing time for CNNs. Our empirical results show our method reduces computational time by a factor of up to 16.3 times the traditional convolution implementation for a 8 \$\textbackslash times\$ 8 kernel and a 224 \$\textbackslash times\$ 224 image.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LLT94BCN\\Highlander and Rodriguez - 2016 - Very Efficient Training of Convolutional Neural Ne.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\U5NU6BAB\\1601.html}
}

@article{hilbers_quantifying_2020,
  title = {Quantifying Demand and Weather Uncertainty in Power System Models Using the m out of n Bootstrap},
  author = {Hilbers, Adriaan P. and Brayshaw, David J. and Gandy, Axel},
  year = {2020},
  month = jan,
  journal = {arXiv:1912.10326 [stat]},
  eprint = {1912.10326},
  primaryclass = {stat},
  urldate = {2020-02-19},
  abstract = {This paper introduces a novel approach to quantify demand \& weather uncertainty in power system models. Recent studies indicate that such sampling uncertainty, originating from demand \& weather time series inputs, leads to significant uncertainties in model outputs and should not be ignored, especially with increasing levels of weather-dependent renewable generation. However, established uncertainty quantification approaches fail in this context due to the computational resources and additional data required for Monte Carlo-based analysis. The methodology introduced in this paper quantifies demand \& weather uncertainty using a time series bootstrap scheme with shorter time series than the original, enhancing computational efficiency and avoiding the need for any additional data. It can be used both to quantify output uncertainty and to determine optimal sample lengths for prescribed confidence levels. Simulations are performed on three generation \& transmission expansion planning models and a simple test is introduced allowing users to determine whether estimated uncertainty bounds are valid. Furthermore, a set of sample power system models, along with 38 years of time series data, are made available as open-source software and may serve as benchmarks for further renewable energy or time series analysis.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9KR9NESW\\Hilbers et al. - 2020 - Quantifying demand and weather uncertainty in powe.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CBTH8GHZ\\1912.html}
}

@article{hinton_deep_2012,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}: {{The Shared Views}} of {{Four Research Groups}}},
  shorttitle = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  year = {2012},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {82--97},
  issn = {1053-5888},
  doi = {10.1109/MSP.2012.2205597},
  urldate = {2019-03-19},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7BSKWRU8\Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf}
}

@article{hinton_distilling_2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  month = mar,
  journal = {arXiv:1503.02531 [cs, stat]},
  eprint = {1503.02531},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JJSJU4RG\Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf}
}

@article{hinton_fast_2006,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  year = {2006},
  month = jul,
  journal = {Neural Computation},
  volume = {18},
  number = {7},
  pages = {1527--1554},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2006.18.7.1527},
  urldate = {2019-03-19},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\DMCY6H9E\Hinton et al. - 2006 - A Fast Learning Algorithm for Deep Belief Nets.pdf}
}

@article{hinton_improving_2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  year = {2012},
  month = jul,
  journal = {arXiv:1207.0580 [cs]},
  eprint = {1207.0580},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JEATQU6P\Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf}
}

@article{hinton_reducing_2006,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  author = {Hinton, G. E.},
  year = {2006},
  month = jul,
  journal = {Science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1127647},
  urldate = {2019-03-19},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WQ6IP5AU\Hinton - 2006 - Reducing the Dimensionality of Data with Neural Ne.pdf}
}

@article{hochreiter_long_1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2019-04-15},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  keywords = {read - in related work},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RXWNIKUM\\2604.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\SK7TA5AA\\neco.1997.9.8.html}
}

@article{hollyman_understanding_2021,
  title = {Understanding Forecast Reconciliation},
  author = {Hollyman, Ross and Petropoulos, Fotios and Tipping, Michael E.},
  year = {2021},
  month = oct,
  journal = {European Journal of Operational Research},
  volume = {294},
  number = {1},
  pages = {149--160},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2021.01.017},
  urldate = {2021-10-06},
  abstract = {A series of recent papers introduce the concept of Forecast Reconciliation, a process by which independently generated forecasts of a collection of linearly related time series are reconciled via the introduction of accounting aggregations that naturally apply to the data. Aside from its clear presentational and operational virtues, the reconciliation approach generally improves the accuracy of the combined forecasts. In this paper, we examine the mechanisms by which this improvement is generated by re-formulating the reconciliation problem as a combination of direct forecasts of each time series with additional indirect forecasts derived from the linear constraints. Our work establishes a direct link between the nascent Forecast Reconciliation literature and the extensive work on Forecast Combination. In the original hierarchical setting, our approach clarifies for the first time how unbiased forecasts for the entire collection can be generated from base forecasts made at any level of the hierarchy, and we illustrate more generally how simple robust combined forecasts can be generated in any multivariate setting subject to linear constraints. In an empirical example, we show that simple combinations of such forecasts generate significant improvements in forecast accuracy where it matters most: where noise levels are highest and the forecasting task is at its most challenging.},
  langid = {english},
  keywords = {Forecast combinations,Forecasting,Hierarchies,Top-down,Unbiasedness},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WZ6XA6SP\\Hollyman et al. - 2021 - Understanding forecast reconciliation.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CTLWH4Z4\\S0377221721000199.html}
}

@article{holt_forecasting_2004,
  title = {Forecasting Seasonals and Trends by Exponentially Weighted Moving Averages},
  author = {Holt, Charles C.},
  year = {2004},
  month = jan,
  journal = {International Journal of Forecasting},
  volume = {20},
  number = {1},
  pages = {5--10},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2003.09.015},
  urldate = {2019-04-17},
  abstract = {The paper provides a systematic development of the forecasting expressions for exponential weighted moving averages. Methods for series with no trend, or additive or multiplicative trend are examined. Similarly, the methods cover non-seasonal, and seasonal series with additive or multiplicative error structures. The paper is a reprinted version of the 1957 report to the Office of Naval Research (ONR 52) and is being published here to provide greater accessibility.},
  keywords = {Exponential smoothing,Forecasting,Local seasonals,Local trends,read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\66AJXSMY\\Holt - 2004 - Forecasting seasonals and trends by exponentially .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5KDWMRDI\\S0169207003001134.html}
}

@article{hothorn_unbiased_2006,
  title = {Unbiased {{Recursive Partitioning}}: {{A Conditional Inference Framework}}},
  shorttitle = {Unbiased {{Recursive Partitioning}}},
  author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
  year = {2006},
  month = sep,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {15},
  number = {3},
  pages = {651--674},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/106186006X133933},
  urldate = {2019-09-06},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WI8NDV6H\Hothorn et al. - 2006 - Unbiased Recursive Partitioning A Conditional Inf.pdf}
}

@misc{hrinchuk_tensorized_2020,
  title = {Tensorized {{Embedding Layers}} for {{Efficient Model Compression}}},
  author = {Hrinchuk, Oleksii and Khrulkov, Valentin and Mirvakhabova, Leyla and Orlova, Elena and Oseledets, Ivan},
  year = {2020},
  month = feb,
  number = {arXiv:1901.10787},
  eprint = {1901.10787},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1901.10787},
  urldate = {2023-05-05},
  abstract = {The embedding layers transforming input words into real vectors are the key components of deep neural networks used in natural language processing. However, when the vocabulary is large, the corresponding weight matrices can be enormous, which precludes their deployment in a limited resource setting. We introduce a novel way of parametrizing embedding layers based on the Tensor Train (TT) decomposition, which allows compressing the model significantly at the cost of a negligible drop or even a slight gain in performance. We evaluate our method on a wide range of benchmarks in natural language processing and analyze the trade-off between performance and compression ratios for a wide range of architectures, from MLPs to LSTMs and Transformers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BDK6X3BM\\Hrinchuk et al. - 2020 - Tensorized Embedding Layers for Efficient Model Co.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\255TZDJU\\1901.html}
}

@article{hu_applying_2019,
  title = {Applying {{SVGD}} to {{Bayesian Neural Networks}} for {{Cyclical Time-Series Prediction}} and {{Inference}}},
  author = {Hu, Xinyu and Szerlip, Paul and Karaletsos, Theofanis and Singh, Rohit},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.05906 [cs, stat]},
  eprint = {1901.05906},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {A regression-based BNN model is proposed to predict spatiotemporal quantities like hourly rider demand with calibrated uncertainties. The main contributions of this paper are (i) A feed-forward deterministic neural network (DetNN) architecture that predicts cyclical time series data with sensitivity to anomalous forecasting events; (ii) A Bayesian framework applying SVGD to train large neural networks for such tasks, capable of producing time series predictions as well as measures of uncertainty surrounding the predictions. Experiments show that the proposed BNN reduces average estimation error by 10\% across 8 U.S. cities compared to a fine-tuned multilayer perceptron (MLP), and 4\% better than the same network architecture trained without SVGD.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\88LECSUA\\Hu et al. - 2019 - Applying SVGD to Bayesian Neural Networks for Cycl.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\83EE4IGJ\\1901.html}
}

@inproceedings{hu_listening_2018,
  title = {Listening to {{Chaotic Whispers}}: {{A Deep Learning Framework}} for {{News-oriented Stock Trend Prediction}}},
  shorttitle = {Listening to {{Chaotic Whispers}}},
  booktitle = {Proceedings of the {{Eleventh ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Hu, Ziniu and Liu, Weiqing and Bian, Jiang and Liu, Xuanzhe and Liu, Tie-Yan},
  year = {2018},
  series = {{{WSDM}} '18},
  pages = {261--269},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3159652.3159690},
  urldate = {2019-04-26},
  abstract = {Stock trend prediction plays a critical role in seeking maximized profit from the stock investment. However, precise trend prediction is very difficult since the highly volatile and non-stationary nature of the stock market. Exploding information on the Internet together with the advancing development of natural language processing and text mining techniques have enabled investors to unveil market trends and volatility from online content. Unfortunately, the quality, trustworthiness, and comprehensiveness of online content related to stock market vary drastically, and a large portion consists of the low-quality news, comments, or even rumors. To address this challenge, we imitate the learning process of human beings facing such chaotic online news, driven by three principles: sequential content dependency, diverse influence, and effective and efficient learning. In this paper, to capture the first two principles, we designed a Hybrid Attention Networks(HAN) to predict the stock trend based on the sequence of recent related news. Moreover, we apply the self-paced learning mechanism to imitate the third principle. Extensive experiments on real-world stock market data demonstrate the effectiveness of our framework. A further simulation illustrates that a straightforward trading strategy based on our proposed framework can significantly increase the annualized return.},
  isbn = {978-1-4503-5581-0},
  keywords = {deep learning,read - in related work,stock trend prediction,text mining},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MD64FNLQ\1712.02136.pdf}
}

@misc{hu_modelbased_2022,
  title = {Model-{{Based Imitation Learning}} for {{Urban Driving}}},
  author = {Hu, Anthony and Corrado, Gianluca and Griffiths, Nicolas and Murez, Zak and Gurau, Corina and Yeo, Hudson and Kendall, Alex and Cipolla, Roberto and Shotton, Jamie},
  year = {2022},
  month = oct,
  number = {arXiv:2210.07729},
  eprint = {2210.07729},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.07729},
  urldate = {2022-11-03},
  abstract = {An accurate model of the environment and the dynamic agents acting in it offers great potential for improving motion planning. We present MILE: a Model-based Imitation LEarning approach to jointly learn a model of the world and a policy for autonomous driving. Our method leverages 3D geometry as an inductive bias and learns a highly compact latent space directly from high-resolution videos of expert demonstrations. Our model is trained on an offline corpus of urban driving data, without any online interaction with the environment. MILE improves upon prior state-of-the-art by 35\% in driving score on the CARLA simulator when deployed in a completely new town and new weather conditions. Our model can predict diverse and plausible states and actions, that can be interpretably decoded to bird's-eye view semantic segmentation. Further, we demonstrate that it can execute complex driving manoeuvres from plans entirely predicted in imagination. Our approach is the first camera-only method that models static scene, dynamic scene, and ego-behaviour in an urban driving environment. The code and model weights are available at https://github.com/wayveai/mile.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\QEPWW7TH\\Hu et al. - 2022 - Model-Based Imitation Learning for Urban Driving.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GNV6FDVB\\2210.html}
}

@article{hua_deep_2018,
  title = {Deep {{Learning}} with {{Long Short-Term Memory}} for {{Time Series Prediction}}},
  author = {Hua, Yuxiu and Zhao, Zhifeng and Li, Rongpeng and Chen, Xianfu and Liu, Zhiming and Zhang, Honggang},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.10161 [cs]},
  eprint = {1810.10161},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Time series prediction can be generalized as a process that extracts useful information from historical records and then determines future values. Learning long-range dependencies that are embedded in time series is often an obstacle for most algorithms, whereas Long Short-Term Memory (LSTM) solutions, as a specific kind of scheme in deep learning, promise to effectively overcome the problem. In this article, we first give a brief introduction to the structure and forward propagation mechanism of the LSTM model. Then, aiming at reducing the considerable computing cost of LSTM, we put forward the Random Connectivity LSTM (RCLSTM) model and test it by predicting traffic and user mobility in telecommunication networks. Compared to LSTM, RCLSTM is formed via stochastic connectivity between neurons, which achieves a significant breakthrough in the architecture formation of neural networks. In this way, the RCLSTM model exhibits a certain level of sparsity, which leads to an appealing decrease in the computational complexity and makes the RCLSTM model become more applicable in latency-stringent application scenarios. In the field of telecommunication networks, the prediction of traffic series and mobility traces could directly benefit from this improvement as we further demonstrate that the prediction accuracy of RCLSTM is comparable to that of the conventional LSTM no matter how we change the number of training samples or the length of input sequences.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XLNIRTVN\\Hua et al. - 2018 - Deep Learning with Long Short-Term Memory for Time.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\E89EC7L7\\1810.html}
}

@article{huang_causal_2019,
  title = {Causal {{Discovery}} and {{Forecasting}} in {{Nonstationary Environments}} with {{State-Space Models}}},
  author = {Huang, Biwei and Zhang, Kun and Gong, Mingming and Glymour, Clark},
  year = {2019},
  month = may,
  journal = {arXiv:1905.10857 [cs, stat]},
  eprint = {1905.10857},
  primaryclass = {cs, stat},
  urldate = {2019-05-28},
  abstract = {In many scientific fields, such as economics and neuroscience, we are often faced with nonstationary time series, and concerned with both finding causal relations and forecasting the values of variables of interest, both of which are particularly challenging in such nonstationary environments. In this paper, we study causal discovery and forecasting for nonstationary time series. By exploiting a particular type of state-space model to represent the processes, we show that nonstationarity helps to identify causal structure and that forecasting naturally benefits from learned causal knowledge. Specifically, we allow changes in both causal strengths and noise variances in the nonlinear state-space models, which, interestingly, renders both the causal structure and model parameters identifiable. Given the causal model, we treat forecasting as a problem in Bayesian inference in the causal model, which exploits the time-varying property of the data and adapts to new observations in a principled manner. Experimental results on synthetic and real-world data sets demonstrate the efficacy of the proposed methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IIYELC4D\\Huang et al. - 2019 - Causal Discovery and Forecasting in Nonstationary .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\44RWCJ4R\\1905.html}
}

@article{huang_centroid_2019,
  title = {Centroid {{Networks}} for {{Few-Shot Clustering}} and {{Unsupervised Few-Shot Classification}}},
  author = {Huang, Gabriel and Larochelle, Hugo and {Lacoste-Julien}, Simon},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.08605 [cs, stat]},
  eprint = {1902.08605},
  primaryclass = {cs, stat},
  urldate = {2019-09-27},
  abstract = {Traditional clustering algorithms such as Kmeans rely heavily on the nature of the chosen metric or data representation. To get meaningful clusters, these representations need to be tailored to the downstream task (e.g. cluster photos by object category, cluster faces by identity). Therefore, we frame clustering as a meta-learning task, few-shot clustering, which allows us to specify how to cluster the data at the meta-training level, despite the clustering algorithm itself being unsupervised.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00001},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\Q9RHUKNA\Huang et al. - 2019 - Centroid Networks for Few-Shot Clustering and Unsu.pdf}
}

@incollection{huang_learning_2011,
  title = {Learning {{Auto-regressive Models}} from {{Sequence}} and {{Non-sequence Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  author = {Huang, Tzu-kuo and Schneider, Jeff G.},
  editor = {{Shawe-Taylor}, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  pages = {1548--1556},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\E3HLGPY2\\Huang and Schneider - 2011 - Learning Auto-regressive Models from Sequence and .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TH555R94\\4482-learning-auto-regressive-models-from-sequence-and-non-sequence-data.html}
}

@article{hyndman_another_2006,
  title = {Another Look at {{Forecast-Accuracy Metrics}} for {{Intermittent Demand}}},
  author = {Hyndman, Rob J},
  year = {2006},
  month = jun,
  journal = {Foresight: The International Journal of Applied Forecasting},
  pages = {4},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\V9ENERH8\Hyndman - ANOTHER LOOK AT FORECAST-ACCURACY METRICS FOR INTE.pdf}
}

@article{hyndman_fast_2016,
  title = {Fast Computation of Reconciled Forecasts for Hierarchical and Grouped Time Series},
  author = {Hyndman, Rob J. and Lee, Alan J. and Wang, Earo},
  year = {2016},
  month = may,
  journal = {Computational Statistics \& Data Analysis},
  volume = {97},
  pages = {16--32},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2015.11.007},
  urldate = {2021-02-04},
  abstract = {It is shown that the least squares approach to reconciling hierarchical time series forecasts can be extended to much more general collections of time series with aggregation constraints. The constraints arise due to the need for forecasts of collections of time series to add up in the same way as the observed time series. It is also shown that the computations involved can be handled efficiently by exploiting the structure of the associated design matrix, or by using sparse matrix routines. The proposed algorithms make forecast reconciliation feasible in business applications involving very large numbers of time series.},
  langid = {english},
  keywords = {Combining forecasts,Grouped time series,Hierarchical time series,Reconciling forecasts,Weighted least squares},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RDVVE2SQ\Hyndman et al. - 2016 - Fast computation of reconciled forecasts for hiera.pdf}
}

@book{hyndman_forecasting_2008,
  title = {Forecasting with {{Exponential Smoothing}}: {{The State Space Approach}}},
  shorttitle = {Forecasting with {{Exponential Smoothing}}},
  author = {Hyndman, Rob and Koehler, Anne B. and Ord, J. Keith and Snyder, Ralph D.},
  year = {2008},
  month = jun,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Exponential smoothing methods have been around since the 1950s, and are still the most popular forecasting methods used in business and industry. However, a modeling framework incorporating stochastic models, likelihood calculation, prediction intervals and procedures for model selection, was not developed until recently. This book brings together all of the important new results on the state space framework for exponential smoothing. It will be of interest to people wanting to apply the methods in their own area of interest as well as for researchers wanting to take the ideas in new directions. Part 1 provides an introduction to exponential smoothing and the underlying models. The essential details are given in Part 2, which also provide links to the most important papers in the literature. More advanced topics are covered in Part 3, including the mathematical properties of the models and extensions of the models for specific problems. Applications to particular domains are discussed in Part 4.},
  isbn = {978-3-540-71918-2},
  langid = {english},
  keywords = {Business \& Economics / Economics / Theory,Business \& Economics / Statistics,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@book{hyndman_forecasting_2018,
  title = {Forecasting: {{Principles}} and {{Practice}}},
  shorttitle = {Forecasting},
  author = {Hyndman, Rob J},
  year = {2018},
  urldate = {2019-04-15},
  abstract = {2nd edition},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\M7XKVZ6R\\fpp-notes.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8RJCPMW8\\fpp2.html}
}

@book{hyndman_forecasting_2021,
  title = {Forecasting: {{Principles}} and {{Practice}} (3rd Ed)},
  shorttitle = {Forecasting},
  author = {Hyndman, Rob J. and Athanasopoulos, George},
  year = {2021},
  publisher = {{OTexts: Melbourne}},
  address = {{Australia}},
  urldate = {2022-05-24},
  abstract = {3rd edition},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BZESYZWP\fpp3.html}
}

@article{hyndman_optimal_2011,
  title = {Optimal Combination Forecasts for Hierarchical Time Series},
  author = {Hyndman, Rob J. and Ahmed, Roman A. and Athanasopoulos, George and Shang, Han Lin},
  year = {2011},
  month = sep,
  journal = {Computational Statistics \& Data Analysis},
  volume = {55},
  number = {9},
  pages = {2579--2589},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2011.03.006},
  urldate = {2021-02-04},
  abstract = {In many applications, there are multiple time series that are hierarchically organized and can be aggregated at several different levels in groups based on products, geography or some other features. We call these ``hierarchical time series''. They are commonly forecast using either a ``bottom-up'' or a ``top-down'' method. In this paper we propose a new approach to hierarchical forecasting which provides optimal forecasts that are better than forecasts produced by either a top-down or a bottom-up approach. Our method is based on independently forecasting all series at all levels of the hierarchy and then using a regression model to optimally combine and reconcile these forecasts. The resulting revised forecasts add up appropriately across the hierarchy, are unbiased and have minimum variance amongst all combination forecasts under some simple assumptions. We show in a simulation study that our method performs well compared to the top-down approach and the bottom-up method. We demonstrate our proposed method by forecasting Australian tourism demand where the data are disaggregated by purpose of travel and geographical region.},
  langid = {english},
  keywords = {Bottom-up forecasting,Combining forecasts,GLS regression,Hierarchical forecasting,Reconciling forecasts,Top-down forecasting},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9RNY3UNI\\Hyndman et al. - 2011 - Optimal combination forecasts for hierarchical tim.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NLKGUZPG\\S0167947311000971.html}
}

@article{iandola_squeezenet_2016,
  title = {{{SqueezeNet}}: {{AlexNet-level}} Accuracy with 50x Fewer Parameters and {$<$}0.{{5MB}} Model Size},
  shorttitle = {{{SqueezeNet}}},
  author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.07360 [cs]},
  eprint = {1602.07360},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Recent research on deep convolutional neural networks (CNNs) has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple CNN architectures that achieve that accuracy level. With equivalent accuracy, smaller CNN architectures offer at least three advantages: (1) Smaller CNNs require less communication across servers during distributed training. (2) Smaller CNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller CNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small CNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques, we are able to compress SqueezeNet to less than 0.5MB (510\texttimes{} smaller than AlexNet).},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\48X9ZQFX\Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer .pdf}
}

@article{ikonomovska_learning_2011,
  title = {Learning Model Trees from Evolving Data Streams},
  author = {Ikonomovska, Elena and Gama, Jo{\~a}o and D{\v z}eroski, Sa{\v s}o},
  year = {2011},
  month = jul,
  journal = {Data Mining and Knowledge Discovery},
  volume = {23},
  number = {1},
  pages = {128--168},
  issn = {1573-756X},
  doi = {10.1007/s10618-010-0201-y},
  urldate = {2019-09-05},
  abstract = {The problem of real-time extraction of meaningful patterns from time-changing data streams is of increasing importance for the machine learning and data mining communities. Regression in time-changing data streams is a relatively unexplored topic, despite the apparent applications. This paper proposes an efficient and incremental stream mining algorithm which is able to learn regression and model trees from possibly unbounded, high-speed and time-changing data streams. The algorithm is evaluated extensively in a variety of settings involving artificial and real data. To the best of our knowledge there is no other general purpose algorithm for incremental learning regression/model trees able to perform explicit change detection and informed adaptation. The algorithm performs online and in real-time, observes each example only once at the speed of arrival, and maintains at any-time a ready-to-use model tree. The tree leaves contain linear models induced online from the examples assigned to them, a process with low complexity. The algorithm has mechanisms for drift detection and model adaptation, which enable it to maintain accurate and updated regression models at any time. The drift detection mechanism exploits the structure of the tree in the process of local change detection. As a response to local drift, the algorithm is able to update the tree structure only locally. This approach improves the any-time performance and greatly reduces the costs of adaptation.},
  langid = {english},
  keywords = {Concept drift,Incremental algorithms,Model trees,Non-stationary data streams,On-line change detection,On-line learning,Regression trees,Stream data mining},
  annotation = {00195},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\XEEI95CX\Ikonomovska et al. - 2011 - Learning model trees from evolving data streams.pdf}
}

@article{ioannou_decision_2016,
  title = {Decision {{Forests}}, {{Convolutional Networks}} and the {{Models}} in-{{Between}}},
  author = {Ioannou, Yani and Robertson, Duncan and Zikic, Darko and Kontschieder, Peter and Shotton, Jamie and Brown, Matthew and Criminisi, Antonio},
  year = {2016},
  month = mar,
  journal = {arXiv:1603.01250 [cs]},
  eprint = {1603.01250},
  primaryclass = {cs},
  urldate = {2019-09-08},
  abstract = {This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {00039},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HG44VXI9\Ioannou et al. - 2016 - Decision Forests, Convolutional Networks and the M.pdf}
}

@article{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = feb,
  journal = {arXiv:1502.03167 [cs]},
  eprint = {1502.03167},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YE3WT7GX\Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf}
}

@article{jacobs_fast_2019,
  title = {A Fast Approach to Optimal Transport: {{The}} Back-and-Forth Method},
  shorttitle = {A Fast Approach to Optimal Transport},
  author = {Jacobs, Matt and L{\'e}ger, Flavien},
  year = {2019},
  month = may,
  journal = {arXiv:1905.12154 [math]},
  eprint = {1905.12154},
  primaryclass = {math},
  urldate = {2019-10-03},
  abstract = {We present a method to efficiently solve the optimal transportation problem for a general class of strictly convex costs. Given two probability measures supported on a discrete grid with n points we compute the optimal transport map between them in O(n log(n)) operations and O(n) storage space. Our approach allows us to solve optimal transportation problems on spatial grids as large as 4096x4096 and 384x384x384 in a matter of minutes.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\L7SQSGBJ\\Jacobs and LÃ©ger - 2019 - A fast approach to optimal transport The back-and.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YWW6II9J\\1905.html}
}

@article{jaderberg_decoupled_2016,
  title = {Decoupled {{Neural Interfaces}} Using {{Synthetic Gradients}}},
  author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = aug,
  journal = {arXiv:1608.05343 [cs]},
  eprint = {1608.05343},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass \textendash{} amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VZWRWBCW\Jaderberg et al. - 2016 - Decoupled Neural Interfaces using Synthetic Gradie.pdf}
}

@article{jahandari_online_2020,
  title = {Online {{Forecasting}} of {{Synchronous Time Series Based}} on {{Evolving Linear Models}}},
  author = {Jahandari, S. and Kalhor, A. and Araabi, B. N.},
  year = {2020},
  month = may,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {50},
  number = {5},
  pages = {1865--1876},
  issn = {2168-2232},
  doi = {10.1109/TSMC.2018.2789936},
  abstract = {This paper suggests evolving linear models as a powerful alternative for online forecasting of synchronous time series. First, using a priori knowledge of an expert, all known possible features are clustered in some categories so that each category includes homogeneous features. Then, a novel evolving correlation-based forward subset selection technique is used to determine relevant features from each category. Next, based on the selected features, an estimation of the output of the system is modeled through an evolving adaptive linear regression model. In evolving systems, selected features and their associated weights could vary over time based on new incoming data samples which contain new information. Finally, a soft combination of output estimations of all categories, in a new sense of Takagi-Sugeno fuzzy system, gives the prediction of the output of the system at each sampling time. The approach offers a certain new view at the enhancement of evolving forecasting models. The proposed approach embodies recursive learning and one-step-ahead incremental algorithms that progressively modify the model to ensure continuous learning, and self-organization of the model structure and its parameters. Two real-world problems, forecasting electricity load of the Electric Reliability Council of Texas region and stock price forecasting of technology sector of Standard \& Poor's 500 index, are provided to validate the developed method. Pros and cons of the proposed approach are comprehensively discussed and shown through simulation results and comparisons with other state-of-the-art techniques.},
  keywords = {Adaptation models,Adaptive linear regression (ALR),Biological system modeling,correlation analysis,evolving adaptive linear regression model,evolving correlation-based forward subset selection technique,evolving forecasting models,evolving linear models (ELMs),evolving systems,Feature extraction,feature selection,Forecasting,forecasting theory,fuzzy set theory,fuzzy systems,homogeneous features,incoming data samples,learning (artificial intelligence),linear models,load prediction,model structure,online forecasting,output estimations,powerful alternative,Prediction algorithms,Predictive models,pricing,regression analysis,sampling time,stock forecasting,stock price forecasting,synchronous time series,Takagi-Sugeno fuzzy system,time series,Time series analysis},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HUTVN5JX\Jahandari et al. - 2020 - Online Forecasting of Synchronous Time Series Base.pdf}
}

@article{januschowski_criteria_2020,
  title = {Criteria for Classifying Forecasting Methods},
  author = {Januschowski, Tim and Gasthaus, Jan and Wang, Yuyang and Salinas, David and Flunkert, Valentin and {Bohlke-Schneider}, Michael and Callot, Laurent},
  year = {2020},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {M4 {{Competition}}},
  volume = {36},
  number = {1},
  pages = {167--177},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.05.008},
  urldate = {2023-05-30},
  abstract = {Classifying forecasting methods as being either of a ``machine learning'' or ``statistical'' nature has become commonplace in parts of the forecasting literature and community, as exemplified by the M4 competition and the conclusion drawn by the organizers. We argue that this distinction does not stem from fundamental differences in the methods assigned to either class. Instead, this distinction is probably of a tribal nature, which limits the insights into the appropriateness and effectiveness of different forecasting methods. We provide alternative characteristics of forecasting methods which, in our view, allow to draw meaningful conclusions. Further, we discuss areas of forecasting which could benefit most from cross-pollination between the ML and the statistics communities.},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7SJCRH89\\Januschowski et al. - 2020 - Criteria for classifying forecasting methods.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BRSWX2KE\\S0169207019301529.html}
}

@article{januschowski_deep_2018,
  title = {Deep {{Learning}} for {{Forecasting}}: {{Current Trends}} and {{Challenges}}},
  shorttitle = {Deep {{Learning}} for {{Forecasting}}},
  author = {Januschowski, Tim and Gasthaus, Jan and Wang, Yuyang and Rangapuram, Syama Sundar and Callot, Laurent},
  year = {2018},
  journal = {Foresight: The International Journal of Applied Forecasting},
  number = {51},
  pages = {42--47},
  urldate = {2020-02-17},
  abstract = {In the first installment of this two-part article, Tim Januschowski and colleagues presented a tutorial on the basics of Deep Learning (DL) through neural networks (NNs), with illustrations of how NNs have been applied for forecasting product sales and other variables at Amazon. In this segment, they describe the pros and cons of forecasting through NNs and discuss some areas of current research designed to improve the application of NNs for forecasting. Copyright International Institute of Forecasters, 2018},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MPNR6H2C\y_3a2018_3ai_3a51_3ap_3a42-47.html}
}

@article{januschowski_forecasting_2022,
  title = {Forecasting with Trees},
  author = {Januschowski, Tim and Wang, Yuyang and Torkkola, Kari and Erkkil{\"a}, Timo and Hasson, Hilaf and Gasthaus, Jan},
  year = {2022},
  month = oct,
  journal = {International Journal of Forecasting},
  series = {Special {{Issue}}: {{M5}} Competition},
  volume = {38},
  number = {4},
  pages = {1473--1481},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.10.004},
  urldate = {2022-11-03},
  abstract = {The prevalence of approaches based on gradient boosted trees among the top contestants in the M5 competition is potentially the most eye-catching result. Tree-based methods out-shone other solutions, in particular deep learning-based solutions. The winners in both tracks of the M5 competition heavily relied on them. This prevalence is even more remarkable given the dominance of other methods in the literature and the M4 competition. This article tries to explain why tree-based methods were so widely used in the M5 competition. We see possibilities for future improvements of tree-based models and then distill some learnings for other approaches, including but not limited to neural networks.},
  langid = {english},
  keywords = {Deep Learning,Global forecasting models,Gradient Boosted Trees,Probabilistic forecasting,Random forests},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\X6QG3GFQ\\Januschowski et al. - 2022 - Forecasting with trees.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4R9DIQ9L\\S0169207021001679.html}
}

@article{jayasekara_timecaps_2019,
  title = {{{TimeCaps}}: {{Learning From Time Series Data}} with {{Capsule Networks}}},
  shorttitle = {{{TimeCaps}}},
  author = {Jayasekara, Hirunima and Jayasundara, Vinoj and Rajasegaran, Jathushan and Jayasekara, Sandaru and Seneviratne, Suranga and Rodrigo, Ranga},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.11800 [cs, stat]},
  eprint = {1911.11800},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {Capsule networks excel in understanding spatial relationships in 2D data for vision related tasks. Even though they are not designed to capture 1D temporal relationships, with TimeCaps we demonstrate that given the ability, capsule networks excel in understanding temporal relationships. To this end, we generate capsules along the temporal and channel dimensions creating two temporal feature detectors which learn contrasting relationships. TimeCaps surpasses the state-of-the-art results by achieving 96.21\% accuracy on identifying 13 Electrocardiogram (ECG) signal beat categories, while achieving on-par results on identifying 30 classes of short audio commands. Further, the instantiation parameters inherently learnt by the capsule networks allow us to completely parameterize 1D signals which opens various possibilities in signal processing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\S8N39JB6\\Jayasekara et al. - 2019 - TimeCaps Learning From Time Series Data with Caps.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MLNSSMV4\\1911.html}
}

@article{jensen_ensemble_2022,
  title = {Ensemble {{Conformalized Quantile Regression}} for {{Probabilistic Time Series Forecasting}}},
  author = {Jensen, Vilde and Bianchi, Filippo Maria and Anfinsen, Stian Normann},
  year = {2022},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--12},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2022.3217694},
  abstract = {This article presents a novel probabilistic forecasting method called ensemble conformalized quantile regression (EnCQR). EnCQR constructs distribution-free and approximately marginally valid prediction intervals (PIs), which are suitable for nonstationary and heteroscedastic time series data. EnCQR can be applied on top of a generic forecasting model, including deep learning architectures. EnCQR exploits a bootstrap ensemble estimator, which enables the use of conformal predictors for time series by removing the requirement of data exchangeability. The ensemble learners are implemented as generic machine learning algorithms performing quantile regression (QR), which allow the length of the PIs to adapt to local variability in the data. In the experiments, we predict time series characterized by a different amount of heteroscedasticity. The results demonstrate that EnCQR outperforms models based only on QR or conformal prediction (CP), and it provides sharper, more informative, and valid PIs.},
  keywords = {Conformal prediction (CP),deep neural networks (NNs),ensemble learning,Ensemble learning,Forecasting,heteroscedasticity,Medical services,Predictive models,Probabilistic forecasting,Probabilistic logic,quantile regression (QR),time series analysis,Time series analysis,Uncertainty,uncertainty quantification},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\W673D4XM\\Jensen et al. - 2022 - Ensemble Conformalized Quantile Regression for Pro.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PETF23C2\\9940232.html}
}

@article{jeon_probabilistic_2019,
  title = {Probabilistic Forecast Reconciliation with Applications to Wind Power and Electric Load},
  author = {Jeon, Jooyoung and Panagiotelis, Anastasios and Petropoulos, Fotios},
  year = {2019},
  month = dec,
  journal = {European Journal of Operational Research},
  volume = {279},
  number = {2},
  pages = {364--379},
  issn = {03772217},
  doi = {10.1016/j.ejor.2019.05.020},
  urldate = {2022-05-24},
  abstract = {New methods are proposed for adjusting probabilistic forecasts to ensure coherence with the aggregation constraints inherent in temporal hierarchies. The different approaches nested within this framework include methods that exploit information at all levels of the hierarchy as well as a novel method based on cross-validation. The methods are evaluated using real data from two wind farms in Crete and electric load in Boston. For these applications, optimal decisions related to grid operations and bidding strategies are based on coherent probabilistic forecasts of energy power. Empirical evidence is also presented showing that probabilistic forecast reconciliation improves the accuracy of the probabilistic forecasts.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\GCMPF3MR\Jeon et al. - 2019 - Probabilistic forecast reconciliation with applica.pdf}
}

@article{jiang_graph_2021,
  title = {Graph {{Neural Network}} for {{Traffic Forecasting}}: {{A Survey}}},
  shorttitle = {Graph {{Neural Network}} for {{Traffic Forecasting}}},
  author = {Jiang, Weiwei and Luo, Jiayun},
  year = {2021},
  month = feb,
  journal = {arXiv:2101.11174 [cs]},
  eprint = {2101.11174},
  primaryclass = {cs},
  urldate = {2021-07-26},
  abstract = {Traffic forecasting is important for the success of intelligent transportation systems. Deep learning models, including convolution neural networks and recurrent neural networks, have been extensively applied in traffic forecasting problems to model spatial and temporal dependencies. In recent years, to model the graph structures in transportation systems as well as contextual information, graph neural networks have been introduced and have achieved state-of-the-art performance in a series of traffic forecasting problems. In this survey, we review the rapidly growing body of research using different graph neural networks, e.g. graph convolutional and graph attention networks, in various traffic forecasting problems, e.g. road traffic flow and speed forecasting, passenger flow forecasting in urban rail transit systems, and demand forecasting in ride-hailing platforms. We also present a comprehensive list of open data and source resources for each problem and identify future research directions. To the best of our knowledge, this paper is the first comprehensive survey that explores the application of graph neural networks for traffic forecasting problems. We have also created a public GitHub repository where the latest papers, open data, and source resources will be updated.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MNFIZAQR\Jiang and Luo - 2021 - Graph Neural Network for Traffic Forecasting A Su.pdf}
}

@misc{jin_large_2023,
  title = {Large {{Models}} for {{Time Series}} and {{Spatio-Temporal Data}}: {{A Survey}} and {{Outlook}}},
  shorttitle = {Large {{Models}} for {{Time Series}} and {{Spatio-Temporal Data}}},
  author = {Jin, Ming and Wen, Qingsong and Liang, Yuxuan and Zhang, Chaoli and Xue, Siqiao and Wang, Xue and Zhang, James and Wang, Yi and Chen, Haifeng and Li, Xiaoli and Pan, Shirui and Tseng, Vincent S. and Zheng, Yu and Chen, Lei and Xiong, Hui},
  year = {2023},
  month = oct,
  number = {arXiv:2310.10196},
  eprint = {2310.10196},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.10196},
  urldate = {2023-10-19},
  abstract = {Temporal data, notably time series and spatio-temporal data, are prevalent in real-world applications. They capture dynamic system measurements and are produced in vast quantities by both physical and virtual sensors. Analyzing these data types is vital to harnessing the rich information they encompass and thus benefits a wide range of downstream tasks. Recent advances in large language and other foundational models have spurred increased use of these models in time series and spatio-temporal data mining. Such methodologies not only enable enhanced pattern recognition and reasoning across diverse domains but also lay the groundwork for artificial general intelligence capable of comprehending and processing common temporal data. In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks. Our objective is to equip practitioners with the knowledge to develop applications and further research in this underexplored domain. We primarily categorize the existing literature into two major clusters: large models for time series analysis (LM4TS) and spatio-temporal data mining (LM4STD). On this basis, we further classify research based on model scopes (i.e., general vs. domain-specific) and application areas/tasks. We also provide a comprehensive collection of pertinent resources, including datasets, model assets, and useful tools, categorized by mainstream applications. This survey coalesces the latest strides in large model-centric research on time series and spatio-temporal data, underscoring the solid foundations, current advances, practical applications, abundant resources, and future research opportunities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UZQSPIIY\\Jin et al. - 2023 - Large Models for Time Series and Spatio-Temporal D.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\J3MITPWJ\\2310.html}
}

@article{joe_families_1990,
  title = {Families of Min-Stable Multivariate Exponential and Multivariate Extreme Value Distributions},
  author = {Joe, Harry},
  year = {1990},
  month = jan,
  journal = {Statistics \& Probability Letters},
  volume = {9},
  number = {1},
  pages = {75--81},
  issn = {0167-7152},
  doi = {10.1016/0167-7152(90)90098-R},
  urldate = {2020-03-03},
  abstract = {Families of min-stable multivariate exponential and multivariate extreme value distributions are presented. Two families have a representation like the Marshall\textemdash Olkin multivariate exponential distribution. A primary application for these distributions is for analyzing multivariate extreme value data, although they may be useable in situations where exponential margins can be assumed.},
  langid = {english},
  keywords = {Extreme value distribution,multivariate exponential},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9YTQY4ZI\\Joe - 1990 - Families of min-stable multivariate exponential an.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MNK46FC5\\016771529090098R.html}
}

@article{johnson_billionscale_2017,
  title = {Billion-Scale Similarity Search with {{GPUs}}},
  author = {Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.08734 [cs]},
  eprint = {1702.08734},
  primaryclass = {cs},
  urldate = {2022-02-05},
  abstract = {Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a design for k-selection that operates at up to 55\% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5\texttimes{} faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach1 for the sake of comparison and reproducibility.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Data Structures and Algorithms,Computer Science - Databases,Computer Science - Information Retrieval},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RB7TI3SI\Johnson et al. - 2017 - Billion-scale similarity search with GPUs.pdf}
}

@article{jordan_evaluating_2019,
  title = {Evaluating {{Probabilistic Forecasts}} with {{{\textbf{scoringRules}}}}},
  author = {Jordan, Alexander and Kr{\"u}ger, Fabian and Lerch, Sebastian},
  year = {2019},
  journal = {Journal of Statistical Software},
  volume = {90},
  number = {12},
  issn = {1548-7660},
  doi = {10.18637/jss.v090.i12},
  urldate = {2021-02-09},
  abstract = {Probabilistic forecasts in the form of probability distributions over future events have become popular in several fields including meteorology, hydrology, economics, and demography. In typical applications, many alternative statistical models and data sources can be used to produce probabilistic forecasts. Hence, evaluating and selecting among competing methods is an important task. The scoringRules package for R provides functionality for comparative evaluation of probabilistic models based on proper scoring rules, covering a wide range of situations in applied work. This paper discusses implementation and usage details, presents case studies from meteorology and economics, and points to the relevant background literature.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\M3A2IBHE\Jordan et al. - 2019 - Evaluating Probabilistic Forecasts with scoring.pdf}
}

@article{jr_regression_,
  title = {Regression {{Modeling Strategies}}},
  author = {Jr, Frank E Harrell},
  pages = {416},
  langid = {english},
  annotation = {09923},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TPQQSU68\Jr - Regression Modeling Strategies.pdf}
}

@inproceedings{jr_selforganized_2011,
  title = {Self-{{Organized Hierarchical Methods}} for {{Time Series Forecasting}}},
  booktitle = {2011 {{IEEE}} 23rd {{International Conference}} on {{Tools}} with {{Artificial Intelligence}}},
  author = {Jr, F. M. Abinader and d Queiroz, A. C. S. and Honda, D. W.},
  year = {2011},
  month = nov,
  pages = {1057--1062},
  issn = {2375-0197},
  doi = {10.1109/ICTAI.2011.180},
  abstract = {Time series forecasting with the use of Artificial Neural Networks (ANN), in special with self-organized maps (SOM), has been explored in the literature with good results. One good strategy for improving computational cost and specialization of SOMs in general is constructing it via hierarchical structures. This work presents four different heuristics for constructing hierarchical SOMs for time series prediction, evaluating their computational cost and forecast precision and providing insight on future enhancements.},
  keywords = {artificial neural networks,forecast precision,Forecasting,Heuristic algorithms,Hierarchical SOM,mathematics computing,Prototypes,self organized hierarchical methods,self organized maps,self-organising feature maps,Self-Organized Maps,time series,Time series analysis,time series forecasting,Time Series Forecasting,Training,Vectors,Vegetation},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IXHBID6U\\Jr et al. - 2011 - Self-Organized Hierarchical Methods for Time Serie.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\S7UF55QG\\6103471.html}
}

@inproceedings{jun-jun_joint_2009,
  title = {A Joint Decision Model of Inventory Control and Promotion Optimization Based on Demand Forecasting},
  booktitle = {2009 {{IEEE International Conference}} on {{Automation}} and {{Logistics}}},
  author = {{Jun-jun}, Gao and Ting, Kang},
  year = {2009},
  month = aug,
  pages = {119--123},
  issn = {2161-816X},
  doi = {10.1109/ICAL.2009.5262965},
  abstract = {We study the problem of jointly determining the promotion optimizing and inventory control of multiple beer product variants in a Chinese retail supermarket based on demand forecasting. We examine the joint effect of two modes of promotional policies: price discount and inventory stimulation. Under the demand with the price sensitivity, inventory sensitivity and randomness, we formulate a more practical two-stage joint optimization model of inventory control and promotion, in which the non-promotional optimal price and allocation of shelf-space of all kinds of products are calculated to achieve maximal profit in the first stage and then to find the optimal promotion brands under the assumptions of non-promoted brands still continue the pricing policy and shelf-space allocation policy in the second stage model. Also a numerical solving algorithm is proposed and an experimental example is given. Our experimental analysis reveals that the two-stage joint optimization model has good feasibility and robustness. The computational results also show that the appropriate inventory control strategies and promotional methods would achieve higher profit.},
  keywords = {Automation,Business,Chinese retail supermarket,demand forecasting,Demand forecasting,inventory control,Inventory control,Inventory management,inventory replenishment,inventory sensitivity,joint optimization model,Logistics,Marketing and sales,Natural languages,optimisation,Optimization methods,Predictive models,price sensitivity,promotion optimization,promotional method,sensitivity analysis,shelf-space allocation policy,stock control,two-stage optimization model},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5YPTNW4N\\Jun-jun and Ting - 2009 - A joint decision model of inventory control and pr.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3UHEYM9D\\5262965.html}
}

@article{kaiser_tensor2tensor_,
  title = {{{Tensor2Tensor Transformers}}},
  author = {Kaiser, {\L}ukasz},
  pages = {33},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7E8Y3QZX\Kaiser - Tensor2Tensor Transformers.pdf}
}

@article{kanchymalay_multivariate_2017,
  title = {Multivariate {{Time Series Forecasting}} of {{Crude Palm Oil Price Using Machine Learning Techniques}}},
  author = {Kanchymalay, Kasturi and Salim, N. and Sukprasert, Anupong and Krishnan, Ramesh and Hashim, Ummi Raba'ah},
  year = {2017},
  month = aug,
  journal = {IOP Conference Series: Materials Science and Engineering},
  volume = {226},
  pages = {012117},
  publisher = {{IOP Publishing}},
  issn = {1757-899X},
  doi = {10.1088/1757-899X/226/1/012117},
  urldate = {2021-02-04},
  abstract = {The aim of this paper was to study the correlation between crude palm oil (CPO) price, selected vegetable oil prices (such as soybean oil, coconut oil, and olive oil, rapeseed oil and sunflower oil), crude oil and the monthly exchange rate. Comparative analysis was then performed on CPO price forecasting results using the machine learning techniques. Monthly CPO prices, selected vegetable oil prices, crude oil prices and monthly exchange rate data from January 1987 to February 2017 were utilized. Preliminary analysis showed a positive and high correlation between the CPO price and soy bean oil price and also between CPO price and crude oil price. Experiments were conducted using multi-layer perception, support vector regression and Holt Winter exponential smoothing techniques. The results were assessed by using criteria of root mean square error (RMSE), means absolute error (MAE), means absolute percentage error (MAPE) and Direction of accuracy (DA). Among these three techniques, support vector regression(SVR) with Sequential minimal optimization (SMO) algorithm showed relatively better results compared to multi-layer perceptron and Holt Winters exponential smoothing method.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6RSZH9TV\Kanchymalay et al. - 2017 - Multivariate Time Series Forecasting of Crude Palm.pdf}
}

@article{kang_ej_2019,
  title = {D\textbackslash 'ej\textbackslash `a vu: Forecasting with Similarity},
  shorttitle = {D\textbackslash 'ej\textbackslash `a Vu},
  author = {Kang, Yanfei and Spiliotis, Evangelos and Petropoulos, Fotios and Athiniotis, Nikolaos and Li, Feng and Assimakopoulos, Vassilios},
  year = {2019},
  month = nov,
  journal = {arXiv:1909.00221 [stat]},
  eprint = {1909.00221},
  primaryclass = {stat},
  urldate = {2020-02-17},
  abstract = {Accurate forecasts are vital for supporting the decisions of modern companies. To improve statistical forecasting performance, forecasters typically select the most appropriate model for each given time series. However, statistical models usually presume some data generation process, while making strong distributional assumptions about the errors. In this paper, we present a new approach to time series forecasting that relaxes these assumptions. A target series is forecasted by identifying similar series from a reference set (de\textasciiacute ja` vu). Instead of extrapolating, the future paths of the similar reference series are aggregated and serve as the basis for the forecasts of the target series. ``Forecasting with similarity'' is a data-centric approach that tackles model uncertainty without depending on statistical forecasting models. We offer definitions for deriving both the point forecasts and the corresponding prediction intervals. We evaluate the approach using a rich collection of real data and show that it results in good forecasting accuracy, especially for yearly series. Finally, while traditional statistical approaches underestimate the uncertainty around the forecasts, our approach results in upper coverage levels that are much closer to the nominal values.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Applications,Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\AWZW57YE\Kang et al. - 2019 - D'ej`a vu forecasting with similarity.pdf}
}

@article{kang_learning_2021,
  title = {Learning to {{Embed Categorical Features}} without {{Embedding Tables}} for {{Recommendation}}},
  author = {Kang, Wang-Cheng and Cheng, Derek Zhiyuan and Yao, Tiansheng and Yi, Xinyang and Chen, Ting and Hong, Lichan and Chi, Ed H.},
  year = {2021},
  month = jun,
  journal = {arXiv:2010.10784 [cs]},
  eprint = {2010.10784},
  primaryclass = {cs},
  urldate = {2021-12-16},
  abstract = {Embedding learning of categorical features (e.g. user/item IDs) is at the core of various recommendation models including matrix factorization and neural collaborative filtering. The standard approach creates an embedding table where each row represents a dedicated embedding vector for every unique feature value. However, this method fails to efficiently handle high-cardinality features and unseen feature values (e.g. new video ID) that are prevalent in real-world recommendation systems. In this paper, we propose an alternative embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a deep embedding network to compute embeddings on the fly. DHE first encodes the feature value to a unique identifier vector with multiple hashing functions and transformations, and then applies a DNN to convert the identifier vector to an embedding. The encoding module is deterministic, non-learnable, and free of storage, while the embedding network is updated during the training time to learn embedding generation. Empirical results show that DHE achieves comparable AUC against the standard one-hot full embedding, with smaller model sizes. Our work sheds light on the design of DNN-based alternative embedding schemes for categorical features without using embedding table lookup.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\65VNTM4N\Kang et al. - 2021 - Learning to Embed Categorical Features without Emb.pdf}
}

@article{karaahmetoglu_spatiotemporal_2021,
  title = {Spatio-Temporal {{Sequence Prediction}} with {{Point Processes}} and {{Self-organizing Decision Trees}}},
  author = {Karaahmetoglu, Oguzhan and Kozat, Suleyman S.},
  year = {2021},
  month = mar,
  journal = {arXiv:2006.14426 [cs, stat]},
  eprint = {2006.14426},
  primaryclass = {cs, stat},
  urldate = {2021-12-10},
  abstract = {We study the spatio-temporal prediction problem and introduce a novel point-process-based prediction algorithm. Spatio-temporal prediction is extensively studied in Machine Learning literature due to its critical real-life applications such as crime, earthquake, and social event prediction. Despite these thorough studies, specific problems inherent to the application domain are not yet fully explored. Here, we address the non-stationary spatio-temporal prediction problem on both densely and sparsely distributed sequences. We introduce a probabilistic approach that partitions the spatial domain into subregions and models the event arrivals in each region with interacting point-processes. Our algorithm can jointly learn the spatial partitioning and the interaction between these regions through a gradient-based optimization procedure. Finally, we demonstrate the performance of our algorithm on both simulated data and two real-life datasets. We compare our approach with baseline and state-of-the-art deep learning-based approaches, where we achieve significant performance improvements. Moreover, we also show the effect of using different parameters on the overall performance through empirical results and explain the procedure for choosing the parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IWCY4QWL\\Karaahmetoglu and Kozat - 2021 - Spatio-temporal Sequence Prediction with Point Pro.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XX435QDJ\\2006.html}
}

@article{karalic_linear_,
  title = {Linear {{Regression}} in {{Regression Tree Leaves}}},
  author = {Karalic, Aram},
  pages = {11},
  abstract = {The advantage of using linear regression in the leaves of a regression tree is analysed in the paper. It is carried out how this modi cation a ects the construction, pruning and interpretation of a regression tree. The modi cation is tested on arti cial and real-life domains where its impact on classi cation error and stability of the induced trees is considered. The results show that the modi cation is bene cial, as it leads to smaller classi cation errors of induced regression trees. The Bayesian approach to estimation of class distributions is used in all experiments.},
  langid = {english},
  annotation = {00070},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\F76NKGRD\Karalic - Linear Regression in Regression Tree Leaves.pdf}
}

@article{kari_novel_2016,
  title = {A {{Novel Family}} of {{Boosted Online Regression Algorithms}} with {{Strong Theoretical Bounds}}},
  author = {Kari, Dariush and Khan, Farhan and Ciftci, Selami and Kozat, Suleyman Serdar},
  year = {2016},
  month = jan,
  journal = {arXiv:1601.00549 [cs, math, stat]},
  eprint = {1601.00549},
  primaryclass = {cs, math, stat},
  urldate = {2019-08-06},
  abstract = {We investigate boosted online regression and propose a novel family of regression algorithms with strong theoretical bounds. In addition, we implement several variants of the proposed generic algorithm. We specifically provide theoretical bounds for the performance of our proposed algorithms that hold in a strong mathematical sense. We achieve guaranteed performance improvement over the conventional online regression methods without any statistical assumptions on the desired data or feature vectors. We demonstrate an intrinsic relationship, in terms of boosting, between the adaptive mixture-of-experts and data reuse algorithms. Furthermore, we introduce a boosting algorithm based on random updates that is significantly faster than the conventional boosting methods and other variants of our proposed algorithms while achieving an enhanced performance gain. Hence, the random updates method is specifically applicable to the fast and high dimensional streaming data. Specifically, we investigate Newton Method-based and Stochastic Gradient Descent-based linear regression algorithms in a mixture-of-experts setting and provide several variants of these well-known adaptation methods. However, the proposed algorithms can be extended to other base learners, e.g., nonlinear, tree-based piecewise linear. Furthermore, we provide theoretical bounds for the computational complexity of our proposed algorithms. We demonstrate substantial performance gains in terms of mean square error over the base learners through an extensive set of benchmark real data sets and simulated examples.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Mathematics - Statistics Theory},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\U4KZPZCD\Kari et al. - 2016 - A Novel Family of Boosted Online Regression Algori.pdf}
}

@article{karl_deep_2016,
  title = {Deep {{Variational Bayes Filters}}: {{Unsupervised Learning}} of {{State Space Models}} from {{Raw Data}}},
  shorttitle = {Deep {{Variational Bayes Filters}}},
  author = {Karl, Maximilian and Soelch, Maximilian and Bayer, Justin and {van der Smagt}, Patrick},
  year = {2016},
  month = may,
  journal = {arXiv:1605.06432 [cs, stat]},
  eprint = {1605.06432},
  primaryclass = {cs, stat},
  urldate = {2019-05-10},
  abstract = {We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Systems and Control,Statistics - Machine Learning},
  annotation = {00077},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BHWHYPNI\\Karl et al. - 2016 - Deep Variational Bayes Filters Unsupervised Learn.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FTHP9NNG\\1605.html}
}

@article{karmy_hierarchical_2019,
  title = {Hierarchical Time Series Forecasting via {{Support Vector Regression}} in the {{European Travel Retail Industry}}},
  author = {Karmy, Juan Pablo and Maldonado, Sebasti{\'a}n},
  year = {2019},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {137},
  pages = {59--73},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2019.06.060},
  urldate = {2021-02-04},
  abstract = {Times series often offers a natural disaggregation in a hierarchical structure. For example, product sales can come from different cities, districts, or states; or be grouped by categories and subcategories. This hierarchical structure can be useful for improving the forecast, and this strategy is known as hierarchical time series (HTS) analysis. In this work, a novel strategy for sales forecasting is proposed using Support Vector Regression (SVR) and hierarchical time series. We formalize three different hierarchical time series approaches: bottom-up SVR, top-down SVR, and middle-out SVR, and use them in a sales forecasting project for the Travel Retail Industry. Various hierarchical structures are proposed for the retail industry in order to achieve accurate product-level predictions. Experiments on these datasets demonstrate the virtues of SVR-based hierarchical time series in terms of predictive performance when compared with the traditional ARIMA and Holt-Winters approaches for this task.},
  langid = {english},
  keywords = {Hierarchical time series,Sales forecasting,Support Vector Regression,Time series analysis},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\32PLZ5IF\\Karmy and Maldonado - 2019 - Hierarchical time series forecasting via Support V.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NNUPG3UX\\S0957417419304683.html}
}

@article{karnin_optimal_2016,
  title = {Optimal {{Quantile Approximation}} in {{Streams}}},
  author = {Karnin, Zohar and Lang, Kevin and Liberty, Edo},
  year = {2016},
  month = apr,
  journal = {arXiv:1603.05346 [cs]},
  eprint = {1603.05346},
  primaryclass = {cs},
  urldate = {2022-02-04},
  abstract = {This paper resolves one of the longest standing basic problems in the streaming computational model. Namely, optimal construction of quantile sketches. An {$\epsilon$} approximate quantile sketch receives a stream of items x1, . . . , xn and allows one to approximate the rank of any query up to additive error {$\epsilon$}n with probability at least 1 - {$\delta$}. The rank of a query x is the number of stream items such that xi {$\leq$} x. The minimal sketch size required for this task is trivially at least 1/{$\epsilon$}. Felber and Ostrovsky obtain a O((1/{$\epsilon$}) log(1/{$\epsilon$})) space sketch for a fixed {$\delta$}. To date, no better upper or lower bounds were known even for randomly permuted streams or for approximating a specific quantile, e.g., the median. This paper obtains an O((1/{$\epsilon$}) log log(1/{$\delta$})) space sketch and a matching lower bound. This resolves the open problem and proves a qualitative gap between randomized and deterministic quantile sketching. One of our contributions is a novel representation and modification of the widely used merge-and-reduce construction. This subtle modification allows for an analysis which is both tight and extremely simple. Similar techniques should be useful for improving other sketching objectives and geometric coreset constructions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\EMM879YD\Karnin et al. - 2016 - Optimal Quantile Approximation in Streams.pdf}
}

@incollection{kazlas_direct_1995,
  title = {Direct {{Multi-Step Time Series Prediction Using TD}}(\textbackslash lambda )},
  booktitle = {Advances in {{Neural Information Processing Systems}} 7},
  author = {Kazlas, Peter T. and Weigend, Andreas S.},
  editor = {Tesauro, G. and Touretzky, D. S. and Leen, T. K.},
  year = {1995},
  pages = {721--728},
  publisher = {{MIT Press}},
  urldate = {2019-04-16},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6VYGNEWQ\\Kazlas and Weigend - 1995 - Direct Multi-Step Time Series Prediction Using TD(.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GN968HNS\\940-direct-multi-step-time-series-prediction-using-td.html}
}

@inproceedings{ke_deepgbm_2019,
  title = {{{DeepGBM}}: {{A Deep Learning Framework Distilled}} by {{GBDT}} for {{Online Prediction Tasks}}},
  shorttitle = {{{DeepGBM}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Ke, Guolin and Xu, Zhenhui and Zhang, Jia and Bian, Jiang and Liu, Tie-Yan},
  year = {2019},
  series = {{{KDD}} '19},
  pages = {384--394},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3292500.3330858},
  urldate = {2019-08-30},
  abstract = {Online prediction has become one of the most essential tasks in many real-world applications. Two main characteristics of typical online prediction tasks include tabular input space and online data generation. Specifically, tabular input space indicates the existence of both sparse categorical features and dense numerical ones, while online data generation implies continuous task-generated data with potentially dynamic distribution. Consequently, effective learning with tabular input space as well as fast adaption to online data generation become two vital challenges for obtaining the online prediction model. Although Gradient Boosting Decision Tree (GBDT) and Neural Network (NN) have been widely used in practice, either of them yields their own weaknesses. Particularly, GBDT can hardly be adapted to dynamic online data generation, and it tends to be ineffective when facing sparse categorical features; NN, on the other hand, is quite difficult to achieve satisfactory performance when facing dense numerical features. In this paper, we propose a new learning framework, DeepGBM, which integrates the advantages of the both NN and GBDT by using two corresponding NN components: (1) CatNN, focusing on handling sparse categorical features. (2) GBDT2NN, focusing on dense numerical features with distilled knowledge from GBDT. Powered by these two components, DeepGBM can leverage both categorical and numerical features while retaining the ability of efficient online update. Comprehensive experiments on a variety of publicly available datasets have demonstrated that DeepGBM can outperform other well-recognized baselines in various online prediction tasks.},
  isbn = {978-1-4503-6201-6},
  keywords = {gradient boosting decision tree,neural network},
  annotation = {00000}
}

@incollection{ke_lightgbm_2017,
  title = {{{LightGBM}}: {{A Highly Efficient Gradient Boosting Decision Tree}}},
  shorttitle = {{{LightGBM}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  year = {2017},
  pages = {3146--3154},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-07-03},
  annotation = {00294},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WRJSS3RM\\Ke et al. - 2017 - LightGBM A Highly Efficient Gradient Boosting Dec.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\AL6BKUBX\\6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.html}
}

@article{ke_tabnn_2018,
  title = {{{TabNN}}: {{A Universal Neural Network Solution}} for {{Tabular Data}}},
  shorttitle = {{{TabNN}}},
  author = {Ke, Guolin and Zhang, Jia and Xu, Zhenhui and Bian, Jiang and Liu, Tie-Yan},
  year = {2018},
  month = sep,
  urldate = {2019-08-30},
  abstract = {Neural Network (NN) has achieved state-of-the-art performances in many tasks within image, speech, and text domains. Such great success is mainly due to special structure design to fit the...},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\T5TS8CNI\\Ke et al. - 2018 - TabNN A Universal Neural Network Solution for Tab.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\M2RBV4HI\\forum.html}
}

@article{kechyn_sales_2018,
  title = {Sales Forecasting Using {{WaveNet}} within the Framework of the {{Kaggle}} Competition},
  author = {Kechyn, Glib and Yu, Lucius and Zang, Yangguang and Kechyn, Svyatoslav},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.04037 [cs]},
  eprint = {1803.04037},
  primaryclass = {cs},
  urldate = {2020-01-27},
  abstract = {We took part in the Corporacion Favorita Grocery Sales Forecasting competition hosted on Kaggle and achieved the 2nd place. In this abstract paper, we present an overall analysis and solution to the underlying machine-learning problem based on time series data, where major challenges are identified and corresponding preliminary methods are proposed. Our approach is based on the adaptation of dilated convolutional neural network for time series forecasting. By applying this technique iteratively to batches of n examples, a big amount of time series data can be eventually processed with a decent speed and accuracy. We hope this paper could serve, to some extent, as a review and guideline of the time series forecasting benchmark, inspiring further attempts and researches.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WCKGLU3D\Kechyn et al. - 2018 - Sales forecasting using WaveNet within the framewo.pdf}
}

@inproceedings{kersbergen_serenade_2022,
  title = {Serenade - {{Low-Latency Session-Based Recommendation}} in e-{{Commerce}} at {{Scale}}},
  booktitle = {Proceedings of the 2022 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Kersbergen, Barrie and Sprangers, Olivier and Schelter, Sebastian},
  year = {2022},
  month = jun,
  pages = {150--159},
  publisher = {{ACM}},
  address = {{Philadelphia PA USA}},
  doi = {10.1145/3514221.3517901},
  urldate = {2023-10-03},
  abstract = {Session-based recommendation predicts the next item with which a user will interact, given a sequence of her past interactions with other items. This machine learning problem targets a core scenario in e-commerce platforms, which aim to recommend interesting items to buy to users browsing the site. Session-based recommenders are difficult to scale due to their exponentially large input space of potential sessions. This impedes offline precomputation of the recommendations, and implies the necessity to maintain state during the online computation of next-item recommendations.},
  isbn = {978-1-4503-9249-5},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BD6XGA8I\modds003.pdf}
}

@article{kestner_fast_2020,
  title = {Fast {{Algorithm}} for Computing a Matrix Transform Used to Detect Trends in Noisy Data},
  author = {Kestner, D. J. and Ierley, G. R. and Kostinski, A. B.},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.09607 [physics]},
  eprint = {2001.09607},
  primaryclass = {physics},
  urldate = {2020-02-19},
  abstract = {A recently discovered universal rank-based matrix method to extract trends from noisy time series is described in [1] but the formula for the output matrix elements, implemented there as an open-access supplement MATLAB computer code, is \$\{\textbackslash cal O\}(N\^4)\$, with \$N\$ the matrix dimension. This can become prohibitively large for time series with hundreds of sample points or more. Based on recurrence relations, here we derive a much faster \$\{\textbackslash cal O\}(N\^2)\$ algorithm and provide code implementations in MATLAB and in open-source JULIA. In some cases one has the output matrix and needs to solve an inverse problem to obtain the input matrix. A fast algorithm and code for this companion problem, also based on the above recurrence relations, are given. Finally, in the narrower, but common, domains of (i) trend detection and (ii) parameter estimation of a linear trend, users require, not the individual matrix elements, but simply their accumulated mean value. For this latter case we provide a yet faster \$\{\textbackslash cal O\}(N)\$ heuristic approximation that relies on a series of rank one matrices. These algorithms are illustrated on a time series of high energy cosmic rays with \$N {$>$} 4 \textbackslash times 10\^4\$. [1] Universal Rank-Order Transform to Extract Signals from Noisy Data, Glenn Ierley and Alex Kostinski, Phys. Rev. X 9 031039 (2019).},
  archiveprefix = {arxiv},
  keywords = {{Physics - Data Analysis, Statistics and Probability}},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4SHUYSQN\\Kestner et al. - 2020 - Fast Algorithm for computing a matrix transform us.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\E4UMVB3I\\2001.html}
}

@article{khanna_economy_2020,
  title = {Economy {{Statistical Recurrent Units For Inferring Nonlinear Granger Causality}}},
  author = {Khanna, Saurabh and Tan, Vincent Y. F.},
  year = {2020},
  month = jan,
  journal = {arXiv:1911.09879 [cs, eess, stat]},
  eprint = {1911.09879},
  primaryclass = {cs, eess, stat},
  urldate = {2020-02-27},
  abstract = {Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes\$'\$ time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XYD6G22U\\Khanna and Tan - 2020 - Economy Statistical Recurrent Units For Inferring .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\N789AJDV\\1911.html}
}

@article{khuong_array_2017,
  title = {Array {{Layouts}} for {{Comparison-Based Searching}}},
  author = {Khuong, Paul-Virak and Morin, Pat},
  year = {2017},
  month = dec,
  journal = {ACM Journal of Experimental Algorithmics},
  volume = {22},
  pages = {1--39},
  issn = {1084-6654, 1084-6654},
  doi = {10.1145/3053370},
  urldate = {2021-03-02},
  abstract = {We attempt to determine the best order and search algorithm to store n comparable data items in an array, A, of length n so that we can, for any query value, x, quickly find the smallest value in A that is greater than or equal to x. In particular, we consider the important case where there are many such queries to the same array, A, which resides entirely in RAM. In addition to the obvious sorted order/binary search combination we consider the Eytzinger (BFS) layout normally used for heaps, an implicit B-tree layout that generalizes the Eytzinger layout, and the van Emde Boas layout commonly used in the cache-oblivious algorithms literature.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\668KFXT8\Khuong and Morin - 2017 - Array Layouts for Comparison-Based Searching.pdf}
}

@article{kibria_short_2006,
  title = {A Short Review of Multivariate T-Distribution},
  author = {Kibria, B M Golam and Joarder, Anwar},
  year = {2006},
  month = jan,
  journal = {Journal of Statistical Research ISSN},
  volume = {40},
  pages = {256--422},
  abstract = {summary This paper reviews most important properties of a location-scale multivariate t-distribution. A conditional representation of the distribution is exploited to outline moments, characteristic function, marginal and conditional distributions, distribution of linear combinations and quadratic forms. Stochastic representa-tion is also used to determine the covariance matrix of the distribution. It also makes an attempt to justify an uncorrelated t-model and overviews distribution of the sum of products matrix and correlation matrix. Estimation strategies for parameters of the model is briefly discussed. Finally the recent trend of linear regression with the uncorrelated t-model is discussed.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\NILV9ENR\Kibria and Joarder - 2006 - A short review of multivariate t-distribution.pdf}
}

@inproceedings{kim_decision_2015,
  title = {A {{Decision Tree Framework}} for {{Spatiotemporal Sequence Prediction}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '15},
  author = {Kim, Taehwan and Yue, Yisong and Taylor, Sarah and Matthews, Iain},
  year = {2015},
  pages = {577--586},
  publisher = {{ACM Press}},
  address = {{Sydney, NSW, Australia}},
  doi = {10.1145/2783258.2783356},
  urldate = {2019-08-30},
  abstract = {We study the problem of learning to predict a spatiotemporal output sequence given an input sequence. In contrast to conventional sequence prediction problems such as part-of-speech tagging (where output sequences are selected using a relatively small set of discrete labels), our goal is to predict sequences that lie within a highdimensional continuous output space. We present a decision tree framework for learning an accurate non-parametric spatiotemporal sequence predictor. Our approach enjoys several attractive properties, including ease of training, fast performance at test time, and the ability to robustly tolerate corrupted training data using a novel latent variable approach. We evaluate on several datasets, and demonstrate substantial improvements over existing decision tree based sequence learning frameworks such as SEARN and DAgger.},
  isbn = {978-1-4503-3664-2},
  langid = {english},
  annotation = {00027},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\9XE6TI4L\Kim et al. - 2015 - A Decision Tree Framework for Spatiotemporal Seque.pdf}
}

@article{kim_financial_2019,
  title = {Financial Series Prediction Using {{Attention LSTM}}},
  author = {Kim, Sangyeon and Kang, Myungjoo},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.10877 [cs, q-fin, stat]},
  eprint = {1902.10877},
  primaryclass = {cs, q-fin, stat},
  urldate = {2019-03-19},
  abstract = {Financial time series prediction, especially with machine learning techniques, is an extensive field of study. In recent times, deep learning methods (especially time series analysis) have performed outstandingly for various industrial problems, with better prediction than machine learning methods. Moreover, many researchers have used deep learning methods to predict financial time series with various models in recent years. In this paper, we will compare various deep learning models, such as multilayer perceptron (MLP), one-dimensional convolutional neural networks (1D CNN), stacked long short-term memory (stacked LSTM), attention networks, and weighted attention networks for financial time series prediction. In particular, attention LSTM is not only used for prediction, but also for visualizing intermediate outputs to analyze the reason of prediction; therefore, we will show an example for understanding the model prediction intuitively with attention vectors. In addition, we focus on time and factors, which lead to an easy understanding of why certain trends are predicted when accessing a given time series table. We also modify the loss functions of the attention models with weighted categorical cross entropy; our proposed model produces a 0.76 hit ratio, which is superior to those of other methods for predicting the trends of the KOSPI 200.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Finance - Statistical Finance,read - not in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NUQ3PA7F\\Kim and Kang - 2019 - Financial series prediction using Attention LSTM.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MJXL8TRL\\1902.html}
}

@inproceedings{kim_predictive_2020,
  title = {Predictive Inference Is Free with the Jackknife+-after-Bootstrap},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kim, Byol and Xu, Chen and Barber, Rina},
  year = {2020},
  volume = {33},
  pages = {4138--4149},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-07},
  abstract = {Ensemble learning is widely used in applications to make predictions in complex decision problems---for example, averaging models fitted to a sequence of samples bootstrapped from the available training data. While such methods offer more accurate, stable, and robust predictions and model estimates, much less is known about how to perform valid, assumption-lean inference on the output of these types of procedures. In this paper, we propose the jackknife+-after-bootstrap (J+aB), a procedure for constructing a predictive interval, which uses only the available bootstrapped samples and their corresponding fitted models, and is therefore "free" in terms of the cost of model fitting. The J+aB offers a predictive coverage guarantee that holds with no assumptions on the distribution of the data, the nature of the fitted model, or the way in which the ensemble of models are aggregated---at worst, the failure rate of the predictive interval is inflated by a factor of 2. Our numerical experiments verify the coverage and accuracy of the resulting predictive intervals on real data.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\NY2KEMQG\Kim et al. - 2020 - Predictive inference is free with the jackknife+-a.pdf}
}

@article{kingma_adam_2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  month = dec,
  journal = {Proceedings of the 3rd International Conference on Learning Representations},
  eprint = {1412.6980},
  urldate = {2019-03-19},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@article{kingma_autoencoding_2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  month = dec,
  journal = {arXiv:1312.6114 [cs, stat]},
  eprint = {1312.6114},
  primaryclass = {cs, stat},
  urldate = {2019-04-12},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RBBH4XHD\Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf}
}

@article{kirshner_learning_,
  title = {Learning with {{Tree-Averaged Densities}} and {{Distributions}}},
  author = {Kirshner, Sergey},
  pages = {8},
  abstract = {We utilize the ensemble of trees framework, a tractable mixture over superexponential number of tree-structured distributions [1], to develop a new model for multivariate density estimation. The model is based on a construction of treestructured copulas \textendash{} multivariate distributions with uniform on [0, 1] marginals. By averaging over all possible tree structures, the new model can approximate distributions with complex variable dependencies. We propose an EM algorithm to estimate the parameters for these tree-averaged models for both the real-valued and the categorical case. Based on the tree-averaged framework, we propose a new model for joint precipitation amounts data on networks of rain stations.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MDR9FXI4\Kirshner - Learning with Tree-Averaged Densities and Distribu.pdf}
}

@inproceedings{kitaev_reformer_2019,
  title = {Reformer: {{The Efficient Transformer}}},
  shorttitle = {Reformer},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Kitaev, Nikita and Kaiser, Lukasz and Levskaya, Anselm},
  year = {2019},
  month = sep,
  urldate = {2020-02-04},
  abstract = {Large Transformer models routinely achieve state-of-the-art results on   a number of tasks but training these models can be prohibitively costly,   especially on long sequences. We introduce two...},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RYF8DUFB\\Kitaev et al. - 2019 - Reformer The Efficient Transformer.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\S94RMRDD\\forum.html}
}

@article{kitaev_reformer_2020,
  title = {Reformer: {{The Efficient Transformer}}},
  shorttitle = {Reformer},
  author = {Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.04451 [cs, stat]},
  eprint = {2001.04451},
  primaryclass = {cs, stat},
  urldate = {2020-01-27},
  abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(\$L\^2\$) to O(\$L\textbackslash log L\$), where \$L\$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of \$N\$ times, where \$N\$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7UDIRE5D\\Kitaev et al. - 2020 - Reformer The Efficient Transformer.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IAHMRTNR\\2001.html}
}

@article{kitiashvili_effects_2020,
  title = {Effects of {{Observational Data Shortage}} on {{Accuracy}} of {{Global Solar Activity Forecast}}},
  author = {Kitiashvili, Irina N.},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.09376 [astro-ph, physics:nlin]},
  eprint = {2001.09376},
  primaryclass = {astro-ph, physics:nlin},
  urldate = {2020-02-19},
  abstract = {Building a reliable forecast of solar activity is a long-standing problem that requires to accurately describe past and current global dynamics. However, synoptic observations of magnetic fields and subsurface flows became available relatively recently. In this paper, we present an investigation of effects of short observational data series on accuracy of solar cycle prediction. This analysis is performed using the annual sunspot number time-series applied to the ParkerKleeorin-Ruzmaikin dynamo model and employing the Ensemble Kalman Filter (EnKF) data assimilation method. The testing of the cycle prediction accuracy is performed for the last six cycles (from Solar Cycle 19 to 24) by sequentially shortening the observational data series that are used for prediction of a target cycle, and evaluating the prediction accuracy according to specified criteria. According to the analysis, reliable activity predictions can be made using relatively short time-series of the sunspot number. It demonstrated that even two cycles of available observations allow us to obtain reasonable forecasts.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Solar and Stellar Astrophysics,Nonlinear Sciences - Chaotic Dynamics},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\EN2Z7F6G\Kitiashvili - 2020 - Effects of Observational Data Shortage on Accuracy.pdf}
}

@article{klambauer_selfnormalizing_2017,
  title = {Self-{{Normalizing Neural Networks}}},
  author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.02515 [cs, stat]},
  eprint = {1706.02515},
  primaryclass = {cs, stat},
  urldate = {2019-04-02},
  abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are ``scaled exponential linear units'' (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance \textemdash{} even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZZJB5YP2\Klambauer et al. - 2017 - Self-Normalizing Neural Networks.pdf}
}

@inproceedings{klein_opennmt_2017,
  title = {{{OpenNMT}}: {{Open-Source Toolkit}} for {{Neural Machine Translation}}},
  shorttitle = {{{OpenNMT}}},
  booktitle = {Proceedings of {{ACL}} 2017, {{System Demonstrations}}},
  author = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander},
  year = {2017},
  pages = {67--72},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-4012},
  urldate = {2019-04-11},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\AKUFJ6T2\Klein et al. - 2017 - OpenNMT Open-Source Toolkit for Neural Machine Tr.pdf}
}

@article{klusowski_analyzing_2019,
  title = {Analyzing {{CART}}},
  author = {Klusowski, Jason M.},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.10086 [cs, stat]},
  eprint = {1906.10086},
  primaryclass = {cs, stat},
  urldate = {2019-09-05},
  abstract = {Decision trees with binary splits are popularly constructed using Classification and Regression Trees (CART) methodology. For binary classification and regression models, this approach recursively divides the data into two near-homogenous daughter nodes according to a split point that maximizes the reduction in sum of squares error (the impurity) along a particular variable. This paper aims to study the bias and adaptive properties of regression trees constructed with CART. In doing so, we derive an interesting connection between the bias and the mean decrease in impurity (MDI) measure of variable importance\textemdash a tool widely used for model interpretability\textemdash defined as the sum of impurity reductions over all non-terminal nodes in the tree. In particular, we show that the probability content of a terminal subnode for a variable is small when the MDI for that variable is large and that this relationship is exponential\textemdash confirming theoretically that decision trees with CART have small bias and are adaptive to signal strength and direction. Finally, we apply these individual tree bounds to tree ensembles and show consistency of Breiman's random forests. The context is surprisingly general and applies to a wide variety of multivariable data generating distributions and regression functions. The main technical tool is an exact characterization of the conditional probability content of the daughter nodes arising from an optimal split, in terms of the partial dependence function and reduction in impurity.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\H6B8E9PA\Klusowski - 2019 - Analyzing CART.pdf}
}

@article{koch_siamese_,
  title = {Siamese {{Neural Networks}} for {{One-Shot Image Recognition}}},
  author = {Koch, Gregory},
  pages = {30},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LNXRN7RV\Koch - Siamese Neural Networks for One-Shot Image Recogni.pdf}
}

@article{kong_deep_2018,
  title = {A {{Deep Neural Network Model}} Using {{Random Forest}} to {{Extract Feature Representation}} for {{Gene Expression Data Classification}}},
  author = {Kong, Yunchuan and Yu, Tianwei},
  year = {2018},
  month = nov,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {1--9},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-34833-6},
  urldate = {2019-09-04},
  abstract = {In predictive model development, gene expression data is associated with the unique challenge that the number of samples (n) is much smaller than the amount of features (p). This ``n\,{$\ll$}\,p'' property has prevented classification of gene expression data from deep learning techniques, which have been proved powerful under ``n\,{$>$}\,p'' scenarios in other application fields, such as image classification. Further, the sparsity of effective features with unknown correlation structures in gene expression profiles brings more challenges for classification tasks. To tackle these problems, we propose a newly developed classifier named Forest Deep Neural Network (fDNN), to integrate the deep neural network architecture with a supervised forest feature detector. Using this built-in feature detector, the method is able to learn sparse feature representations and feed the representations into a neural network to mitigate the overfitting problem. Simulation experiments and real data analyses using two RNA-seq expression datasets are conducted to evaluate fDNN's capability. The method is demonstrated a useful addition to current predictive models with better classification performance and more meaningful selected features compared to ordinary random forests and deep neural networks.},
  copyright = {2018 The Author(s)},
  langid = {english},
  annotation = {00003},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DQ94RWBJ\\Kong and Yu - 2018 - A Deep Neural Network Model using Random Forest to.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PV32GPL6\\s41598-018-34833-6.html}
}

@inproceedings{kontschieder_deep_2015,
  title = {Deep {{Neural Decision Forests}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Kontschieder, Peter and Fiterau, Madalina and Criminisi, Antonio and Bulo, Samuel Rota},
  year = {2015},
  month = dec,
  pages = {1467--1475},
  publisher = {{IEEE}},
  address = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.172},
  urldate = {2019-08-30},
  abstract = {We present Deep Neural Decision Forests \textendash{} a novel approach that unifies classification trees with the representation learning functionality known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find onpar or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84\%/6.38\% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67\% error obtained by the best GoogLeNet architecture (7 models, 144 crops).},
  isbn = {978-1-4673-8391-2},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\EXTL2FAJ\Kontschieder et al. - 2015 - Deep Neural Decision Forests.pdf}
}

@article{koochali_if_2021,
  title = {If {{You Like It}}, {{GAN It}}\textemdash{{Probabilistic Multivariate Times Series Forecast}} with {{GAN}}},
  author = {Koochali, Alireza and Dengel, Andreas and Ahmed, Sheraz},
  year = {2021},
  journal = {Engineering Proceedings},
  volume = {5},
  number = {1},
  pages = {40},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2673-4591},
  doi = {10.3390/engproc2021005040},
  urldate = {2023-03-20},
  abstract = {The contribution of this paper is two-fold. First, we present ProbCast\textemdash a novel probabilistic model for multivariate time-series forecasting. We employ a conditional GAN framework to train our model with adversarial training. Second, we propose a framework that lets us transform a deterministic model into a probabilistic one with improved performance. The motivation of the framework is to either transform existing highly accurate point forecast models to their probabilistic counterparts or to train GANs stably by selecting the architecture of GAN's component carefully and efficiently. We conduct experiments over two publicly available datasets\textemdash an electricity consumption dataset and an exchange-rate dataset. The results of the experiments demonstrate the remarkable performance of our model as well as the successful application of our proposed framework.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {forecasting,generative adversarial networks,prediction,probabilistic,time-series},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PJ4ZGLIJ\Koochali et al. - 2021 - If You Like It, GAN Itâ€”Probabilistic Multivariate .pdf}
}

@incollection{koolen_minimax_2015,
  title = {Minimax {{Time Series Prediction}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Koolen, Wouter M and Malek, Alan and Bartlett, Peter L and Abbasi, Yasin},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {2557--2565},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  keywords = {read - in related work},
  annotation = {00013},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CLXU9WUC\\Koolen et al. - 2015 - Minimax Time Series Prediction.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IFUY7XPM\\5730-minimax-time-series-prediction.html}
}

@article{koopman_state_,
  title = {State {{Space Time Series Analysis}}},
  author = {Koopman, Siem Jan},
  pages = {39},
  langid = {english},
  annotation = {03525},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LC83LDVX\Koopman - State Space Time Series Analysis.pdf}
}

@inproceedings{korkmaz_combining_2015,
  title = {Combining {{Heterogeneous Data Sources}} for {{Civil Unrest Forecasting}}},
  booktitle = {Proceedings of the 2015 {{IEEE}}/{{ACM International Conference}} on {{Advances}} in {{Social Networks Analysis}} and {{Mining}} 2015},
  author = {Korkmaz, Gizem and Cadena, Jose and Kuhlman, Chris J. and Marathe, Achla and Vullikanti, Anil and Ramakrishnan, Naren},
  year = {2015},
  series = {{{ASONAM}} '15},
  pages = {258--265},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2808797.2808847},
  urldate = {2019-04-16},
  abstract = {Detecting and forecasting civil unrest events (protests, strikes, etc.) is of key interest to social scientists and policy makers because these events can lead to significant societal and cultural changes. We analyze protest dynamics in six countries of Latin America on a daily level, from November 2012 through August 2014, using multiple data sources that capture social, political and economic contexts within which civil unrest occurs. We use logistic regression models with Lasso to select a sparse feature set from our diverse datasets, in order to predict the probability of occurrence of civil unrest events in these countries. The models contain predictors extracted from social media sites (Twitter and blogs) and news sources, in addition to volume of requests to Tor, a widely-used anonymity network. Two political event databases and country-specific exchange rates are also used. Our forecasting models are evaluated using a Gold Standard Report (GSR), which is compiled by an independent group of social scientists and experts on Latin America. The experimental results, measured by F1-scores, are in the range 0.68 to 0.95, and demonstrate the efficacy of using a multi-source approach for predicting civil unrest. Case studies illustrate the insights into unrest events that are obtained with our methods.},
  isbn = {978-1-4503-3854-7},
  keywords = {read - not in related work},
  annotation = {00000}
}

@article{kosiorowski_generalized_2018,
  title = {Generalized {{Exponential}} Smoothing in Prediction of Hierarchical Time Series},
  author = {Kosiorowski, Daniel and Mielczarek, Dominik and Rydlewski, Jerzy P. and Snarska, Ma{\l}gorzata},
  year = {2018},
  journal = {Statistics in Transition New Series},
  volume = {19},
  number = {2},
  eprint = {1612.02195},
  primaryclass = {stat},
  pages = {331--350},
  issn = {1234-7655, 2450-0291},
  doi = {10.21307/stattrans-2018-019},
  urldate = {2022-05-25},
  abstract = {Shang and Hyndman (2017) proposed a grouped functional time series forecasting approach as a combination of individual forecasts obtained using generalized least squares method. We modify their methodology using generalized exponential smoothing technique for the most disaggregated functional time series in order to obtain more robust predictor. We discuss some properties of our proposals basing on results obtained via simulation studies and analysis of real data related to a prediction of a demand for electricity in Australia in 2016.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\JXPVSGSG\\Kosiorowski et al. - 2018 - Generalized Exponential smoothing in prediction of.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\G6PPUPFH\\1612.html}
}

@article{kourentzes_elucidate_2021,
  title = {Elucidate Structure in Intermittent Demand Series},
  author = {Kourentzes, Nikolaos and Athanasopoulos, George},
  year = {2021},
  month = jan,
  journal = {European Journal of Operational Research},
  volume = {288},
  number = {1},
  pages = {141--152},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2020.05.046},
  urldate = {2022-05-25},
  abstract = {Intermittent demand forecasting has been widely researched in the context of spare parts management. However, it is becoming increasingly relevant to many other areas, such as retailing, where at the very disaggregate level time series may be highly intermittent, but at more aggregate levels are likely to exhibit trends and seasonal patterns. The vast majority of intermittent demand forecasting methods are inappropriate for producing forecasts with such features. We propose using temporal hierarchies to produce forecasts that demonstrate these traits at the various aggregation levels, effectively informing the resulting intermittent forecasts of these patterns that are identifiable only at higher levels. We conduct an empirical evaluation on real data and demonstrate statistically significant gains for both point and quantile forecasts.},
  langid = {english},
  keywords = {Forecast combination,Forecast reconciliation,Forecasting,Temporal aggregation,Temporal hierarchies},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3VLJ5Y6H\\Kourentzes and Athanasopoulos - 2021 - Elucidate structure in intermittent demand series.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FINJAEN7\\S0377221720304926.html}
}

@inproceedings{kraska_case_2018,
  title = {The {{Case}} for {{Learned Index Structures}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
  year = {2018},
  month = may,
  pages = {489--504},
  publisher = {{ACM}},
  address = {{Houston TX USA}},
  doi = {10.1145/3183713.3196909},
  urldate = {2021-05-18},
  abstract = {Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show that our learned indexes can have significant advantages over traditional indexes. More importantly, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work provides just a glimpse of what might be possible.},
  isbn = {978-1-4503-4703-7},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\AL2RPEEL\Kraska et al. - 2018 - The Case for Learned Index Structures.pdf}
}

@article{krauss_deep_2017,
  title = {Deep Neural Networks, Gradient-Boosted Trees, Random Forests: {{Statistical}} Arbitrage on the {{S}}\&{{P}} 500},
  shorttitle = {Deep Neural Networks, Gradient-Boosted Trees, Random Forests},
  author = {Krauss, Christopher and Do, Xuan Anh and Huck, Nicolas},
  year = {2017},
  month = jun,
  journal = {European Journal of Operational Research},
  volume = {259},
  number = {2},
  pages = {689--702},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2016.10.031},
  urldate = {2019-04-26},
  abstract = {In recent years, machine learning research has gained momentum: new developments in the field of deep learning allow for multiple levels of abstraction and are starting to supersede well-known and powerful tree-based techniques mainly operating on the original feature space. All these methods can be applied to various fields, including finance. This paper implements and analyzes the effectiveness of deep neural networks (DNN), gradient-boosted-trees (GBT), random forests (RAF), and several ensembles of these methods in the context of statistical arbitrage. Each model is trained on lagged returns of all stocks in the S\&P 500, after elimination of survivor bias. From 1992 to 2015, daily one-day-ahead trading signals are generated based on the probability forecast of a stock to outperform the general market. The highest k probabilities are converted into long and the lowest k probabilities into short positions, thus censoring the less certain middle part of the ranking. Empirical findings are promising. A simple, equal-weighted ensemble (ENS1) consisting of one deep neural network, one gradient-boosted tree, and one random forest produces out-of-sample returns exceeding 0.45 percent per day for k=10, prior to transaction costs. Irrespective of the fact that profits are declining in recent years, our findings pose a severe challenge to the semi-strong form of market efficiency.},
  keywords = {Deep learning,Ensemble learning,Finance,Gradient-boosting,Random forests,read - not in related work},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FQZ969IA\\Krauss et al. - 2017 - Deep neural networks, gradient-boosted trees, rand.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YV2FH8DX\\S0377221716308657.html}
}

@article{krishnan_deep_2015,
  title = {Deep {{Kalman Filters}}},
  author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.05121 [cs, stat]},
  eprint = {1511.05121},
  primaryclass = {cs, stat},
  urldate = {2019-04-15},
  abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the ``Healing MNIST'' dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read - not in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3HQLB7X2\Krishnan et al. - 2015 - Deep Kalman Filters.pdf}
}

@inproceedings{krishnan_structured_2017,
  title = {Structured {{Inference Networks}} for {{Nonlinear State Space Models}}},
  booktitle = {Proceedings of the {{Thirty-First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
  year = {2017},
  series = {{{AAAI}}'17},
  pages = {2101--2109},
  publisher = {{AAAI Press}},
  address = {San Francisco, California, USA},
  urldate = {2019-05-10},
  abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.},
  annotation = {00086},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\GPXLA5AH\Krishnan et al. - 2016 - Structured Inference Networks for Nonlinear State .pdf}
}

@article{krizhevsky_imagenet_2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {00010782},
  doi = {10.1145/3065386},
  urldate = {2019-03-19},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5EFNVUJ8\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@inproceedings{krstanovic_fourierbased_2019,
  title = {Fourier-{{Based Parametrization}} of {{Convolutional Neural Networks}} for {{Robust Time Series Forecasting}}},
  booktitle = {Discovery {{Science}}},
  author = {Krstanovic, Sascha and Paulheim, Heiko},
  editor = {Kralj Novak, Petra and {\v S}muc, Tomislav and D{\v z}eroski, Sa{\v s}o},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {522--532},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-33778-0_39},
  abstract = {Classical statistical models for time series forecasting most often make a number of assumptions about the data at hand, therewith, requiring intensive manual preprocessing steps prior to modeling. As a consequence, it is very challenging to come up with a more generic forecasting framework. Extensive hyperparameter optimization and ensemble architectures are common strategies to tackle this problem, however, this comes at the cost of high computational complexity. Instead of optimizing hyperparameters by training multiple models, we propose a method to estimate optimal hyperparameters directly from the characteristics of the time series at hand. To that end, we use Convolutional Neural Networks (CNNs) for time series forecasting and determine a part of the network layout based on the time series' Fourier coefficients. Our approach significantly reduces the amount of required model configuration time and shows competitive performance on time series data across various domains. A comparison to popular, state of the art forecasting algorithms reveals further improvements in runtime and practicability.},
  isbn = {978-3-030-33778-0},
  langid = {english},
  keywords = {Fourier analysis,Neural networks,Time series forecasting}
}

@article{krueger_regularizing_2015,
  title = {Regularizing {{RNNs}} by {{Stabilizing Activations}}},
  author = {Krueger, David and Memisevic, Roland},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.08400 [cs, stat]},
  eprint = {1511.08400},
  primaryclass = {cs, stat},
  urldate = {2019-04-02},
  abstract = {We stabilize the activations of Recurrent Neural Networks (RNNs) by penalizing the squared distance between successive hidden states' norms. This penalty term is an effective regularizer for RNNs including LSTMs and IRNNs, improving performance on character-level language modeling and phoneme recognition, and outperforming weight noise and dropout. We achieve competitive performance (18.6\% PER) on the TIMIT phoneme recognition task for RNNs evaluated without beam search or an RNN transducer. With this penalty term, IRNN can achieve similar performance to LSTM on language modeling, although adding the penalty term to the LSTM results in superior performance. Our penalty term also prevents the exponential growth of IRNN's activations outside of their training horizon, allowing them to generalize to much longer sequences.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QCEIF34U\Krueger and Memisevic - 2015 - Regularizing RNNs by Stabilizing Activations.pdf}
}

@article{krueger_zoneout_2017,
  title = {Zoneout: {{Regularizing RNNs}} by {{Randomly Preserving Hidden Activations}}},
  shorttitle = {Zoneout},
  author = {Krueger, David and Maharaj, Tegan and Kram{\'a}r, J{\'a}nos and Pezeshki, Mohammad and Ballas, Nicolas and Ke, Nan Rosemary and Goyal, Anirudh and Bengio, Yoshua and Courville, Aaron and Pal, Chris},
  year = {2017},
  month = sep,
  journal = {arXiv:1606.01305 [cs]},
  eprint = {1606.01305},
  primaryclass = {cs},
  urldate = {2019-12-17},
  abstract = {We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization (Cooijmans et al., 2016) yields state-of-the-art results on permuted sequential MNIST.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {00164},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QRD4VXXS\Krueger et al. - 2017 - Zoneout Regularizing RNNs by Randomly Preserving .pdf}
}

@article{kumar_fully_2022,
  title = {From {{Fully Trained}} to {{Fully Random Embeddings}}: {{Improving Neural Machine Translation}} with {{Compact Word Embedding Tables}}},
  shorttitle = {From {{Fully Trained}} to {{Fully Random Embeddings}}},
  author = {Kumar, Krtin and Passban, Peyman and Rezagholizadeh, Mehdi and Lau, Yiusing and Liu, Qun},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {10},
  pages = {10930--10937},
  issn = {2374-3468},
  doi = {10.1609/aaai.v36i10.21340},
  urldate = {2023-05-05},
  abstract = {Embedding matrices are key components in neural natural language processing (NLP) models that are responsible to provide numerical representations of input tokens (i.e. words or subwords). In this paper, we analyze the impact and utility of such matrices in the context of neural machine translation (NMT). We show that detracting syntactic and semantic information from word embeddings and running NMT systems with random embeddings is not as damaging as it initially sounds. We also show how incorporating only a limited amount of task-specific knowledge from fully-trained embeddings can boost the performance NMT systems. Our findings demonstrate that in exchange for negligible deterioration in performance, any NMT model can be run with partially random embeddings. Working with such structures means a minimal memory requirement as there is no longer need to store large embedding tables, which is a significant gain in industrial and on-device settings. We evaluated our embeddings in translating English into German and French and achieved a 5.3x compression rate. Despite having a considerably smaller architecture, our models in some cases are even able to outperform state-of-the-art baselines.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Speech \& Natural Language Processing (SNLP)},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TEVQAGHE\Kumar et al. - 2022 - From Fully Trained to Fully Random Embeddings Imp.pdf}
}

@misc{kunz_deep_2023,
  title = {Deep {{Learning}} Based {{Forecasting}}: A Case Study from the Online Fashion Industry},
  shorttitle = {Deep {{Learning}} Based {{Forecasting}}},
  author = {Kunz, Manuel and Birr, Stefan and Raslan, Mones and Ma, Lei and Li, Zhen and Gouttes, Adele and Koren, Mateusz and Naghibi, Tofigh and Stephan, Johannes and Bulycheva, Mariia and Grzeschik, Matthias and Keki{\'c}, Armin and Narodovitch, Michael and Rasul, Kashif and Sieber, Julian and Januschowski, Tim},
  year = {2023},
  month = may,
  number = {arXiv:2305.14406},
  eprint = {2305.14406},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.14406},
  urldate = {2023-06-08},
  abstract = {Demand forecasting in the online fashion industry is particularly amendable to global, data-driven forecasting models because of the industry's set of particular challenges. These include the volume of data, the irregularity, the high amount of turn-over in the catalog and the fixed inventory assumption. While standard deep learning forecasting approaches cater for many of these, the fixed inventory assumption requires a special treatment via controlling the relationship between price and demand closely. In this case study, we describe the data and our modelling approach for this forecasting problem in detail and present empirical results that highlight the effectiveness of our approach.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6GFPZYNV\\Kunz et al. - 2023 - Deep Learning based Forecasting a case study from.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4X792ZES\\2305.html}
}

@article{kunzel_linear_2019,
  title = {Linear {{Aggregation}} in {{Tree-based Estimators}}},
  author = {K{\"u}nzel, S{\"o}ren R. and Saarinen, Theo F. and Liu, Edward W. and Sekhon, Jasjeet S.},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.06463 [stat]},
  eprint = {1906.06463},
  primaryclass = {stat},
  urldate = {2019-09-05},
  abstract = {Regression trees and their ensemble methods are popular methods for non-parametric regression \textemdash{} combining strong predictive performance with interpretable estimators. In order to improve their utility for smooth response surfaces, we study regression trees and random forests with linear aggregation functions. We introduce a new algorithm which finds the best axis-aligned split to fit optimal linear aggregation functions on the corresponding nodes and implement this method in the provably fastest way. This algorithm enables us to create more interpretable trees and obtain better predictive performance on a wide range of data sets. We also provide a software package that implements our algorithm. Applying the algorithm to several real-world data sets, we showcase its favorable performance in an extensive simulation study in terms of EMSE and demonstrate the improved interpretability of resulting estimators on a large real-world data set.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\B4FG7V53\KÃ¼nzel et al. - 2019 - Linear Aggregation in Tree-based Estimators.pdf}
}

@article{kuremoto_time_2014,
  title = {Time Series Forecasting Using a Deep Belief Network with Restricted {{Boltzmann}} Machines},
  author = {Kuremoto, Takashi and Kimura, Shinsuke and Kobayashi, Kunikazu and Obayashi, Masanao},
  year = {2014},
  month = aug,
  journal = {Neurocomputing},
  series = {Advanced {{Intelligent Computing Theories}} and {{Methodologies}}},
  volume = {137},
  pages = {47--56},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2013.03.047},
  urldate = {2020-02-17},
  abstract = {Multi-layer perceptron (MLP) and other artificial neural networks (ANNs) have been widely applied to time series forecasting since 1980s. However, for some problems such as initialization and local optima existing in applications, the improvement of ANNs is, and still will be the most interesting study for not only time series forecasting but also other intelligent computing fields. In this study, we propose a method for time series prediction using Hinton and Salakhutdinov×³s deep belief nets (DBN) which are probabilistic generative neural network composed by multiple layers of restricted Boltzmann machine (RBM). We use a 3-layer deep network of RBMs to capture the feature of input space of time series data, and after pretraining of RBMs using their energy functions, gradient descent training, i.e., back-propagation learning algorithm is used for fine-tuning connection weights between ``visible layers'' and ``hidden layers'' of RBMs. To decide the sizes of neural networks and the learning rates, Kennedy and Eberhart×³s particle swarm optimization (PSO) is adopted during the training processes. Furthermore, ``trend removal'', a preprocessing to the original data, is also approached in the forecasting experiment using CATS benchmark data. Additionally, approximating and short-term prediction of chaotic time series such as Lorenz chaos and logistic map were also applied by the proposed method.},
  langid = {english},
  keywords = {CATS benchmark,Chaos,Deep belief nets,Multi-layer perceptron,Restricted Boltzmann machine,Time series forecasting},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\97PUZ2X3\\Kuremoto et al. - 2014 - Time series forecasting using a deep belief networ.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HK3EHBL3\\S0925231213007388.html}
}

@article{kuremoto_training_2019,
  title = {Training {{Deep Neural Networks}} with {{Reinforcement Learning}} for {{Time Series Forecasting}}},
  author = {Kuremoto, Takashi and Hirata, Takaomi and Obayashi, Masanao and Mabu, Shingo and Kobayashi, Kunikazu},
  year = {2019},
  month = apr,
  journal = {Time Series Analysis - Data, Methods, and Applications},
  doi = {10.5772/intechopen.85457},
  urldate = {2020-02-17},
  abstract = {As a kind of efficient nonlinear function approximators, artificial neural networks (ANN) have been popularly applied to time series forecasting. The training method of ANN usually utilizes error back-propagation (BP) which is a supervised learning algorithm proposed by Rumelhart et al. in 1986; meanwhile, authors proposed to improve the robustness of the ANN for unknown time series prediction using a reinforcement learning algorithm named stochastic gradient ascent (SGA) originally proposed by Kimura and Kobayashi for control problems in 1998. We also successfully use a deep belief net (DBN) stacked by multiple restricted Boltzmann machines (RBMs) to realized time series forecasting in 2012. In this chapter, a state-of-the-art time series forecasting system that combines RBMs and multilayer perceptron (MLP) and uses SGA training algorithm is introduced. Experiment results showed the high prediction precision of the novel system not only for benchmark data but also for real phenomenon time series data.},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Q6PNMZ2G\\Kuremoto et al. - 2019 - Training Deep Neural Networks with Reinforcement L.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZJE8K4RZ\\training-deep-neural-networks-with-reinforcement-learning-for-time-series-forecasting.html}
}

@incollection{kuznetsov_discriminative_2017,
  title = {Discriminative {{State Space Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Kuznetsov, Vitaly and Mohri, Mehryar},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5671--5679},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\P7UGT8LF\\Kuznetsov and Mohri - 2017 - Discriminative State Space Models.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\K7ZTIBL5\\7150-discriminative-state-space-models.html}
}

@inproceedings{kuznetsov_learning_2015,
  title = {Learning {{Theory}} and {{Algorithms}} for {{Forecasting Non-stationary Time Series}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Kuznetsov, Vitaly and Mohri, Mehryar},
  year = {2015},
  series = {{{NIPS}}'15},
  pages = {541--549},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  urldate = {2019-04-17},
  abstract = {We present data-dependent learning bounds for the general scenario of non-stationary non-mixing stochastic processes. Our learning guarantees are expressed in terms of a data-dependent measure of sequential complexity and a discrepancy measure that can be estimated from data under some mild assumptions. We use our learning bounds to devise new algorithms for non-stationary time series forecasting for which we report some preliminary experimental results.},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6946YQQH\Kuznetsov and Mohri - Learning Theory and Algorithms for Forecasting Non.pdf}
}

@inproceedings{laclau_coclustering_2017,
  title = {Co-Clustering {{Through Optimal Transport}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {Laclau, Charlotte and Redko, Ievgen and Matei, Basarab and Bennani, Youn{\`e}s and Brault, Vincent},
  year = {2017},
  series = {{{ICML}}'17},
  pages = {1955--1964},
  publisher = {{JMLR.org}},
  address = {Sydney, NSW, Australia},
  urldate = {2019-09-27},
  abstract = {In this paper, we present a novel method for co-clustering, an unsupervised learning approach that aims at discovering homogeneous groups of data instances and features by grouping them simultaneously. The proposed method uses the entropy regularized optimal transport between empirical measures defined on data instances and features in order to obtain an estimated joint probability density function represented by the optimal coupling matrix. This matrix is further factorized to obtain the induced row and columns partitions using multiscale representations approach. To justify our method theoretically, we show how the solution of the regularized optimal transport can be seen from the variational inference perspective thus motivating its use for co-clustering. The algorithm derived for the proposed method and its kernelized version based on the notion of Gromov-Wasserstein distance are fast, accurate and can determine automatically the number of both row and column clusters. These features are vividly demonstrated through extensive experimental evaluations.},
  annotation = {00007}
}

@article{lago_forecasting_2018,
  title = {Forecasting Spot Electricity Prices: {{Deep}} Learning Approaches and Empirical Comparison of Traditional Algorithms},
  shorttitle = {Forecasting Spot Electricity Prices},
  author = {Lago, Jesus and De Ridder, Fjo and De Schutter, Bart},
  year = {2018},
  month = jul,
  journal = {Applied Energy},
  volume = {221},
  pages = {386--405},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2018.02.069},
  urldate = {2019-05-01},
  abstract = {In this paper, a novel modeling framework for forecasting electricity prices is proposed. While many predictive models have been already proposed to perform this task, the area of deep learning algorithms remains yet unexplored. To fill this scientific gap, we propose four different deep learning models for predicting electricity prices and we show how they lead to improvements in predictive accuracy. In addition, we also consider that, despite the large number of proposed methods for predicting electricity prices, an extensive benchmark is still missing. To tackle that, we compare and analyze the accuracy of 27 common approaches for electricity price forecasting. Based on the benchmark results, we show how the proposed deep learning models outperform the state-of-the-art methods and obtain results that are statistically significant. Finally, using the same results, we also show that: (i) machine learning methods yield, in general, a better accuracy than statistical models; (ii) moving average terms do not improve the predictive accuracy; (iii) hybrid models do not outperform their simpler counterparts.},
  keywords = {Benchmark study,Deep learning,Electricity price forecasting},
  annotation = {00052},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6T5UFNW9\\Lago et al. - 2018 - Forecasting spot electricity prices Deep learning.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\SW2PJLBE\\S030626191830196X.html}
}

@article{lai_modeling_2017,
  title = {Modeling {{Long-}} and {{Short-Term Temporal Patterns}} with {{Deep Neural Networks}}},
  author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.07015 [cs]},
  eprint = {1703.07015},
  primaryclass = {cs},
  urldate = {2019-04-03},
  abstract = {Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) and the Recurrent Neural Network (RNN) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. Furthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. All the data and experiment codes are available online.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read - in related work},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\W8VYN85N\Lai et al. - 2017 - Modeling Long- and Short-Term Temporal Patterns wi.pdf}
}

@inproceedings{lai_modeling_2018,
  title = {Modeling {{Long-}} and {{Short-Term Temporal Patterns}} with {{Deep Neural Networks}}},
  booktitle = {The 41st {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  author = {Lai, Guokun and Chang, Wei-Cheng and Yang, Yiming and Liu, Hanxiao},
  year = {2018},
  month = jun,
  series = {{{SIGIR}} '18},
  pages = {95--104},
  address = {{Ann Arbor, MI, USA}},
  doi = {10.1145/3209978.3210006},
  urldate = {2020-01-17},
  abstract = {Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these real-world applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Long- and Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) and the Recurrent Neural Network (RNN) to extract short-term local dependency patterns among variables and to discover long-term patterns for time series trends. Furthermore, we leverage traditional autoregressive model to tackle the scale insensitive problem of the neural network model. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. All the data and experiment codes are available online.},
  isbn = {978-1-4503-5657-2},
  keywords = {autoregressive models,multivariate time series,neural network},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YLSL4WBD\Lai et al. - 2018 - Modeling Long- and Short-Term Temporal Patterns wi.pdf}
}

@article{lake_humanlevel_2015,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  author = {Lake, B. M. and Salakhutdinov, R. and Tenenbaum, J. B.},
  year = {2015},
  month = dec,
  journal = {Science},
  volume = {350},
  number = {6266},
  pages = {1332--1338},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab3050},
  urldate = {2019-03-19},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LAGBT88V\Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf}
}

@article{lamb_professor_2016,
  title = {Professor {{Forcing}}: {{A New Algorithm}} for {{Training Recurrent Networks}}},
  shorttitle = {Professor {{Forcing}}},
  author = {Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.09038 [cs, stat]},
  eprint = {1610.09038},
  primaryclass = {cs, stat},
  urldate = {2019-04-11},
  abstract = {The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-stepahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\U7PJD6RG\Lamb et al. - 2016 - Professor Forcing A New Algorithm for Training Re.pdf}
}

@article{langkvist_review_2014,
  title = {A Review of Unsupervised Feature Learning and Deep Learning for Time-Series Modeling},
  author = {L{\"a}ngkvist, Martin and Karlsson, Lars and Loutfi, Amy},
  year = {2014},
  month = jun,
  journal = {Pattern Recognition Letters},
  volume = {42},
  pages = {11--24},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2014.01.008},
  urldate = {2019-04-26},
  abstract = {This paper gives a review of the recent developments in deep learning and unsupervised feature learning for time-series problems. While these techniques have shown promise for modeling static data, such as computer vision, applying them to time-series data is gaining increasing attention. This paper overviews the particular challenges present in time-series data and provides a review of the works that have either applied time-series data to unsupervised feature learning algorithms or alternatively have contributed to modifications of feature learning algorithms to take into account the challenges present in time-series data.},
  keywords = {Deep learning,Time-series,Unsupervised feature learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\KH8Y3U3D\\LÃ¤ngkvist et al. - 2014 - A review of unsupervised feature learning and deep.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PTBFI6FF\\S0167865514000221.html}
}

@inproceedings{laptev_generic_2015,
  title = {Generic and {{Scalable Framework}} for {{Automated Time-series Anomaly Detection}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '15},
  author = {Laptev, Nikolay and Amizadeh, Saeed and Flint, Ian},
  year = {2015},
  pages = {1939--1947},
  publisher = {{ACM Press}},
  address = {{Sydney, NSW, Australia}},
  doi = {10.1145/2783258.2788611},
  urldate = {2019-04-17},
  abstract = {This paper introduces a generic and scalable framework for automated anomaly detection on large scale time-series data. Early detection of anomalies plays a key role in maintaining consistency of person's data and protects corporations against malicious attackers. Current state of the art anomaly detection approaches suffer from scalability, use-case restrictions, difficulty of use and a large number of false positives. Our system at Yahoo, EGADS, uses a collection of anomaly detection and forecasting models with an anomaly filtering layer for accurate and scalable anomaly detection on timeseries. We compare our approach against other anomaly detection systems on real and synthetic data with varying time-series characteristics. We found that our framework allows for 50-60\% improvement in precision and recall for a variety of use-cases. Both the data and the framework are being open-sourced. The open-sourcing of the data, in particular, represents the first of its kind effort to establish the standard benchmark for anomaly detection.},
  isbn = {978-1-4503-3664-2},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00078},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RTNYRHBB\Laptev et al. - 2015 - Generic and Scalable Framework for Automated Time-.pdf}
}

@inproceedings{laptev_timeseries_2017,
  title = {Time-Series {{Extreme Event Forecasting}} with {{Neural Networks}} at {{Uber}}},
  booktitle = {{{ICML}} 2017 {{Time Series Workshop}}},
  author = {Laptev, Nikolay and Yosinski, Jason and Li, Erran L. and Smyl, Slawek},
  year = {2017},
  urldate = {2019-04-17},
  abstract = {Accurate time-series forecasting during high variance segments (e.g., holidays), is critical for anomaly detection, optimal resource allocation, budget planning and other related tasks. At Uber accurate prediction for completed trips during special events can lead to a more efficient driver allocation resulting in a decreased wait time for the riders. State of the art methods for handling this task often rely on a combination of univariate forecasting models (e.g., Holt-Winters) and machine learning methods (e.g., random forest). Such a system, however, is hard to tune, scale and add exogenous variables. Motivated by the recent resurgence of Long Short Term Memory networks we propose a novel endto-end recurrent neural network architecture that outperforms the current state of the art event forecasting methods on Uber data and generalizes well to a public M3 dataset used for time-series forecasting competitions.},
  keywords = {Anomaly detection,Biological Neural Networks,Device driver,Handling (Psychology),Independence Day: Resurgence,Long short-term memory,Machine learning,Network architecture,Projections and Predictions,Random forest,read - in related work,Recurrent neural network,Resource Allocation,Silo (dataset),Time series},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LVJLXZD5\Laptev et al. - Time-series Extreme Event Forecasting with Neural .pdf}
}

@phdthesis{larriva_semigreedy_2019a,
  title = {Semi-{{Greedy Construction}} of {{Oblique-Split Decision Trees}}},
  author = {Larriva, Matthew Rudolph},
  year = {2019},
  urldate = {2019-08-30},
  abstract = {Classication and Regression Trees (CART) are a method of structured prediction widely used in machine learning. Favored for their robustness to non-linear relationships and interpretability (James, Witten, Hastie, \&amp; Tibshirani, 2013), they have seen continued application since their broad introduction in 1984 (Breiman, Friedman, Olshen, \&amp; Stone, 1984). The shortcomings of a single-tree model, especially overfitting and lack of competitiveness compared to other machine learning methods, have been overcome through advances in bagging, pruning, and boosting (James et al., 2013). Decision trees are fitted in a top-down, greedy fashion because non-greedy fitting is computationally intractable. While this leads to efficient computation, using top-down, greedy estimates can produce suboptimal models. To overcome this suboptimality I propose a semi-greedy method of decision tree construction. First, I reformulate the problem based on the work of Norouzi et. al. (2015)(Norouzi, Collins, Johnson, Fleet, \&amp; Kohli, 2015), and use an algorithm which optimizes the split functions at all levels of the tree jointly, based on a global objective. This algorithm is parameterized by a single scalar for which I propose a bound. Second, I propose a dimensionality reduction step for cases when the semi-greedy algorithm becomes intractable. In the five datasets tested (Breast Cancer Wisconsin, Banknotes Authentication, Haberman Survival Data, Blood Transfusion Data, Fertility Data), the algorithm produces higher accuracy than two existing greedy tree packages in R (rpart, tree). And, it is computationally tractable for low-width datasets (approximately 8 features or fewer). For higher order problems, I propose a preprocessing step which reduces the higher-width data to a more tractable subset. I show that a dimensionally reduced space, processed using the semi-greedy algorithm, outperforms the greedy methods (rpart, trees) trained with both full and reduced data sets.},
  langid = {english},
  school = {UCLA},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\F4576569\\Larriva - 2019 - Semi-Greedy Construction of Oblique-Split Decision.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EGCR3QEI\\2w3950hh.html}
}

@article{laskin_reinforcement_2020,
  title = {Reinforcement {{Learning}} with {{Augmented Data}}},
  author = {Laskin, Michael and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
  year = {2020},
  month = may,
  journal = {arXiv:2004.14990 [cs, stat]},
  eprint = {2004.14990},
  primaryclass = {cs, stat},
  urldate = {2020-05-25},
  abstract = {Learning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\A3GDHVHI\Laskin et al. - 2020 - Reinforcement Learning with Augmented Data.pdf}
}

@article{lauderdale_modelbased_2020,
  title = {Model-Based Pre-Election Polling for National and Sub-National Outcomes in the {{US}} and {{UK}}},
  author = {Lauderdale, Benjamin E. and Bailey, Delia and Blumenau, Jack and Rivers, Douglas},
  year = {2020},
  month = apr,
  journal = {International Journal of Forecasting},
  volume = {36},
  number = {2},
  pages = {399--413},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.05.012},
  urldate = {2022-04-11},
  abstract = {We describe a strategy for applying multilevel regression and post-stratification (MRP) methods to pre-election polling. Using a combination of contemporaneous polling, census data, past election polling, past election results, and other sources of information, we are able to construct probabilistic, internally consistent estimates of national votes and the sub-national electoral districts that determine seats or electoral votes in many electoral systems. We report on the performance of the general framework in three applications that were conducted and released publicly in advance of the 2016 UK Referendum on EU Membership, the 2016 US Presidential Election, and the 2017 UK General Election.},
  langid = {english},
  keywords = {Election forecasting,Multilevel regression and stratification,Polling,UK politics,US politics},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\83XNYAKA\\Lauderdale et al. - 2020 - Model-based pre-election polling for national and .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CWLBRTLV\\S016920701930189X.html}
}

@article{le_building_2011,
  title = {Building High-Level Features Using Large Scale Unsupervised Learning},
  author = {Le, Quoc V. and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
  year = {2011},
  month = dec,
  journal = {arXiv:1112.6209 [cs]},
  eprint = {1112.6209},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {We consider the problem of building highlevel, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8\% accuracy in recognizing 22,000 object categories from ImageNet, a leap of 70\% relative improvement over the previous state-of-the-art.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\SYKAV9MR\Le et al. - 2011 - Building high-level features using large scale uns.pdf}
}

@article{lecun_deep_2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  urldate = {2019-03-19},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3VY9BY77\LeCun et al. - 2015 - Deep learning.pdf}
}

@article{lee_demand_2019,
  title = {Demand {{Forecasting}} from {{Spatiotemporal Data}} with {{Graph Networks}} and {{Temporal-Guided Embedding}}},
  author = {Lee, Doyup and Jung, Suehun and Cheon, Yeongjae and Kim, Dongil and You, Seungil},
  year = {2019},
  month = oct,
  journal = {arXiv:1905.10709 [cs, stat]},
  eprint = {1905.10709},
  primaryclass = {cs, stat},
  urldate = {2020-02-19},
  abstract = {Short-term demand forecasting models commonly combine convolutional and recurrent layers to extract complex spatiotemporal patterns in data. Long-term histories are also used to consider periodicity and seasonality patterns as time series data. In this study, we propose an efficient architecture, Temporal-Guided Network (TGNet), which utilizes graph networks and temporal-guided embedding. Graph networks extract invariant features to permutations of adjacent regions instead of convolutional layers. Temporal-guided embedding explicitly learns temporal contexts from training data and is substituted for the input of long-term histories from days/weeks ago. TGNet learns an autoregressive model, conditioned on temporal contexts of forecasting targets from temporal-guided embedding. Finally, our model achieves competitive performances with other baselines on three spatiotemporal demand dataset from real-world, but the number of trainable parameters is about 20 times smaller than a state-of-the-art baseline. We also show that temporal-guided embedding learns temporal contexts as intended and TGNet has robust forecasting performances even to atypical event situations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PF8ELKV9\\Lee et al. - 2019 - Demand Forecasting from Spatiotemporal Data with G.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\AGNJVY93\\1905.html}
}

@book{lee_filtering_2010,
  title = {Filtering Non-Linear State Space Models: Methods and Economic Applications},
  shorttitle = {Filtering Non-Linear State Space Models},
  author = {Lee, Kai Ming},
  year = {2010},
  series = {Tinbergen {{Institute}} Research Series},
  number = {474},
  publisher = {{Thela Thesis}},
  address = {{Amsterdam}},
  isbn = {978-90-361-0169-1},
  langid = {english},
  annotation = {00000  OCLC: 837590866},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TCVRZQJX\Lee - 2010 - Filtering non-linear state space models methods a.pdf}
}

@article{lee_hierarchical_2017,
  title = {{Hierarchical time series forecasting with an application to traffic accident counts}},
  author = {Lee, Jooeun and Seong, Byeongchan},
  year = {2017},
  journal = {The Korean Journal of Applied Statistics},
  volume = {30},
  number = {1},
  pages = {181--193},
  publisher = {{The Korean Statistical Society}},
  issn = {1225-066X},
  doi = {10.5351/KJAS.2017.30.1.181},
  urldate = {2021-02-04},
  abstract = {ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê³„ì¸µì  ì‹œê³„ì—´ ìžë£Œ ë¶„ì„ì„ ìœ„í•œ ëŒ€í‘œì ì¸ ë‘ ê°€ì§€ ë°©ë²•ì¸ ìƒí–¥ì‹ê³¼ ìµœì ì¡°í•© ì˜ˆì¸¡ë²•ì„ ì†Œê°œí•œë‹¤. ì´ëŸ¬í•œ ì˜ˆì¸¡ë²•ì€ ê³„ì¸µì  ì‹œê³„ì—´ì„ êµ¬ì„±í•˜ëŠ” ëª¨ë“  ê³„ì—´ì„ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ë…ë¦½ì  ì˜ˆì¸¡ê³¼ ë‹¬ë¦¬, ìž„ì˜ì˜ ì¡°ì • ê³¼ì •ì´ ì—†ì´ í•˜ìœ„ ê³„ì¸µ ê³„ì—´ì˜ ì˜ˆì¸¡ê°’ì˜ í•©ì€ í•­ìƒ ìƒìœ„ ê³„ì¸µì˜ ì˜ˆì¸¡ê°’ê³¼ ì¼ì¹˜í•˜ê²Œ ëœë‹¤. ë˜í•œ, ë…ë¦½ì  ì˜ˆì¸¡ê³¼ ë¹„êµí•˜ì—¬ ì˜ˆì¸¡ë ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤. ê³„ì¸µì  ì˜ˆì¸¡ë²•ì˜ íš¨ìœ¨ì„±ì„ ì‚´íŽ´ë³´ê¸° ìœ„í•˜ì—¬ êµ­ë‚´ 16ê°œ ì‹œë„ë³„ ë‚¨ë…€ êµí†µì‚¬ê³  ë°œìƒê±´ìˆ˜ ì‹œê³„ì—´ ìžë£Œë¥¼ ì˜ˆì¸¡í•˜ì˜€ë‹¤. ì´ë¥¼ í†µí•˜ì—¬ êµí†µì‚¬ê³  ë°œìƒê±´ìˆ˜ì— ëŒ€í•œ ê° ê³„ì¸µì˜ ì˜ˆì¸¡ì—ì„œ ê³„ì¸µì  ë°©ë²•ê³¼ ë…ë¦½ì  ë°©ë²•ì˜ ì°¨ì´ì  ë° ìš°ìˆ˜ì„±ì„ ë¹„êµí•˜ì˜€ë‹¤. The paper introduces bottom-up and optimal combination methods that can analyze and forecast hierarchical time series. These methods allow forecasts at lower levels to be summed consistently to upper levels without any ad-hoc adjustment. They can also potentially improve forecast performance in comparison to independent forecasts. We forecast regional traffic accident counts as time series data in order to identify efficiency gains from hierarchical forecasting. We observe that bottom-up or optimal combination methods are superior to independent methods in terms of forecast accuracy.},
  langid = {korean-han},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\8H5FV5XX\Lee and Seong - 2017 - Hierarchical time series forecasting with an appli.pdf}
}

@article{lefakis_efficient_2019,
  title = {Efficient {{Regularized Piecewise-Linear Regression Trees}}},
  author = {Lefakis, Leonidas and Zadorozhnyi, Oleksandr and Blanchard, Gilles},
  year = {2019},
  month = jun,
  journal = {arXiv:1907.00275 [cs, stat]},
  eprint = {1907.00275},
  primaryclass = {cs, stat},
  urldate = {2019-09-05},
  abstract = {We present a detailed analysis of the class of regression decision tree algorithms which employ a regulized piecewise-linear node-splitting criterion and have regularized linear models at the leaves. From a theoretic standpoint, based on Rademacher complexity framework, we present new high-probability upper bounds for the generalization error for the proposed classes of regularized regression decision tree algorithms, including LASSO-type, and 2 regularization for linear models at the leaves. Theoretical result are further extended by considering a general type of variable selection procedure. Furthermore, in our work we demonstrate that the analyzed class of regression trees is not only numerically stable but can furthermore be made tractable via an algorithmic implementation, presented herein, as well as with the help of modern GPU technology. Empirically, we present results on multiple datasets which highlight the strengths and potential pitfalls, of the proposed tree algorithms compared to baselines which grow trees based on piecewise constant models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\88VWBPMB\Lefakis et al. - 2019 - Efficient Regularized Piecewise-Linear Regression .pdf}
}

@inproceedings{leguen_probabilistic_2020,
  title = {Probabilistic {{Time Series Forecasting}} with {{Shape}} and {{Temporal Diversity}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LE GUEN, Vincent and THOME, Nicolas},
  year = {2020},
  volume = {33},
  pages = {4427--4440},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-07},
  abstract = {Probabilistic forecasting consists in predicting a distribution of possible future outcomes. In this paper, we address this problem for non-stationary time series, which is very challenging yet crucially important. We introduce the STRIPE model for representing structured diversity based on shape and time features, ensuring both probable predictions while being sharp and accurate. STRIPE is agnostic to the forecasting model, and we equip it with a diversification mechanism relying on determinantal point processes (DPP). We introduce two DPP kernels for modelling diverse trajectories in terms of shape and time, which are both differentiable and proved to be positive semi-definite. To have an explicit control on the diversity structure, we also design an iterative sampling mechanism to disentangle shape and time representations in the latent space. Experiments carried out on synthetic datasets show that STRIPE significantly outperforms baseline methods for representing diversity, while maintaining accuracy of the forecasting model. We also highlight the relevance of the iterative sampling scheme and the importance to use different criteria for measuring quality and diversity. Finally, experiments on real datasets illustrate that STRIPE is able to outperform state-of-the-art probabilistic forecasting approaches in the best sample prediction.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3BI3EIAJ\LE GUEN and THOME - 2020 - Probabilistic Time Series Forecasting with Shape a.pdf}
}

@incollection{leisch_stationarity_1999,
  title = {Stationarity and {{Stability}} of {{Autoregressive Neural Network Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 11},
  author = {Leisch, Friedrich and Trapletti, Adrian and Hornik, Kurt},
  editor = {Kearns, M. J. and Solla, S. A. and Cohn, D. A.},
  year = {1999},
  pages = {267--273},
  publisher = {{MIT Press}},
  urldate = {2019-04-17},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Q7G669CG\\Leisch et al. - 1999 - Stationarity and Stability of Autoregressive Neura.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FJK2NAM9\\1529-stationarity-and-stability-of-autoregressive-neural-network-processes.html}
}

@article{lesort_state_2018,
  title = {State Representation Learning for Control: {{An}} Overview},
  shorttitle = {State Representation Learning for Control},
  author = {Lesort, Timoth{\'e}e and {D{\'i}az-Rodr{\'i}guez}, Natalia and Goudou, Jean-Fran{\c o}is and Filliat, David},
  year = {2018},
  month = dec,
  journal = {Neural Networks},
  volume = {108},
  pages = {379--392},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.07.006},
  urldate = {2019-05-10},
  abstract = {Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. The representation is learned to capture the variation in the environment generated by the agent's actions; this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension characteristic of the representation helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning. This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.},
  keywords = {Disentanglement of control factors,Learning disentangled representations,Low dimensional embedding learning,Reinforcement learning,Robotics,State representation learning},
  annotation = {00026},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3XJADTHF\\Lesort et al. - 2018 - State representation learning for control An over.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MUF64FQZ\\S0893608018302053.html}
}

@article{li_area_2018,
  title = {Area {{Attention}}},
  author = {Li, Yang and Kaiser, Lukasz and Bengio, Samy and Si, Si},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.10126 [cs, stat]},
  eprint = {1810.10126},
  primaryclass = {cs, stat},
  urldate = {2019-04-01},
  abstract = {Existing attention mechanisms are trained to attend to individual items in a collection (the memory) with a predefined, fixed granularity, e.g., a word token or an image grid. We propose area attention: a way to attend to areas in the memory, where each area contains a group of items that are structurally adjacent, e.g., spatially for a 2D memory such as images, or temporally for a 1D memory such as natural language sentences. Importantly, the shape and the size of an area are dynamically determined via learning, which enables a model to attend to information with varying granularity. Area attention can easily work with existing model architectures such as multi-head attention for simultaneously attending to multiple areas in the memory. We evaluate area attention on two tasks: neural machine translation (both character and token-level) and image captioning, and improve upon strong (state-of-the-art) baselines in all the cases. These improvements are obtainable with a basic form of area attention that is parameter free.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6VTUFNIS\Li et al. - 2018 - Area Attention.pdf}
}

@article{li_bayesian_2020,
  title = {A {{Bayesian Long Short-Term Memory Model}} for {{Value}} at {{Risk}} and {{Expected Shortfall Joint Forecasting}}},
  author = {Li, Zhengkun and Tran, Minh-Ngoc and Wang, Chao and Gerlach, Richard and Gao, Junbin},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.08374 [cs, q-fin, stat]},
  eprint = {2001.08374},
  primaryclass = {cs, q-fin, stat},
  urldate = {2020-02-19},
  abstract = {Value-at-Risk (VaR) and Expected Shortfall (ES) are widely used in the financial sector to measure the market risk and manage the extreme market movement. The recent link between the quantile score function and the Asymmetric Laplace density has led to a flexible likelihood-based framework for joint modelling of VaR and ES. It is of high interest in financial applications to be able to capture the underlying joint dynamics of these two quantities. We address this problem by developing a hybrid model that is based on the Asymmetric Laplace quasi-likelihood and employs the Long Short-Term Memory (LSTM) time series modelling technique from Machine Learning to capture efficiently the underlying dynamics of VaR and ES. We refer to this model as LSTM-AL. We adopt the adaptive Markov chain Monte Carlo (MCMC) algorithm for Bayesian inference in the LSTM-AL model. Empirical results show that the proposed LSTM-AL model can improve the VaR and ES forecasting accuracy over a range of well-established competing models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Finance - Risk Management,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9M7IAZN2\\Li et al. - 2020 - A Bayesian Long Short-Term Memory Model for Value .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\U2Z334IQ\\2001.html}
}

@article{li_continual_2019,
  title = {Continual {{Learning Using Bayesian Neural Networks}}},
  author = {Li, HongLin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.04112 [cs, stat]},
  eprint = {1910.04112},
  primaryclass = {cs, stat},
  urldate = {2020-02-27},
  abstract = {Continual learning models allow to learn and adapt to new changes and tasks over time. However, in continual and sequential learning scenarios in which the models are trained using different data with various distributions, neural networks tend to forget the previously learned knowledge. This phenomenon is often referred to as catastrophic forgetting. The catastrophic forgetting is an inevitable problem in continual learning models for dynamic environments. To address this issue, we propose a method, called Continual Bayesian Learning Networks (CBLN), which enables the networks to allocate additional resources to adapt to new tasks without forgetting the previously learned tasks. Using a Bayesian Neural Network, CBLN maintains a mixture of Gaussian posterior distributions that are associated with different tasks. The proposed method tries to optimise the number of resources that are needed to learn each task and avoids an exponential increase in the number of resources that are involved in learning multiple tasks. The proposed method does not need to access the past training data and can choose suitable weights to classify the data points during the test time automatically based on an uncertainty criterion. We have evaluated our method on the MNIST and UCR time-series datasets. The evaluation results show that our method can address the catastrophic forgetting problem at a promising rate compared to the state-of-the-art models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\84IUCGJ7\\Li et al. - 2019 - Continual Learning Using Bayesian Neural Networks.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7G5GPTDI\\1910.html}
}

@article{li_diffusion_2017,
  title = {Diffusion {{Convolutional Recurrent Neural Network}}: {{Data-Driven Traffic Forecasting}}},
  shorttitle = {Diffusion {{Convolutional Recurrent Neural Network}}},
  author = {Li, Yaguang and Yu, Rose and Shahabi, Cyrus and Liu, Yan},
  year = {2017},
  month = jul,
  journal = {arXiv:1707.01926 [cs, stat]},
  eprint = {1707.01926},
  primaryclass = {cs, stat},
  urldate = {2019-04-26},
  abstract = {Spatiotemporal forecasting has various applications in neuroscience, climate and transportation domain. Traffic forecasting is one canonical example of such learning task. The task is challenging due to (1) complex spatial dependency on road networks, (2) non-linear temporal dynamics with changing road conditions and (3) inherent difficulty of long-term forecasting. To address these challenges, we propose to model the traffic flow as a diffusion process on a directed graph and introduce Diffusion Convolutional Recurrent Neural Network (DCRNN), a deep learning framework for traffic forecasting that incorporates both spatial and temporal dependency in the traffic flow. Specifically, DCRNN captures the spatial dependency using bidirectional random walks on the graph, and the temporal dependency using the encoder-decoder architecture with scheduled sampling. We evaluate the framework on two real-world large scale road network traffic datasets and observe consistent improvement of 12\% - 15\% over state-of-the-art baselines.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\X4AI4AXJ\Li et al. - 2017 - Diffusion Convolutional Recurrent Neural Network .pdf}
}

@inproceedings{li_diffusion_2018,
  title = {Diffusion {{Convolutional Recurrent Neural Network}}: {{Data-Driven Traffic Forecasting}}},
  shorttitle = {Diffusion {{Convolutional Recurrent Neural Network}}},
  booktitle = {6th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2018, {{Vancouver}}, {{BC}}, {{Canada}}, {{April}} 30 - {{May}} 3, 2018, {{Conference Track Proceedings}}},
  author = {Li, Yaguang and Yu, Rose and Shahabi, Cyrus and Liu, Yan},
  year = {2018},
  publisher = {{OpenReview.net}},
  urldate = {2020-01-17}
}

@article{li_ealstm_2018,
  title = {{{EA-LSTM}}: {{Evolutionary Attention-based LSTM}} for {{Time Series Prediction}}},
  shorttitle = {{{EA-LSTM}}},
  author = {Li, Youru and Zhu, Zhenfeng and Kong, Deqiang and Han, Hua and Zhao, Yao},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.03760 [cs, stat]},
  eprint = {1811.03760},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {Time series prediction with deep learning methods, especially long short-term memory neural networks (LSTMs), have scored significant achievements in recent years. Despite the fact that the LSTMs can help to capture long-term dependencies, its ability to pay different degree of attention on sub-window feature within multiple time-steps is insufficient. To address this issue, an evolutionary attention-based LSTM training with competitive random search is proposed for multivariate time series prediction. By transferring shared parameters, an evolutionary attention learning approach is introduced to the LSTMs model. Thus, like that for biological evolution, the pattern for importance-based attention sampling can be confirmed during temporal relationship mining. To refrain from being trapped into partial optimization like traditional gradient-based methods, an evolutionary computation inspired competitive random search method is proposed, which can well configure the parameters in the attention layer. Experimental results have illustrated that the proposed model can achieve competetive prediction performance compared with other baseline methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read - in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3LVKFGKT\\Li et al. - 2018 - EA-LSTM Evolutionary Attention-based LSTM for Tim.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\59YVDLDR\\1811.html}
}

@incollection{li_enhancing_2019,
  title = {Enhancing the {{Locality}} and {{Breaking}} the {{Memory Bottleneck}} of {{Transformer}} on {{Time Series Forecasting}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
  year = {2019},
  pages = {5244--5254},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-01-17},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\JDNX8TIY\\Li et al. - 2019 - Enhancing the Locality and Breaking the Memory Bot.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5BZDWSXG\\8766-enhancing-the-locality-and-breaking-the-memory-bottleneck-of-transformer-on-time-series-fo.html}
}

@article{li_forecast_2019,
  title = {A Forecast Reconciliation Approach to Cause-of-Death Mortality Modeling},
  author = {Li, Han and Li, Hong and Lu, Yang and Panagiotelis, Anastasios},
  year = {2019},
  month = may,
  journal = {Insurance: Mathematics and Economics},
  volume = {86},
  pages = {122--133},
  issn = {0167-6687},
  doi = {10.1016/j.insmatheco.2019.02.011},
  urldate = {2022-05-24},
  abstract = {Life expectancy has been increasing sharply around the globe since the second half of the 20th century. Mortality modeling and forecasting have therefore attracted increasing attention from various areas, such as the public pension systems, commercial insurance sectors, as well as actuarial, demographic and epidemiological research. Compared to the aggregate mortality experience, cause-specific mortality rates contain more detailed information, and can help us better understand the ongoing mortality improvements. However, when conducting cause-of-death mortality modeling, it is important to ensure coherence in the forecasts. That is, the forecasts of cause-specific mortality rates should add up to the forecasts of the aggregate mortality rates. In this paper, we propose a novel forecast reconciliation approach to achieve this goal. We use the age-specific mortality experience in the U.S. during 1970\textendash 2015 as a case study. Seven major causes of death are considered in this paper. By incorporating both the disaggregate cause-specific data and the aggregate total-level data, we achieve better forecasting results at both levels and coherence across forecasts. Moreover, we perform a cluster analysis on the cause-specific mortality data. It is shown that combining mortality experience from causes with similar mortality patterns can provide additional useful information, and thus further improve forecast accuracy. Finally, based on the proposed reconciliation approach, we conduct a scenario-based analysis to project future mortality rates under the assumption of certain causes being eliminated.},
  langid = {english},
  keywords = {Cause-elimination,Cause-of-death,Clustering,Forecast reconciliation,Hierarchical time series,Mortality modeling},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RZEZHUJQ\Li et al. - 2019 - A forecast reconciliation approach to cause-of-dea.pdf}
}

@article{li_gated_2017,
  title = {Gated {{Graph Sequence Neural Networks}}},
  author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
  year = {2017},
  month = sep,
  journal = {arXiv:1511.05493 [cs, stat]},
  eprint = {1511.05493},
  primaryclass = {cs, stat},
  urldate = {2021-12-14},
  abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be described as abstract data structures.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\U22K8VPP\Li et al. - 2017 - Gated Graph Sequence Neural Networks.pdf}
}

@article{li_gentle_,
  title = {A {{Gentle Introduction}} to {{Gradient Boosting}}},
  author = {Li, Cheng},
  pages = {80},
  langid = {english},
  annotation = {00015},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BJD3A5JV\Li - A Gentle Introduction to Gradient Boosting.pdf}
}

@article{li_quantilefrequency_2019,
  title = {Quantile-{{Frequency Analysis}} and {{Spectral Divergence Metrics}} for {{Diagnostic Checks}} of {{Time Series With Nonlinear Dynamics}}},
  author = {Li, Ta-Hsin},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.02545 [q-fin, stat]},
  eprint = {1908.02545},
  primaryclass = {q-fin, stat},
  urldate = {2020-02-17},
  abstract = {Nonlinear dynamic volatility has been observed in many financial time series. The recently proposed quantile periodogram offers an alternative way to examine this phenomena in the frequency domain. The quantile periodogram is constructed from trigonometric quantile regression of time series data at different frequencies and quantile levels. It is a useful tool for quantile-frequency analysis (QFA) of nonlinear serial dependence. This paper introduces a number of spectral divergence metrics based on the quantile periodogram for diagnostic checks of financial time series models and model-based discriminant analysis. The parametric bootstrapping technique is employed to compute the \$p\$-values of the metrics. The usefulness of the proposed method is demonstrated empirically by a case study using the daily log returns of the S\textbackslash\&P 500 index over three periods of time together with their GARCH-type models. The results show that the QFA method is able to provide additional insights into the goodness of fit of these financial time series models that may have been missed by conventional tests. The results also show that the QFA method offers a more informative way of discriminant analysis for detecting regime changes in time series.},
  archiveprefix = {arxiv},
  keywords = {Quantitative Finance - Statistical Finance,Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3QEM8XR8\1908.html}
}

@article{li_textbased_2018,
  title = {Text-Based Crude Oil Price Forecasting: {{A}} Deep Learning Approach},
  shorttitle = {Text-Based Crude Oil Price Forecasting},
  author = {Li, Xuerong and Shang, Wei and Wang, Shouyang},
  year = {2018},
  month = oct,
  journal = {International Journal of Forecasting},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2018.07.006},
  urldate = {2019-03-19},
  abstract = {This study proposes a new, novel crude oil price forecasting method based on online media text mining, with the aim of capturing the more immediate market antecedents of price fluctuations. Specifically, this is an early attempt to apply deep learning techniques to crude oil forecasting, and to extract hidden patterns within online news media using a convolutional neural network (CNN). While the news-text sentiment features and the features extracted by the CNN model reveal significant relationships with the price change, they need to be grouped according to their topics in the price forecasting in order to obtain a greater forecasting accuracy. This study further proposes a feature grouping method based on the Latent Dirichlet Allocation (LDA) topic model for distinguishing effects from various online news topics. Optimized input variable combination is constructed using lag order selection and feature selection methods. Our empirical results suggest that the proposed topic-sentiment synthesis forecasting models perform better than the older benchmark models. In addition, text features and financial features are shown to be complementary in producing more accurate crude oil price forecasts.},
  keywords = {Convolutional neural network,Financial markets,Oil price forecasting,Online news,Text analysis},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CKK6TAV9\\Li et al. - 2018 - Text-based crude oil price forecasting A deep lea.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\27NA4L7F\\S0169207018301110.html}
}

@article{li_visualizing_,
  title = {Visualizing the {{Decision-making Process}} in {{Deep Neural Decision Forest}}},
  author = {Li, Shichao and Cheng, Kwang-Ting},
  pages = {4},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WNC49C5C\Li and Cheng - Visualizing the Decision-making Process in Deep Ne.pdf}
}

@misc{liang_conformal_2023,
  title = {Conformal Inference Is (Almost) Free for Neural Networks Trained with Early Stopping},
  author = {Liang, Ziyi and Zhou, Yanfei and Sesia, Matteo},
  year = {2023},
  month = jan,
  number = {arXiv:2301.11556},
  eprint = {2301.11556},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.11556},
  urldate = {2023-03-07},
  abstract = {Early stopping based on hold-out data is a popular regularization technique designed to mitigate overfitting and increase the predictive accuracy of neural networks. Models trained with early stopping often provide relatively accurate predictions, but they generally still lack precise statistical guarantees unless they are further calibrated using independent hold-out data. This paper addresses the above limitation with conformalized early stopping: a novel method that combines early stopping with conformal calibration while efficiently recycling the same hold-out data. This leads to models that are both accurate and able to provide exact predictive inferences without multiple data splits nor overly conservative adjustments. Practical implementations are developed for different learning tasks -- outlier detection, multi-class classification, regression -- and their competitive performance is demonstrated on real data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GTFAFP2R\\Liang et al. - 2023 - Conformal inference is (almost) free for neural ne.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BGKJUAMP\\2301.html}
}

@inproceedings{liang_geoman_2018,
  title = {{{GeoMAN}}: {{Multi-level Attention Networks}} for {{Geo-sensory Time Series Prediction}}},
  shorttitle = {{{GeoMAN}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Liang, Yuxuan and Ke, Songyu and Zhang, Junbo and Yi, Xiuwen and Zheng, Yu},
  year = {2018},
  month = jul,
  pages = {3428--3434},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Stockholm, Sweden}},
  doi = {10.24963/ijcai.2018/476},
  urldate = {2019-04-03},
  abstract = {Numerous sensors have been deployed in different geospatial locations to continuously and cooperatively monitor the surrounding environment, such as the air quality. These sensors generate multiple geo-sensory time series, with spatial correlations between their readings. Forecasting geo-sensory time series is of great importance yet very challenging as it is affected by many complex factors, i.e., dynamic spatio-temporal correlations and external factors. In this paper, we predict the readings of a geo-sensor over several future hours by using a multi-level attention-based recurrent neural network that considers multiple sensors' readings, meteorological data, and spatial data. More specifically, our model consists of two major parts: 1) a multi-level attention mechanism to model the dynamic spatio-temporal dependencies. 2) a general fusion module to incorporate the external factors from different domains. Experiments on two types of real-world datasets, viz., air quality data and water quality data, demonstrate that our method outperforms nine baseline methods.},
  isbn = {978-0-9992411-2-7},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\9ZQK8F4U\Liang et al. - 2018 - GeoMAN Multi-level Attention Networks for Geo-sen.pdf}
}

@article{liaw_classification_2002,
  title = {Classification and {{Regression}} by {{randomForest}}},
  author = {Liaw, Andy and Wiener, Matthew},
  year = {2002},
  volume = {2},
  pages = {5},
  urldate = {2019-04-15},
  langid = {english},
  keywords = {read - in related work},
  annotation = {08765},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\W9KMZE49\Liaw and Wiener - 2002 - Classiï¬cation and Regression by randomForest.pdf}
}

@inproceedings{liberty_simple_2013,
  title = {Simple and Deterministic Matrix Sketching},
  booktitle = {Proceedings of the 19th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Liberty, Edo},
  year = {2013},
  month = aug,
  pages = {581--588},
  publisher = {{ACM}},
  address = {{Chicago Illinois USA}},
  doi = {10.1145/2487575.2487623},
  urldate = {2022-02-04},
  abstract = {A sketch of a matrix A is another matrix B which is significantly smaller than A, but still approximates it well. Finding such sketches efficiently is an important building block in modern algorithms for approximating, for example, the PCA of massive matrices. This task is made more challenging in the streaming model, where each row of the input matrix can be processed only once and storage is severely limited.},
  isbn = {978-1-4503-2174-7},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\EHPY9HSR\Liberty - 2013 - Simple and deterministic matrix sketching.pdf}
}

@article{lika_facing_2014,
  title = {Facing the Cold Start Problem in Recommender Systems},
  author = {Lika, Blerina and Kolomvatsos, Kostas and Hadjiefthymiades, Stathes},
  year = {2014},
  month = mar,
  journal = {Expert Systems with Applications},
  volume = {41},
  number = {4, Part 2},
  pages = {2065--2073},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2013.09.005},
  urldate = {2020-01-19},
  abstract = {A recommender system (RS) aims to provide personalized recommendations to users for specific items (e.g., music, books). Popular techniques involve content-based (CB) models and collaborative filtering (CF) approaches. In this paper, we deal with a very important problem in RSs: The cold start problem. This problem is related to recommendations for novel users or new items. In case of new users, the system does not have information about their preferences in order to make recommendations. We propose a model where widely known classification algorithms in combination with similarity techniques and prediction mechanisms provide the necessary means for retrieving recommendations. The proposed approach incorporates classification methods in a pure CF system while the use of demographic data help for the identification of other users with similar behavior. Our experiments show the performance of the proposed system through a large number of experiments. We adopt the widely known dataset provided by the GroupLens research group. We reveal the advantages of the proposed solution by providing satisfactory numerical results in different experimental scenarios.},
  langid = {english},
  keywords = {Cold start problem,Recommender systems},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Z4IJP843\\Lika et al. - 2014 - Facing the cold start problem in recommender syste.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZV5CKGJQ\\S0957417413007240.html}
}

@article{lillicrap_continuous_2015,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2015},
  month = sep,
  journal = {arXiv:1509.02971 [cs, stat]},
  eprint = {1509.02971},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies ``end-to-end'': directly from raw pixel inputs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LQ7294AM\Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf}
}

@article{lim_recurrent_2019,
  title = {Recurrent {{Neural Filters}}: {{Learning Independent Bayesian Filtering Steps}} for {{Time Series Prediction}}},
  shorttitle = {Recurrent {{Neural Filters}}},
  author = {Lim, Bryan and Zohren, Stefan and Roberts, Stephen},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.08096 [cs, eess, stat]},
  eprint = {1901.08096},
  primaryclass = {cs, eess, stat},
  urldate = {2019-05-10},
  abstract = {Despite the recent popularity of deep generative state space models, few comparisons have been made between network architectures and the inference steps of the Bayesian filtering framework \textendash{} with most models simultaneously approximating both state transition and update steps with a single recurrent neural network (RNN). In this paper, we introduce the Recurrent Neural Filter (RNF), a novel recurrent variational autoencoder architecture that learns distinct representations for each Bayesian filtering step, captured by a series of encoders and decoders. Testing this on three real-world time series datasets, we demonstrate that decoupling representations not only improves the accuracy of one-step-ahead forecasts while providing realistic uncertainty estimates, but also facilitates multistep prediction through the separation of encoder stages.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  annotation = {00001},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\CK5553ZD\Lim et al. - 2019 - Recurrent Neural Filters Learning Independent Bay.pdf}
}

@article{lim_recurrent_2020,
  title = {Recurrent {{Neural Filters}}: {{Learning Independent Bayesian Filtering Steps}} for {{Time Series Prediction}}},
  shorttitle = {Recurrent {{Neural Filters}}},
  author = {Lim, Bryan and Zohren, Stefan and Roberts, Stephen},
  year = {2020},
  month = jan,
  journal = {arXiv:1901.08096 [cs, eess, stat]},
  eprint = {1901.08096},
  primaryclass = {cs, eess, stat},
  urldate = {2020-02-19},
  abstract = {Despite the recent popularity of deep generative state space models, few comparisons have been made between network architectures and the inference steps of the Bayesian filtering framework -- with most models simultaneously approximating both state transition and update steps with a single recurrent neural network (RNN). In this paper, we introduce the Recurrent Neural Filter (RNF), a novel recurrent autoencoder architecture that learns distinct representations for each Bayesian filtering step, captured by a series of encoders and decoders. Testing this on three real-world time series datasets, we demonstrate that the decoupled representations learnt not only improve the accuracy of one-step-ahead forecasts while providing realistic uncertainty estimates, but also facilitate multistep prediction through the separation of encoder stages},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\26JAQH8W\\Lim et al. - 2020 - Recurrent Neural Filters Learning Independent Bay.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FFT8584Y\\1901.html}
}

@article{lim_temporal_2019,
  title = {Temporal {{Fusion Transformers}} for {{Interpretable Multi-horizon Time Series Forecasting}}},
  author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.09363 [cs, stat]},
  eprint = {1912.09363},
  primaryclass = {cs, stat},
  urldate = {2020-01-27},
  abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs \textendash{} including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically \textendash{} without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) \textendash{} a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VSM79SBY\Lim et al. - 2019 - Temporal Fusion Transformers for Interpretable Mul.pdf}
}

@article{lim_temporal_2021,
  title = {Temporal {{Fusion Transformers}} for Interpretable Multi-Horizon Time Series Forecasting},
  author = {Lim, Bryan and Ar{\i}k, Sercan {\"O}. and Loeff, Nicolas and Pfister, Tomas},
  year = {2021},
  month = oct,
  journal = {International Journal of Forecasting},
  volume = {37},
  number = {4},
  pages = {1748--1764},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.03.012},
  urldate = {2023-06-08},
  abstract = {Multi-horizon forecasting often contains a complex mix of inputs \textendash{} including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed in the past \textendash{} without any prior information on how they interact with the target. Several deep learning methods have been proposed, but they are typically `black-box' models that do not shed light on how they use the full range of inputs present in practical scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) \textendash{} a novel attention-based architecture that combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, TFT uses recurrent layers for local processing and interpretable self-attention layers for long-term dependencies. TFT utilizes specialized components to select relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of scenarios. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and highlight three practical interpretability use cases of TFT.},
  langid = {english},
  keywords = {Attention mechanisms,Deep learning,Explainable AI,Interpretability,Multi-horizon forecasting,Time series},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BRNYG9YA\Lim et al. - 2021 - Temporal Fusion Transformers for interpretable mul.pdf}
}

@article{lim_time_2020,
  title = {Time {{Series Forecasting With Deep Learning}}: {{A Survey}}},
  shorttitle = {Time {{Series Forecasting With Deep Learning}}},
  author = {Lim, Bryan and Zohren, Stefan},
  year = {2020},
  month = sep,
  journal = {arXiv:2004.13408 [cs, stat]},
  eprint = {2004.13408},
  primaryclass = {cs, stat},
  urldate = {2021-02-04},
  abstract = {Numerous deep learning architectures have been developed to accommodate the diversity of time series datasets across different domains. In this article, we survey common encoder and decoder designs used in both one-step-ahead and multi-horizon time series forecasting -- describing how temporal information is incorporated into predictions by each model. Next, we highlight recent developments in hybrid deep learning models, which combine well-studied statistical models with neural network components to improve pure methods in either category. Lastly, we outline some ways in which deep learning can also facilitate decision support with time series data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\U7DMS9L7\\Lim and Zohren - 2020 - Time Series Forecasting With Deep Learning A Surv.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\J8KR4RN8\\2004.html}
}

@article{lim_timeseries_2021,
  title = {Time-Series Forecasting with Deep Learning: A Survey},
  shorttitle = {Time-Series Forecasting with Deep Learning},
  author = {Lim, Bryan and Zohren, Stefan},
  year = {2021},
  month = apr,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {379},
  number = {2194},
  pages = {20200209},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2020.0209},
  urldate = {2021-10-06},
  abstract = {Numerous deep learning architectures have been developed to accommodate the diversity of time-series datasets across different domains. In this article, we survey common encoder and decoder designs used in both one-step-ahead and multi-horizon time-series forecasting\textemdash describing how temporal information is incorporated into predictions by each model. Next, we highlight recent developments in hybrid deep learning models, which combine well-studied statistical models with neural network components to improve pure methods in either category. Lastly, we outline some ways in which deep learning can also facilitate decision support with time-series data. This article is part of the theme issue `Machine learning for weather and climate modelling'.},
  keywords = {counterfactual prediction,deep neural networks,hybrid models,interpretability,time-series forecasting,uncertainty estimation},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\Z744FVSC\Lim and Zohren - 2021 - Time-series forecasting with deep learning a surv.pdf}
}

@article{lin_fourier_2019,
  title = {A {{Fourier}} Domain Acceleration Framework for Convolutional Neural Networks},
  author = {Lin, Jinhua and Ma, Lin and Yao, Yu},
  year = {2019},
  month = oct,
  journal = {Neurocomputing},
  volume = {364},
  pages = {254--268},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.06.080},
  urldate = {2021-01-21},
  abstract = {Acceleration of training and inference of convolutional neural networks (CNNs) plays a significant role in deep learning efforts for large-scale datasets. However, it is difficult to accelerate the training and inference of CNNs based on traditional Fourier domain acceleration frameworks because Fourier domain training and inference are related to many complicated factors, such as the architecture of Fourier domain propagation passes, the representation of the activation function and the design of downsampling operations. A conceptually intuitive, useful and general Fourier domain acceleration framework for CNNs is proposed in this paper. Taking the proposed Fourier domain rectified linear unit (FReLU) as an activation function and the proposed Fourier domain pooling function (FPool) as a downsampling function, a Fourier domain acceleration framework is established for CNNs, and the inverse activation function (FReLU-1) and inverse downsampling function (FPool-1) are further obtained for the backward propagation pass. Furthermore, a block decomposition pipeline is integrated into the Fourier domain forward/backward propagation passes of CNNs to accelerate the training and inference of CNNs. The results show that the proposed acceleration framework can accelerate the training and inference of CNNs by a significant factor without reducing the recognition precision.},
  langid = {english},
  keywords = {Activation function,Convolutional neural networks,Deep learning,Downsampling operations,Forward/backward propagation passes},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\JWJG5WRZ\\Lin et al. - 2019 - A Fourier domain acceleration framework for convol.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PKV4KX39\\S0925231219309932.html}
}

@inproceedings{lin_hybrid_2017,
  title = {Hybrid {{Neural Networks}} for {{Learning}} the {{Trend}} in {{Time Series}}},
  booktitle = {Proceedings of the {{Twenty-Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Lin, Tao and Guo, Tian and Aberer, Karl},
  year = {2017},
  month = aug,
  pages = {2273--2279},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Melbourne, Australia}},
  doi = {10.24963/ijcai.2017/316},
  urldate = {2019-04-25},
  abstract = {Trend of time series characterizes the intermediate upward and downward behaviour of time series. Learning and forecasting the trend in time series data play an important role in many real applications, ranging from resource allocation in data centers, load schedule in smart grid, and so on. Inspired by the recent successes of neural networks, in this paper we propose TreNet, a novel end-toend hybrid neural network to learn local and global contextual features for predicting the trend of time series. TreNet leverages convolutional neural networks (CNNs) to extract salient features from local raw data of time series. Meanwhile, considering the long-range dependency existing in the sequence of historical trends, TreNet uses a long-short term memory recurrent neural network (LSTM) to capture such dependency. Then, a feature fusion layer is to learn joint representation for predicting the trend. TreNet demonstrates its effectiveness by outperforming CNN, LSTM, the cascade of CNN and LSTM, Hidden Markov Model based method and various kernel based baselines on real datasets.},
  isbn = {978-0-9992411-0-3},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00018},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\S7TBHR2Q\Lin et al. - 2017 - Hybrid Neural Networks for Learning the Trend in T.pdf}
}

@inproceedings{lin_locally_2021,
  title = {Locally {{Valid}} and {{Discriminative Prediction Intervals}} for {{Deep Learning Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lin, Zhen and Trivedi, Shubhendu and Sun, Jimeng},
  year = {2021},
  volume = {34},
  pages = {8378--8391},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-07},
  abstract = {Crucial for building trust in deep learning models for critical real-world applications is efficient and theoretically sound uncertainty quantification, a task that continues to be challenging. Useful uncertainty information is expected to have two key properties: It should be valid (guaranteeing coverage) and discriminative (more uncertain when the expected risk is high). Moreover, when combined with deep learning (DL) methods, it should be scalable and affect the DL model performance minimally. Most existing Bayesian methods lack frequentist coverage guarantees and usually affect model performance. The few available frequentist methods are rarely discriminative and/or violate coverage guarantees due to unrealistic assumptions. Moreover, many methods are expensive or require substantial modifications to the base neural network. Building upon recent advances in conformal prediction [13, 33] and leveraging the classical idea of kernel regression, we propose Locally Valid and Discriminative prediction intervals (LVD), a simple, efficient, and lightweight method to construct discriminative prediction intervals (PIs) for almost any DL model. With no assumptions on the data distribution, such PIs also offer finite-sample local coverage guarantees (contrasted to the simpler marginal coverage). We empirically verify, using diverse datasets, that besides being the only locally valid method for DL, LVD also exceeds or matches the performance (including coverage rate and prediction accuracy) of existing uncertainty quantification methods, while offering additional benefits in scalability and flexibility.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5UCSPNVE\Lin et al. - 2021 - Locally Valid and Discriminative Prediction Interv.pdf}
}

@article{lin_medical_2019,
  title = {Medical {{Time Series Classification}} with {{Hierarchical Attention-based Temporal Convolutional Networks}}: {{A Case Study}} of {{Myotonic Dystrophy Diagnosis}}},
  shorttitle = {Medical {{Time Series Classification}} with {{Hierarchical Attention-based Temporal Convolutional Networks}}},
  author = {Lin, Lei and Xu, Beilei and Wu, Wencheng and Richardson, Trevor and Bernal, Edgar A.},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.11748 [cs]},
  eprint = {1903.11748},
  primaryclass = {cs},
  urldate = {2019-10-10},
  abstract = {Myotonia, which refers to delayed muscle relaxation after contraction, is the main symptom of myotonic dystrophy patients. We propose a hierarchical attention-based temporal convolutional network (HA-TCN) for myotonic dystrohpy diagnosis from handgrip time series data, and introduce mechanisms that enable model explainability. We compare the performance of the HA-TCN model against that of benchmark TCN models, LSTM models with and without attention mechanisms, and SVM approaches with handcrafted features. In terms of classification accuracy and F1 score, we found all deep learning models have similar levels of performance, and they all outperform SVM. Further, the HA-TCN model outperforms its TCN counterpart with regards to computational efficiency regardless of network depth, and in terms of performance particularly when the number of hidden layers is small. Lastly, HA-TCN models can consistently identify relevant time series segments in the relaxation phase of the handgrip time series, and exhibit increased robustness to noise when compared to attention-based LSTM models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  annotation = {00002},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\CUZU7F95\1903.11748.pdf}
}

@article{liu_ensemble_2019,
  title = {An {{Ensemble Model Based}} on {{Adaptive Noise Reducer}} and {{Over-Fitting Prevention LSTM}} for {{Multivariate Time Series Forecasting}}},
  author = {Liu, F. and Cai, M. and Wang, L. and Lu, Y.},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {26102--26115},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2900371},
  abstract = {Multivariate time series forecasting recently has received extensive attention with its wide application in finance, transportation, environment, and so on. However, few of the currently developed models have considered the impact of noise on prediction. Since multivariate time series contains multiple subsequences with strong nonlinear fluctuations, it is also difficult to obtain satisfactory prediction results. In this paper, aiming at improving prediction performance, we have proposed a novel ensemble three-phase model called adaptive noise reducer-stacked auto-encoder-validating-AdaBoost-based long short-term memory (ANR-SAE-VALSTM). We start with an introduction of a novel ANR for time series noise elimination. The SAEs are then used to extract features from the de-noised multivariate time series. Finally, we feed the de-noised features into the VALSTM to train an ensemble over-fitting prevention predictor. The proposed model is employed on the Beijing PM2.5 dataset and GEFCom2014 Electricity Price dataset. Compared with other popular models, the proposed model has achieved the best prediction performance in all prediction horizons. In addition, a careful ablation study is conducted to demonstrate the efficiency of our model design.},
  keywords = {Adaptation models,adaptive noise reducer,adaptive noise reducer-stacked auto-encoder-validating-AdaBoost-based long short-term memory,adaptive signal processing,ANR-SAE-VALSTM,Autoregressive processes,de-noised features,de-noised multivariate time series,ensemble model,feature extraction,Feature extraction,Forecasting,learning (artificial intelligence),long short-term memory,multivariate time series forecasting,Multivariate time series forecasting,nonlinear fluctuations,over-fitting prevention LSTM,over-fitting prevention predictor,Prediction algorithms,Predictive models,read - in related work,recurrent neural nets,signal denoising,stacked auto-encoders,time series,Time series analysis,time series noise elimination,validating AdaBoost algorithm},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7I7DDW35\\Liu et al. - 2019 - An Ensemble Model Based on Adaptive Noise Reducer .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZBB5NA77\\8648338.html}
}

@article{liu_machine_2020,
  title = {A Machine Learning Methodology for Real-Time Forecasting of the 2019-2020 {{COVID-19}} Outbreak Using {{Internet}} Searches, News Alerts, and Estimates from Mechanistic Models},
  author = {Liu, Dianbo and Clemente, Leonardo and Poirier, Canelle and Ding, Xiyu and Chinazzi, Matteo and Davis, Jessica T. and Vespignani, Alessandro and Santillana, Mauricio},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.04019 [cs, q-bio, stat]},
  eprint = {2004.04019},
  primaryclass = {cs, q-bio, stat},
  urldate = {2020-05-12},
  abstract = {We present a timely and novel methodology that combines disease estimates from mechanistic models with digital traces, via interpretable machine-learning methodologies, to reliably forecast COVID-19 activity in Chinese provinces in real-time. Specifically, our method is able to produce stable and accurate forecasts 2 days ahead of current time, and uses as inputs (a) official health reports from Chinese Center Disease for Control and Prevention (China CDC), (b) COVID-19-related internet search activity from Baidu, (c) news media activity reported by Media Cloud, and (d) daily forecasts of COVID-19 activity from GLEAM, an agent-based mechanistic model. Our machine-learning methodology uses a clustering technique that enables the exploitation of geo-spatial synchronicities of COVID-19 activity across Chinese provinces, and a data augmentation technique to deal with the small number of historical disease activity observations, characteristic of emerging outbreaks. Our model's predictive power outperforms a collection of baseline models in 27 out of the 32 Chinese provinces, and could be easily extended to other geographies currently affected by the COVID-19 outbreak to help decision makers.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Populations and Evolution,Statistics - Machine Learning,Statistics - Other Statistics},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PN33KUDT\Liu et al. - 2020 - A machine learning methodology for real-time forec.pdf}
}

@article{liu_numericalbased_2019,
  title = {A {{Numerical-Based Attention Method}} for {{Stock Market Prediction With Dual Information}}},
  author = {Liu, G. and Wang, X.},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {7357--7367},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2886367},
  abstract = {Stock market prediction is of great importance for financial analysis. Traditionally, many studies only use the news or numerical data for the stock market prediction. In the recent years, in order to explore their complementary, some studies have been conducted to equally treat dual sources of information. However, numerical data often play a much more important role compared with the news. In addition, the existing simple combination cannot exploit their complementarity. In this paper, we propose a numerical-based attention (NBA) method for dual sources stock market prediction. Our major contributions are summarized as follows. First, we propose an attention-based method to effectively exploit the complementarity between news and numerical data in predicting the stock prices. The stock trend information hidden in the news is transformed into the importance distribution of numerical data. Consequently, the news is encoded to guide the selection of numerical data. Our method can effectively filter the noise and make full use of the trend information in news. Then, in order to evaluate our NBA model, we collect news corpus and numerical data to build three datasets from two sources: the China Security Index 300 (CSI300) and the Standard \& Poor's 500 (S\&P500). Extensive experiments are conducted, showing that our NBA is superior to previous models in dual sources stock price prediction.},
  keywords = {Decoding,Deep learning,Indexes,machine learning,Market research,natural language processing,Numerical models,prediction methods,Predictive models,read - not in related work,stock markets,Stock markets},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HDIEBSQV\\Liu and Wang - 2019 - A Numerical-Based Attention Method for Stock Marke.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WUJ77SBX\\8573780.html}
}

@article{liu_urban_2016,
  title = {Urban {{Water Quality Prediction Based}} on {{Multi-task Multi-view Learning}}},
  author = {Liu, Ye and Zheng, Yu and Liang, Yuxuan and Liu, Shuming and Rosenblum, David S},
  year = {2016},
  journal = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16)},
  pages = {7},
  urldate = {2019-04-15},
  abstract = {Urban water quality is of great importance to our daily lives. Prediction of urban water quality help control water pollution and protect human health. In this work, we forecast the water quality of a station over the next few hours, using a multitask multi-view learning method to fuse multiple datasets from different domains. In particular, our learning model comprises two alignments. The first alignment is the spaio-temporal view alignment, which combines local spatial and temporal information of each station. The second alignment is the prediction alignment among stations, which captures their spatial correlations and performs copredictions by incorporating these correlations. Extensive experiments on real-world datasets demonstrate the effectiveness of our approach.},
  langid = {english},
  keywords = {read - not in related work},
  annotation = {00144},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\9AMXMS6X\Liu et al. - Urban Water Quality Prediction Based on Multi-task.pdf}
}

@article{lu_accelerating_,
  title = {Accelerating {{Gradient Boosting Machine}}},
  author = {Lu, Haihao and Karimireddy, Sai Praneeth and Ponomareva, Natalia and Mirrokni, Vahab},
  pages = {17},
  abstract = {Gradient Boosting Machine (GBM) (Friedman, 2001) is an extremely powerful supervised learning algorithm that is widely used in practice. GBM routinely features as a leading algorithm in machine learning competitions such as Kaggle and the KDDCup. In this work, we propose Accelerated Gradient Boosting Machine (AGBM) by incorporating Nesterov's acceleration techniques into the design of GBM. The difficulty in accelerating GBM lies in the fact that weak (inexact) learners are commonly used, and therefore the errors can accumulate in the momentum term. To overcome it, we design a ``corrected pseudo residual'' and fit best weak learner to this corrected pseudo residual, in order to perform the z-update. Thus, we are able to derive novel computational guarantees for AGBM. This is the first GBM type of algorithm with theoretically-justified accelerated convergence rate. Finally we demonstrate with a number of numerical experiments the effectiveness of AGBM over conventional GBM in obtaining a model with good training and/or testing data fidelity.},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\GXSVH2SQ\Lu et al. - Accelerating Gradient Boosting Machine.pdf}
}

@article{lu_deeppink_2018,
  title = {{{DeepPINK}}: Reproducible Feature Selection in Deep Neural Networks},
  shorttitle = {{{DeepPINK}}},
  author = {Lu, Yang Young and Fan, Yingying and Lv, Jinchi and Noble, William Stafford},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.01185 [cs, stat]},
  eprint = {1809.01185},
  primaryclass = {cs, stat},
  urldate = {2019-07-30},
  abstract = {Deep learning has become increasingly popular in both supervised and unsupervised machine learning thanks to its outstanding empirical performance. However, because of their intrinsic complexity, most deep learning methods are largely treated as black box tools with little interpretability. Even though recent attempts have been made to facilitate the interpretability of deep neural networks (DNNs), existing methods are susceptible to noise and lack of robustness. Therefore, scientists are justifiably cautious about the reproducibility of the discoveries, which is often related to the interpretability of the underlying statistical models. In this paper, we describe a method to increase the interpretability and reproducibility of DNNs by incorporating the idea of feature selection with controlled error rate. By designing a new DNN architecture and integrating it with the recently proposed knockoffs framework, we perform feature selection with a controlled error rate, while maintaining high power. This new method, DeepPINK (Deep feature selection using Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data sets to demonstrate its empirical utility.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZVG28V7Y\Lu et al. - 2018 - DeepPINK reproducible feature selection in deep n.pdf}
}

@misc{lundberg_interpretable_2018,
  title = {Interpretable {{Machine Learning}} with {{XGBoost}}},
  author = {Lundberg, Scott},
  year = {2018},
  month = apr,
  journal = {Towards Data Science},
  urldate = {2019-04-18},
  abstract = {This is a story about the danger of interpreting your machine learning model incorrectly, and the value of interpreting it correctly\ldots},
  howpublished = {https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\9PBPFL7N\interpretable-machine-learning-with-xgboost-9ec80d148d27.html}
}

@incollection{lundberg_unified_2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Lundberg, Scott M and Lee, Su-In},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {4765--4774},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-18},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YI6BWPD3\\Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6TZNBC46\\7062-a-unified-approach-to-interpreting-model-predictions.html}
}

@inproceedings{luong_effective_2015,
  title = {Effective {{Approaches}} to {{Attention-based Neural Machine Translation}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  pages = {1412--1421},
  publisher = {{Association for Computational Linguistics}},
  address = {{Lisbon, Portugal}},
  doi = {10.18653/v1/D15-1166},
  urldate = {2019-04-11},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5GB2RMMR\Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf}
}

@inproceedings{lutz_pump_2020,
  title = {Pump {{Up}} the {{Volume}}: {{Processing Large Data}} on {{GPUs}} with {{Fast Interconnects}}},
  shorttitle = {Pump {{Up}} the {{Volume}}},
  booktitle = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Lutz, Clemens and Bre{\ss}, Sebastian and Zeuch, Steffen and Rabl, Tilmann and Markl, Volker},
  year = {2020},
  month = jun,
  pages = {1633--1649},
  publisher = {{ACM}},
  address = {{Portland OR USA}},
  doi = {10.1145/3318464.3389705},
  urldate = {2020-11-19},
  isbn = {978-1-4503-6735-6},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TD2T9PNJ\Lutz et al. - 2020 - Pump Up the Volume Processing Large Data on GPUs .pdf}
}

@article{lv_forest_2019,
  title = {Forest {{Representation Learning Guided}} by {{Margin Distribution}}},
  author = {Lv, Shen-Huan and Yang, Liang and Zhou, Zhi-Hua},
  year = {2019},
  month = may,
  journal = {arXiv:1905.03052 [cs, stat]},
  eprint = {1905.03052},
  primaryclass = {cs, stat},
  urldate = {2019-08-30},
  abstract = {In this paper, we reformulate the forest representation learning approach as an additive model which boosts the augmented feature instead of the prediction. We substantially improve the upper bound of generalization gap from \$\textbackslash mathcal\{O\}(\textbackslash sqrt\textbackslash frac\{\textbackslash ln m\}\{m\})\$ to \$\textbackslash mathcal\{O\}(\textbackslash frac\{\textbackslash ln m\}\{m\})\$, while \$\textbackslash lambda\$ - the margin ratio between the margin standard deviation and the margin mean is small enough. This tighter upper bound inspires us to optimize the margin distribution ratio \$\textbackslash lambda\$. Therefore, we design the margin distribution reweighting approach (mdDF) to achieve small ratio \$\textbackslash lambda\$ by boosting the augmented feature. Experiments and visualizations confirm the effectiveness of the approach in terms of performance and representation learning ability. This study offers a novel understanding of the cascaded deep forest from the margin-theory perspective and further uses the mdDF approach to guide the layer-by-layer forest representation learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Q9CAIMHZ\\Lv et al. - 2019 - Forest Representation Learning Guided by Margin Di.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ML6BC8PU\\1905.html}
}

@article{ma_cdsa_2019,
  title = {{{CDSA}}: {{Cross-Dimensional Self-Attention}} for {{Multivariate}}, {{Geo-tagged Time Series Imputation}}},
  shorttitle = {{{CDSA}}},
  author = {Ma, Jiawei and Shou, Zheng and Zareian, Alireza and Mansour, Hassan and Vetro, Anthony and Chang, Shih-Fu},
  year = {2019},
  month = aug,
  journal = {arXiv:1905.09904 [cs, stat]},
  eprint = {1905.09904},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {Many real-world applications involve multivariate, geo-tagged time series data: at each location, multiple sensors record corresponding measurements. For example, air quality monitoring system records PM2.5, CO, etc. The resulting time-series data often has missing values due to device outages or communication errors. In order to impute the missing values, state-of-the-art methods are built on Recurrent Neural Networks (RNN), which process each time stamp sequentially, prohibiting the direct modeling of the relationship between distant time stamps. Recently, the self-attention mechanism has been proposed for sequence modeling tasks such as machine translation, significantly outperforming RNN because the relationship between each two time stamps can be modeled explicitly. In this paper, we are the first to adapt the self-attention mechanism for multivariate, geo-tagged time series data. In order to jointly capture the self-attention across multiple dimensions, including time, location and the sensor measurements, while maintain low computational complexity, we propose a novel approach called Cross-Dimensional Self-Attention (CDSA) to process each dimension sequentially, yet in an order-independent manner. Our extensive experiments on four real-world datasets, including three standard benchmarks and our newly collected NYC-traffic dataset, demonstrate that our approach outperforms the state-of-the-art imputation and forecasting methods. A detailed systematic analysis confirms the effectiveness of our design choices.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\SA648F4T\\Ma et al. - 2019 - CDSA Cross-Dimensional Self-Attention for Multiva.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\QK2X9FVN\\1905.html}
}

@article{ma_cdsa_2019a,
  title = {{{CDSA}}: {{Cross-Dimensional Self-Attention}} for {{Multivariate}}, {{Geo-tagged Time Series Imputation}}},
  shorttitle = {{{CDSA}}},
  author = {Ma, Jiawei and Shou, Zheng and Zareian, Alireza and Mansour, Hassan and Vetro, Anthony and Chang, Shih-Fu},
  year = {2019},
  month = aug,
  journal = {arXiv:1905.09904 [cs, stat]},
  eprint = {1905.09904},
  primaryclass = {cs, stat},
  urldate = {2019-11-12},
  abstract = {Many real-world applications involve multivariate, geo-tagged time series data: at each location, multiple sensors record corresponding measurements. For example, air quality monitoring system records PM2.5, CO, etc. The resulting time-series data often has missing values due to device outages or communication errors. In order to impute the missing values, state-of-the-art methods are built on Recurrent Neural Networks (RNN), which process each time stamp sequentially, prohibiting the direct modeling of the relationship between distant time stamps. Recently, the self-attention mechanism has been proposed for sequence modeling tasks such as machine translation, significantly outperforming RNN because the relationship between each two time stamps can be modeled explicitly. In this paper, we are the first to adapt the self-attention mechanism for multivariate, geo-tagged time series data. In order to jointly capture the self-attention across multiple dimensions, including time, location and the sensor measurements, while maintain low computational complexity, we propose a novel approach called Cross-Dimensional Self-Attention (CDSA) to process each dimension sequentially, yet in an order-independent manner. Our extensive experiments on four real-world datasets, including three standard benchmarks and our newly collected NYC-traffic dataset, demonstrate that our approach outperforms the state-of-the-art imputation and forecasting methods. A detailed systematic analysis confirms the effectiveness of our design choices.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VW3BDL8J\Ma et al. - 2019 - CDSA Cross-Dimensional Self-Attention for Multiva.pdf}
}

@misc{ma_histgnn_2022,
  title = {{{HiSTGNN}}: {{Hierarchical Spatio-temporal Graph Neural Networks}} for {{Weather Forecasting}}},
  shorttitle = {{{HiSTGNN}}},
  author = {Ma, Minbo and Xie, Peng and Teng, Fei and Li, Tianrui and Wang, Bin and Ji, Shenggong and Zhang, Junbo},
  year = {2022},
  month = jan,
  number = {arXiv:2201.09101},
  eprint = {2201.09101},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2201.09101},
  urldate = {2022-05-25},
  abstract = {Weather Forecasting is an attractive challengeable task due to its influence on human life and complexity in atmospheric motion. Supported by massive historical observed time series data, the task is suitable for data-driven approaches, especially deep neural networks. Recently, the Graph Neural Networks (GNNs) based methods have achieved excellent performance for spatio-temporal forecasting. However, the canonical GNNs-based methods only individually model the local graph of meteorological variables per station or the global graph of whole stations, lacking information interaction between meteorological variables in different stations. In this paper, we propose a novel Hierarchical Spatio-Temporal Graph Neural Network (HiSTGNN) to model cross-regional spatio-temporal correlations among meteorological variables in multiple stations. An adaptive graph learning layer and spatial graph convolution are employed to construct self-learning graph and study hidden dependency among nodes of variable-level and station-level graph. For capturing temporal pattern, the dilated inception as the backbone of gate temporal convolution is designed to model long and various meteorological trends. Moreover, a dynamic interaction learning is proposed to build bidirectional information passing in hierarchical graph. Experimental results on three real-world meteorological datasets demonstrate the superior performance of HiSTGNN beyond 7 baselines and it reduces the errors by 4.2\% to 11.6\% especially compared to state-of-the-art weather forecasting method.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\P682JKYJ\\Ma et al. - 2022 - HiSTGNN Hierarchical Spatio-temporal Graph Neural.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PAUTLUZ8\\2201.html}
}

@article{maddix_deep_2018,
  title = {Deep {{Factors}} with {{Gaussian Processes}} for {{Forecasting}}},
  author = {Maddix, Danielle C. and Wang, Yuyang and Smola, Alex},
  year = {2018},
  month = nov,
  journal = {arXiv:1812.00098 [cs, stat]},
  eprint = {1812.00098},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {A large collection of time series poses significant challenges for classical and neural forecasting approaches. Classical time series models fail to fit data well and to scale to large problems, but succeed at providing uncertainty estimates. The converse is true for deep neural networks. In this paper, we propose a hybrid model that incorporates the benefits of both approaches. Our new method is data-driven and scalable via a latent, global, deep component. It also handles uncertainty through a local classical Gaussian Process model. Our experiments demonstrate that our method obtains higher accuracy than state-of-the-art methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\M2Q7PF5S\\Maddix et al. - 2018 - Deep Factors with Gaussian Processes for Forecasti.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HKA8EV4Z\\1812.html}
}

@article{mainassara_estimation_2019,
  title = {Estimation of Multivariate Asymmetric Power {{GARCH}} Models},
  author = {Ma{\"i}nassara, Yacouba Boubacar and Kadmiri, Othman and Saussereau, Bruno},
  year = {2019},
  month = oct,
  journal = {arXiv:1812.02061 [math, stat]},
  eprint = {1812.02061},
  primaryclass = {math, stat},
  urldate = {2020-02-19},
  abstract = {It is now widely accepted that volatility models have to incorporate the so-called leverage effect in order to to model the dynamics of daily financial returns.We suggest a new class of multivariate power transformed asymmetric models. It includes several functional forms of multivariate GARCH models which are of great interest in financial modeling and time series literature. We provide an explicit necessary and sufficient condition to establish the strict stationarity of the model. We derive the asymptotic properties of the quasi-maximum likelihood estimator of the parameters. These properties are established both when the power of the transformation is known or is unknown. The asymptotic results are illustrated by Monte Carlo experiments. An application to real financial data is also proposed.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8HSV7CJL\\MaÃ¯nassara et al. - 2019 - Estimation of multivariate asymmetric power GARCH .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\P6BXRAKC\\1812.html}
}

@article{makridakis_m4_2020,
  title = {The {{M4 Competition}}: 100,000 Time Series and 61 Forecasting Methods},
  shorttitle = {The {{M4 Competition}}},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  year = {2020},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {M4 {{Competition}}},
  volume = {36},
  number = {1},
  pages = {54--74},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.04.014},
  urldate = {2020-03-12},
  abstract = {The M4 Competition follows on from the three previous M competitions, the purpose of which was to learn from empirical evidence both how to improve the forecasting accuracy and how such learning could be used to advance the theory and practice of forecasting. The aim of M4 was to replicate and extend the three previous competitions by: (a) significantly increasing the number of series, (b) expanding the number of forecasting methods, and (c) including prediction intervals in the evaluation process as well as point forecasts. This paper covers all aspects of M4 in detail, including its organization and running, the presentation of its results, the top-performing methods overall and by categories, its major findings and their implications, and the computational requirements of the various methods. Finally, it summarizes its main conclusions and states the expectation that its series will become a testing ground for the evaluation of new methods and the improvement of the practice of forecasting, while also suggesting some ways forward for the field.},
  langid = {english},
  keywords = {Benchmarking methods,Forecasting accuracy,Forecasting competitions,M competitions,Machine learning methods,Practice of forecasting,Prediction intervals,Time series methods},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YQTYVVCP\\Makridakis et al. - 2020 - The M4 Competition 100,000 time series and 61 for.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\JEZ296MV\\S0169207019301128.html}
}

@book{makridakis_m5_2020,
  title = {The {{M5 Accuracy}} Competition: {{Results}}, Findings and Conclusions},
  shorttitle = {The {{M5 Accuracy}} Competition},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilis},
  year = {2020},
  month = oct,
  abstract = {This paper describes the M5 Accuracy competition, the first of two parallel challenges of the latest M competition whose objective is to advance the theory and practice of forecasting. The M5 Accuracy competition focused on a retail sales forecasting application and extended the results of the previous four competitions by: (a) significantly expanding the number of participating methods, especially those in the category of Machine Learning, (b) including exogenous/explanatory variables in addition to the time series data, (c) using grouped, correlated time series, and (d) focusing on series that display intermittency. The paper presents the background, design, and implementation of the competition, its results, the top-performing methods, as well as its major findings and conclusions. It also discusses the implications of its findings and suggests directions for future research.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7A6WRP9A\Makridakis et al. - 2020 - The M5 Accuracy competition Results, findings and.pdf}
}

@book{makridakis_m5_2020a,
  title = {The {{M5 Uncertainty}} Competition: {{Results}}, Findings and Conclusions},
  shorttitle = {The {{M5 Uncertainty}} Competition},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilis and Chen, Zhi and Gaba, Anil and Tsetlin, Ilia and Winkler, Robert},
  year = {2020},
  month = nov,
  abstract = {This paper describes the M5 "Uncertainty'' competition, the second of two parallel challenges of the latest M competition whose aim is to advance the theory and practice of forecasting. The particular objective of the M5 "Uncertainty'' competition was to precisely estimate the uncertainty distribution of the realized values of 42,840 time series that represent the hierarchical unit sales of the largest retail company in the world by revenue, Walmart. To do so, the competition required the estimation of nine different quantiles, namely the median and the 50\%, 67\%, 95\%, and 99\% prediction intervals that can sufficiently approximate the complete distribution of future sales. The paper provides details on the implementation and execution of the M5 "Uncertainty'' competition, presents its results and the top performing methods, and summarizes its major findings and conclusions. Finally, it discusses the implications of its findings and suggests directions for future research.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\AJFH55GE\Makridakis et al. - 2020 - The M5 Uncertainty competition Results, findings .pdf}
}

@article{makridakis_m5_2021,
  title = {The {{M5}} Competition: {{Background}}, Organization, and Implementation},
  shorttitle = {The {{M5}} Competition},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  year = {2021},
  month = sep,
  journal = {International Journal of Forecasting},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.07.007},
  urldate = {2021-10-18},
  abstract = {The M5 competition follows the previous four M competitions, whose purpose is to learn from empirical evidence how to improve forecasting performance and advance the theory and practice of forecasting. M5 focused on a retail sales forecasting application with the objective to produce the most accurate point forecasts for 42,840 time series that represent the hierarchical unit sales of the largest retail company in the world, Walmart, as well as to provide the most accurate estimates of the uncertainty of these forecasts. Hence, the competition consisted of two parallel challenges, namely the Accuracy and Uncertainty forecasting competitions. M5 extended the results of the previous M competitions by: (a) significantly expanding the number of participating methods, especially those in the category of machine learning; (b) evaluating the performance of the uncertainty distribution along with point forecast accuracy; (c) including exogenous/explanatory variables in addition to the time series data; (d) using grouped, correlated time series; and (e) focusing on series that display intermittency. This paper describes the background, organization, and implementations of the competition, and it presents the data used and their characteristics. Consequently, it serves as introductory material to the results of the two forecasting challenges to facilitate their understanding.},
  langid = {english},
  keywords = {Accuracy,Forecasting competitions,M competitions,Retail sales forecasting,Time series,Uncertainty},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\KLKVPL6D\\Makridakis et al. - 2021 - The M5 competition Background, organization, and .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MIBP3STK\\S0169207021001187.html}
}

@article{makridakis_m5_2022,
  title = {M5 Accuracy Competition: {{Results}}, Findings, and Conclusions},
  shorttitle = {M5 Accuracy Competition},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  year = {2022},
  month = oct,
  journal = {International Journal of Forecasting},
  series = {Special {{Issue}}: {{M5}} Competition},
  volume = {38},
  number = {4},
  pages = {1346--1364},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.11.013},
  urldate = {2023-06-05},
  abstract = {In this study, we present the results of the M5 ``Accuracy'' competition, which was the first of two parallel challenges in the latest M competition with the aim of advancing the theory and practice of forecasting. The main objective in the M5 ``Accuracy'' competition was to accurately predict 42,840 time series representing the hierarchical unit sales for the largest retail company in the world by revenue, Walmart. The competition required the submission of 30,490 point forecasts for the lowest cross-sectional aggregation level of the data, which could then be summed up accordingly to estimate forecasts for the remaining upward levels. We provide details of the implementation of the M5 ``Accuracy'' challenge, as well as the results and best performing methods, and summarize the major findings and conclusions. Finally, we discuss the implications of these findings and suggest directions for future research.},
  langid = {english},
  keywords = {Accuracy,Forecasting competitions,M competitions,Machine learning,Retail sales forecasting,Time series},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZAFJXYYT\Makridakis et al. - 2022 - M5 accuracy competition Results, findings, and co.pdf}
}

@article{makridakis_statistical_2018,
  title = {Statistical and {{Machine Learning}} Forecasting Methods: {{Concerns}} and Ways Forward},
  shorttitle = {Statistical and {{Machine Learning}} Forecasting Methods},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  year = {2018},
  month = mar,
  journal = {PLOS ONE},
  volume = {13},
  number = {3},
  pages = {e0194889},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0194889},
  urldate = {2019-09-16},
  abstract = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
  langid = {english},
  keywords = {Algorithms,Computing methods,Forecasting,Neural networks,Optimization,Preprocessing,Statistical methods,Support vector machines},
  annotation = {00081},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DSNLV95D\\Makridakis et al. - 2018 - Statistical and Machine Learning forecasting metho.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Z2PAUTQ5\\article.html}
}

@article{malinin_uncertainty_2021,
  title = {Uncertainty in {{Gradient Boosting}} via {{Ensembles}}},
  author = {Malinin, Andrey and Prokhorenkova, Liudmila and Ustimenko, Aleksei},
  year = {2021},
  month = apr,
  journal = {arXiv:2006.10562 [cs, stat]},
  eprint = {2006.10562},
  primaryclass = {cs, stat},
  urldate = {2021-10-09},
  abstract = {For many practical, high-risk applications, it is essential to quantify uncertainty in a model's predictions to avoid costly mistakes. While predictive uncertainty is widely studied for neural networks, the topic seems to be under-explored for models based on gradient boosting. However, gradient boosting often achieves state-ofthe-art results on tabular data. This work examines a probabilistic ensemblebased framework for deriving uncertainty estimates in the predictions of gradient boosting classification and regression models. We conducted experiments on a range of synthetic and real datasets and investigated the applicability of ensemble approaches to gradient boosting models that are themselves ensembles of decision trees. Our analysis shows that ensembles of gradient boosting models successfully detect anomalous inputs while having limited ability to improve the predicted total uncertainty. Importantly, we also propose a concept of a virtual ensemble to get the benefits of an ensemble via only one gradient boosting model, which significantly reduces complexity.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3P5XG492\Malinin et al. - 2021 - Uncertainty in Gradient Boosting via Ensembles.pdf}
}

@book{mallick_dynamic_2020,
  title = {Dynamic {{Graph Neural Network}} for {{Traffic Forecasting}} in {{Wide Area Networks}}},
  author = {Mallick, Tanwi and Kiran, Mariam and Mohammed, Bashir and Balaprakash, Prasanna},
  year = {2020},
  month = aug,
  abstract = {Wide area networking infrastructures (WANs), particularly science and research WANs, are the backbone for moving large volumes of scientific data between experimental facilities and data centers. With demands growing at exponential rates, these networks are struggling to cope with large data volumes, real-time responses, and overall network performance. Network operators are increasingly looking for innovative ways to manage the limited underlying network resources. Forecasting network traffic is a critical capability for proactive resource management, congestion mitigation, and dedicated transfer provisioning. To this end, we propose a nonautoregressive graph-based neural network for multistep network traffic forecasting. Specifically, we develop a dynamic variant of diffusion convolutional recurrent neural networks to forecast traffic in research WANs. We evaluate the efficacy of our approach on real traffic from ESnet, the U.S. Department of Energy's dedicated science network. Our results show that compared to classical forecasting methods, our approach explicitly learns the dynamic nature of spatiotemporal traffic patterns, showing significant improvements in forecasting accuracy. Our technique can surpass existing statistical and deep learning approaches by achieving approximately 20\% mean absolute percentage error for multiple hours of forecasts despite dynamic network traffic settings.}
}

@article{mancuso_machine_2021,
  title = {A Machine Learning Approach for Forecasting Hierarchical Time Series},
  author = {Mancuso, Paolo and Piccialli, Veronica and Sudoso, Antonio M.},
  year = {2021},
  month = nov,
  journal = {Expert Systems with Applications},
  volume = {182},
  pages = {115102},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2021.115102},
  urldate = {2021-10-18},
  abstract = {In this paper, we propose a machine learning approach for forecasting hierarchical time series. When dealing with hierarchical time series, apart from generating accurate forecasts, one needs to select a suitable method for producing reconciled forecasts. Forecast reconciliation is the process of adjusting forecasts to make them coherent across the hierarchy. In literature, coherence is often enforced by using a post-processing technique on the base forecasts produced by suitable time series forecasting methods. On the contrary, our idea is to use a deep neural network to directly produce accurate and reconciled forecasts. We exploit the ability of a deep neural network to extract information capturing the structure of the hierarchy. We impose the reconciliation at training time by minimizing a customized loss function. In many practical applications, besides time series data, hierarchical time series include explanatory variables that are beneficial for increasing the forecasting accuracy. Exploiting this further information, our approach links the relationship between time series features extracted at any level of the hierarchy and the explanatory variables into an end-to-end neural network providing accurate and reconciled point forecasts. The effectiveness of the approach is validated on three real-world datasets, where our method outperforms state-of-the-art competitors in hierarchical forecasting.},
  langid = {english},
  keywords = {Deep neural network,Forecast,Hierarchical time series,Machine learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BRPL6NJB\\Mancuso et al. - 2021 - A machine learning approach for forecasting hierar.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\G4TRG4MZ\\S0957417421005431.html}
}

@inproceedings{manzoor_rush_2017,
  title = {{{RUSH}}!: {{Targeted Time-limited Coupons}} via {{Purchase Forecasts}}},
  shorttitle = {{{RUSH}}!},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}  - {{KDD}} '17},
  author = {Manzoor, Emaad and Akoglu, Leman},
  year = {2017},
  pages = {1923--1931},
  publisher = {{ACM Press}},
  address = {{Halifax, NS, Canada}},
  doi = {10.1145/3097983.3098104},
  urldate = {2019-04-17},
  abstract = {Time-limited promotions that exploit consumers' sense of urgency to boost sales account for billions of dollars in consumer spending each year. However, it is challenging to discover the right timing and duration of a promotion to increase its chances of being redeemed. In this work, we consider the problem of delivering time-limited discount coupons, where we partner with a large national bank functioning as a commission-based third-party coupon provider. Speci cally, we use large-scale anonymized transaction records to model consumer spending and forecast future purchases, based on which we generate data-driven, personalized coupons. Our proposed model RUSH! (1) predicts both the time and category of the next event; (2) captures correlations between purchases in di erent categories (such as shopping triggering dining purchases); (3) incorporates temporal dynamics of purchase behavior (such as increased spending on weekends); (4) is composed of additive factors that are easily interpretable; and nally (5) scales linearly to millions of transactions. We design a cost-bene t framework that facilitates systematic evaluation in terms of our application, and show that RUSH! provides higher expected value than various baselines that do not jointly model time and category information.},
  isbn = {978-1-4503-4887-4},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6CMZAL3X\Manzoor and Akoglu - 2017 - RUSH! Targeted Time-limited Coupons via Purchase .pdf}
}

@article{mao_recursive_2018,
  title = {Recursive {{Particle Filter-Based RBF Network}} on {{Time Series Prediction}} of {{Measurement Data}}},
  author = {Mao, Wei-Lung},
  year = {2018},
  month = oct,
  journal = {Neural Processing Letters},
  issn = {1573-773X},
  doi = {10.1007/s11063-018-9933-2},
  urldate = {2019-04-03},
  abstract = {Most physical phenomena in nature are chaotic or close to chaos. An irregular time series can be generated or measured with a purely deterministic equation of motion in nonlinear and chaotic systems. This paper presents an adaptive state-space particle filtering (PF)-based trained radial basis function (RBF) network for chaotic and nonstationary observation\textendash prediction. The recursive Bayesian filtering algorithm, which uses the particle representation of density function, is adopted to accomplish nonlinear and non-Gaussian state estimation and achieve improved convergence rate and quality of solution. Four sampling importance resampling approaches, namely, multinomial, systematic, stratified, and residual resampling methods, are considered to resolve weight degeneracy. The effectiveness of our proposed methods is investigated using two chaotic time series and three measurement datasets, including the Mackey\textendash Glass time series, Rossler time series, monthly Lake Erie levels, monthly water usage series, and SML2010 data set. The performances are evaluated through an extensive simulation by computing the average mean square error, mean absolute percentage error, and average relative variance metrics. Simulation results show that the proposed PF-based RBF structure can provide more effective and accurate prediction performances compared with the conventional gradient descent, extended Kalman filter (EKF), and decoupled EKF algorithms (DEKF).},
  langid = {english},
  keywords = {Extended Kalman filtering (EKF) algorithm,Measurement data,Particle filtering (PF) algorithm,Radial basis function (RBF) network,Sampling importance resampling (SIR) algorithm,Time series prediction},
  annotation = {00000}
}

@inproceedings{mariet_foundations_2019,
  title = {Foundations of {{Sequence-to-Sequence Modeling}} for {{Time Series}}},
  booktitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Mariet, Zelda and Kuznetsov, Vitaly},
  year = {2019},
  month = apr,
  pages = {408--417},
  urldate = {2019-04-25},
  abstract = {The availability of large amounts of time series data, paired with the performance of deep-learning algorithms on a broad class of problems, has recently led to significant interest in the use of s...},
  langid = {english},
  keywords = {read - in related work},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5SRWDLVK\\Mariet and Kuznetsov - 2019 - Foundations of Sequence-to-Sequence Modeling for T.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\E2VFDUMJ\\mariet19a.html}
}

@article{marteau_separation_2019,
  title = {On the Separation of Shape and Temporal Patterns in Time Series -{{Application}} to Signature Authentication-},
  author = {Marteau, Pierre-Fran{\c c}ois},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.09360 [cs, eess]},
  eprint = {1911.09360},
  primaryclass = {cs, eess},
  urldate = {2020-02-17},
  abstract = {In this article we address the problem of separation of shape and time components in time series. The concept ofshape that we tackle is termed temporally neutral to consider that it may possibly exist outside of any temporal specification, as it is the case for a geometric form. We propose to exploit and adapt a probabilistic temporal alignment algorithm, initially designed to estimate the centroid of a set of time series, to build some heuristicelements of solution to this separation problem. We show on some controlled synthetic data that this algorithm meets empirically our initial objectives. We finally evaluate it on real data, in the context of some on-line handwritten signature authentication benchmarks. On the three evaluated tasks, our approach based on the separation of signature shape and associated temporal patterns is positioned slightly above the current state of the art demonstrating the applicative benefit of this separating problem.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Discrete Mathematics,Computer Science - Information Retrieval,Electrical Engineering and Systems Science - Signal Processing},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EQHBIV2Z\\Marteau - 2019 - On the separation of shape and temporal patterns i.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ST85GCAM\\1911.html}
}

@misc{masserano_adaptive_2023,
  title = {Adaptive {{Sampling}} for {{Probabilistic Forecasting}} under {{Distribution Shift}}},
  author = {Masserano, Luca and Rangapuram, Syama Sundar and Kapoor, Shubham and Nirwan, Rajbir Singh and Park, Youngsuk and {Bohlke-Schneider}, Michael},
  year = {2023},
  month = feb,
  number = {arXiv:2302.11870},
  eprint = {2302.11870},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-31},
  abstract = {The world is not static: This causes real-world time series to change over time through external, and potentially disruptive, events such as macroeconomic cycles or the COVID-19 pandemic. We present an adaptive sampling strategy that selects the part of the time series history that is relevant for forecasting. We achieve this by learning a discrete distribution over relevant time steps by Bayesian optimization. We instantiate this idea with a two-step method that is pre-trained with uniform sampling and then training a lightweight adaptive architecture with adaptive sampling. We show with synthetic and real-world experiments that this method adapts to distribution shift and significantly reduces the forecasting error of the base model for three out of five datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9ZIVVTFT\\Masserano et al. - 2023 - Adaptive Sampling for Probabilistic Forecasting un.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IK396DIH\\2302.html}
}

@inproceedings{matsubara_dynamic_2019,
  title = {Dynamic {{Modeling}} and {{Forecasting}} of {{Time-evolving Data Streams}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}  - {{KDD}} '19},
  author = {Matsubara, Yasuko and Sakurai, Yasushi},
  year = {2019},
  pages = {458--468},
  publisher = {{ACM Press}},
  address = {{Anchorage, AK, USA}},
  doi = {10.1145/3292500.3330947},
  urldate = {2019-09-10},
  abstract = {Given a large, semi-in nite collection of co-evolving data sequences (e.g., IoT/sensor streams), which contains multiple distinct dynamic time-series patterns, our aim is to incrementally monitor current dynamic patterns and forecast future behavior. We present an intuitive model, namely O M , which provides a good summary of time-series evolution in streams. We also propose a scalable and e ective algorithm for tting and forecasting time-series data streams. Our method is designed as a dynamic, interactive and exible system, and is based on latent non-linear di erential equations. Our proposed method has the following advantages: (a) It is e ective: it captures important time-evolving patterns in data streams and enables real-time, long-range forecasting; (b) It is general: our model is general and practical and can be applied to various types of time-evolving data streams; (c) It is scalable: our algorithm does not depend on data size, and thus is applicable to very large sequences. Extensive experiments on real datasets demonstrate that O M makes long-range forecasts, and consistently outperforms the best existing state-of-the-art methods as regards accuracy and execution speed.},
  isbn = {978-1-4503-6201-6},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\N8H3ARDZ\Matsubara and Sakurai - 2019 - Dynamic Modeling and Forecasting of Time-evolving .pdf}
}

@inproceedings{matsubara_regime_2016,
  title = {Regime {{Shifts}} in {{Streams}}: {{Real-time Forecasting}} of {{Co-evolving Time Sequences}}},
  shorttitle = {Regime {{Shifts}} in {{Streams}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '16},
  author = {Matsubara, Yasuko and Sakurai, Yasushi},
  year = {2016},
  pages = {1045--1054},
  publisher = {{ACM Press}},
  address = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2939755},
  urldate = {2019-04-17},
  abstract = {Given a large, online stream of multiple co-evolving event sequences, such as sensor data and Web-click logs, that contains various types of non-linear dynamic evolving patterns of different durations, how can we efficiently and effectively capture important patterns? How do we go about forecasting long-term future events? In this paper, we present REGIMECAST, an efficient and effective method for forecasting co-evolving data streams. REGIMECAST is designed as an adaptive non-linear dynamical system, which is inspired by the concept of ``regime shifts'' in natural dynamical systems. Our method has the following properties: (a) Effective: it operates on large data streams, captures important patterns and performs long-term forecasting; (b) Adaptive: it automatically and incrementally recognizes the latent trends and dynamic evolution patterns (i.e., regimes) that are unknown in advance; (c) Scalable: it is fast and the computation cost does not depend on the length of data streams; (d) Any-time: it provides a response at any time and generates long-range future events.},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00022},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\CBHZMUY4\Matsubara and Sakurai - 2016 - Regime Shifts in Streams Real-time Forecasting of.pdf}
}

@inproceedings{matsumoto_optimal_2015,
  title = {Optimal {{Parallel Hardware K-Sorter}} and {{Top K-Sorter}}, with {{FPGA Implementations}}},
  booktitle = {2015 14th {{International Symposium}} on {{Parallel}} and {{Distributed Computing}}},
  author = {Matsumoto, Naoyuki and Nakano, Koji and Ito, Yasuaki},
  year = {2015},
  month = jun,
  pages = {138--147},
  publisher = {{IEEE}},
  address = {{Limassol, Cyprus}},
  doi = {10.1109/ISPDC.2015.23},
  urldate = {2022-02-15},
  abstract = {This paper presents a FIFO-based parallel merge sorter optimized for the latest FPGA. More specifically, we show a sorter that sorts K keys in latency K +log2 K -1 using log2 K comparators.},
  isbn = {978-1-4673-7147-6 978-1-4673-7148-3},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\SBUE3HPV\Matsumoto et al. - 2015 - Optimal Parallel Hardware K-Sorter and Top K-Sorte.pdf}
}

@article{maume-deschamps_multivariate_2017,
  title = {Multivariate Extensions of Expectiles Risk Measures},
  author = {{Maume-Deschamps}, V{\'e}ronique and Rulli{\`e}re, Didier and Said, Khalil},
  year = {2017},
  month = jan,
  journal = {Dependence Modeling},
  volume = {5},
  number = {1},
  pages = {20--44},
  publisher = {{De Gruyter Open Access}},
  issn = {2300-2298},
  doi = {10.1515/demo-2017-0002},
  urldate = {2021-11-16},
  abstract = {This paper is devoted to the introduction and study of a new family of multivariate elicitable risk measures. We call the obtained vector-valued measures multivariate expectiles. We present the different approaches used to construct our measures. We discuss the coherence properties of these multivariate expectiles. Furthermore, we propose a stochastic approximation tool of these risk measures.},
  langid = {english},
  keywords = {Capital allocation,Coherence properties,Copulas,Dependence modeling,Elicitability,Multivariate expectiles,Multivariate risk measures,Risk management,Risk theory,Solvency 2,Stochastic approximation},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\CHWDPCKE\Maume-Deschamps et al. - 2017 - Multivariate extensions of expectiles risk measure.pdf}
}

@incollection{meek_autoregressive_2002,
  title = {Autoregressive {{Tree Models}} for {{Time-Series Analysis}}},
  booktitle = {Proceedings of the 2002 {{SIAM International Conference}} on {{Data Mining}}},
  author = {Meek, C. and Chickering, D. and Heckerman, D.},
  year = {2002},
  month = apr,
  series = {Proceedings},
  pages = {229--244},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972726.14},
  urldate = {2019-09-05},
  abstract = {1 Introduction The analysis and modeling of time-series data is an important area of research for many communities. In this paper, our goal is to identify models for continuousvalued time-series data that are useful for data mining in that they (1) can be learned eficiently from data, (2) support accurate predictions, and (3) are easy to interpret. To these ends, we describe an interpretable class of models that we call AutoRegressive Tree models, or ART models, that are a generalization of standard autoregressive (AR) models. We describe learning methods for ART models and compare these methods to those for alternative models. Our experiments, performed on 2,494 time-series data sets from the International Institute of Forecasters, demonstrate that ART models provide superior predictive accuracy. We concentrate on the problem of modeling the evolution of values of a continuous variable over time; that is, we model a univariate time series. The generalization to multivariate time-series analysis is straightforward and is discussed in Section 6.},
  isbn = {978-0-89871-517-0},
  annotation = {00103},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\S5RFNXMX\\Meek et al. - 2002 - Autoregressive Tree Models for Time-Series Analysi.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\G4VVQRHD\\1.9781611972726.html}
}

@patent{meek_bayesian_2010,
  title = {Bayesian Approach for Learning Regression Decision Graph Models and Regression Models for Time Series Analysis},
  author = {Meek, Christopher A. and Heckerman, David E. and Rounthwaite, Robert L. and Chickering, David Maxwell and Thiesson, Bo},
  year = {2010},
  month = feb,
  number = {US7660705B1},
  urldate = {2019-09-05},
  assignee = {Microsoft Corp},
  nationality = {US},
  keywords = {data,decision graph,leaf,leaves,model},
  annotation = {00048},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FY4KK9PK\Meek et al. - 2010 - Bayesian approach for learning regression decision.pdf}
}

@inproceedings{mei_nonnegative_2017,
  title = {Nonnegative {{Matrix Factorization}} for {{Time Series Recovery From}} a {{Few Temporal Aggregates}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Mei, Jiali and Castro, Yohann De and Goude, Yannig and H{\'e}brail, Georges},
  year = {2017},
  month = jul,
  pages = {2382--2390},
  urldate = {2019-04-17},
  abstract = {Motivated by electricity consumption reconstitution, we propose a new matrix recovery method using nonnegative matrix factorization (NMF). The task tackled here is to reconstitute electricity consu...},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4PZYZ4CE\\Mei et al. - 2017 - Nonnegative Matrix Factorization for Time Series R.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\2GBMCEHT\\mei17a.html}
}

@article{meinshausen_quantile_2006,
  title = {Quantile {{Regression Forests}}},
  author = {Meinshausen, Nicolai},
  year = {2006},
  journal = {Journal of Machine Learning Research},
  volume = {7},
  number = {35},
  pages = {983--999},
  issn = {1533-7928},
  urldate = {2020-11-18},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3AGLVZW3\\Meinshausen - 2006 - Quantile Regression Forests.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NELVH8JV\\meinshausen06a.html}
}

@article{meng_communicationefficient_2016,
  title = {A {{Communication-Efficient Parallel Algorithm}} for {{Decision Tree}}},
  author = {Meng, Qi and Ke, Guolin and Wang, Taifeng and Chen, Wei and Ye, Qiwei and Ma, Zhi-Ming and Liu, Tie-Yan},
  year = {2016},
  month = nov,
  journal = {arXiv:1611.01276 [cs]},
  eprint = {1611.01276},
  primaryclass = {cs},
  urldate = {2019-10-03},
  abstract = {Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called Parallel Voting Decision Tree (PV-Tree), to tackle this challenge. After partitioning the training data onto a number of (e.g., M ) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-k attributes are selected from each machine according to its local data. Then, globally top-2k attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-2k attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the trade-off between accuracy and efficiency.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  annotation = {00032},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\IJ97XGJB\Meng et al. - 2016 - A Communication-Efficient Parallel Algorithm for D.pdf}
}

@article{meng_scoring_2020,
  title = {Scoring {{Functions}} for {{Multivariate Distributions}} and {{Level Sets}}},
  author = {Meng, Xiaochun and Taylor, James W. and Taieb, Souhaib Ben and Li, Siran},
  year = {2020},
  month = jun,
  journal = {arXiv:2002.09578 [math, q-fin, stat]},
  eprint = {2002.09578},
  primaryclass = {math, q-fin, stat},
  urldate = {2021-02-09},
  abstract = {Interest in predicting multivariate probability distributions is growing due to the increasing availability of rich datasets and computational developments. Scoring functions enable the comparison of forecast accuracy, and can potentially be used for estimation. A scoring function for multivariate distributions that has gained some popularity is the energy score. This is a generalization of the continuous ranked probability score (CRPS), which is widely used for univariate distributions. A little-known, alternative generalization is the multivariate CRPS (MCRPS). We propose a theoretical framework for scoring functions for multivariate distributions, which encompasses the energy score and MCRPS, as well as the quadratic score, which has also received little attention. We demonstrate how this framework can be used to generate new scores. For univariate distributions, it is well-established that the CRPS can be expressed as the integral over a quantile score. We show that, in a similar way, scoring functions for multivariate distributions can be "disintegrated" to obtain scoring functions for level sets. Using this, we present scoring functions for different types of level set, including those for densities and cumulative distributions. To compute the scoring functions, we propose a simple numerical algorithm. We illustrate our proposals using simulated and stock returns data.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory,Quantitative Finance - Statistical Finance,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HNTIQWBY\\Meng et al. - 2020 - Scoring Functions for Multivariate Distributions a.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6FQIIHDU\\2002.html}
}

@inproceedings{menick_practical_2020,
  title = {Practical {{Real Time Recurrent Learning}} with a {{Sparse Approximation}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Menick, Jacob and Elsen, Erich and Evci, Utku and Osindero, Simon and Simonyan, Karen and Graves, Alex},
  year = {2020},
  month = sep,
  urldate = {2021-01-20},
  abstract = {Recurrent neural networks are usually trained with backpropagation through time, which requires storing a complete history of network states, and prohibits updating the weights "online" (after...},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8T769HPK\\Menick et al. - 2020 - Practical Real Time Recurrent Learning with a Spar.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\X4VVLMPI\\forum.html}
}

@article{mi_variational_2018,
  title = {Variational {{Wasserstein Clustering}}},
  author = {Mi, Liang and Zhang, Wen and Gu, Xianfeng and Wang, Yalin},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.09045 [cs]},
  eprint = {1806.09045},
  primaryclass = {cs},
  urldate = {2019-10-03},
  abstract = {We propose a new clustering method based on optimal transportation. We solve optimal transportation with variational principles, and investigate the use of power diagrams as transportation plans for aggregating arbitrary domains into a fixed number of clusters. We iteratively drive centroids through target domains while maintaining the minimum clustering energy by adjusting the power diagrams. Thus, we simultaneously pursue clustering and the Wasserstein distances between the centroids and the target domains, resulting in a measure-preserving mapping. We demonstrate the use of our method in domain adaptation, remeshing, and representation learning on synthetic and real data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {00007},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\IXYKXP52\Mi et al. - 2018 - Variational Wasserstein Clustering.pdf}
}

@incollection{michalski_modeling_2014,
  title = {Modeling {{Deep Temporal Dependencies}} with {{Recurrent Grammar Cells}}""},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Michalski, Vincent and Memisevic, Roland and Konda, Kishore},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {1925--1933},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BY543YYV\\Michalski et al. - 2014 - Modeling Deep Temporal Dependencies with Recurrent.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LWH8VSGB\\5549-modeling-deep-temporal-dependencies-with-recurrent-grammar-cells.html}
}

@article{mishra_simple_2018,
  title = {A {{Simple Neural Attentive Meta-Learner}}},
  author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  year = {2018},
  month = feb,
  journal = {arXiv:1707.03141 [cs, stat]},
  eprint = {1707.03141},
  primaryclass = {cs, stat},
  urldate = {2020-01-27},
  abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BF3VCNIM\Mishra et al. - 2018 - A Simple Neural Attentive Meta-Learner.pdf}
}

@article{mnih_asynchronous_2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.01783 [cs]},
  eprint = {1602.01783},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BFLVL8QE\Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf}
}

@article{mnih_humanlevel_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2019-03-19},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VZPH8G94\Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf}
}

@article{mnih_playing_2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  journal = {arXiv:1312.5602 [cs]},
  eprint = {1312.5602},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2ZJVQIZ6\Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf}
}

@article{mo_exponent_2016,
  title = {Exponent Back Propagation Neural Network Forecasting for Financial Cross-Correlation Relationship},
  author = {Mo, Haiyan and Wang, Jun and Niu, Hongli},
  year = {2016},
  month = jul,
  journal = {Expert Systems with Applications},
  volume = {53},
  pages = {106--116},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2015.12.045},
  urldate = {2021-02-04},
  abstract = {An improved neural network is developed to predict the cross-correlations between two financial time series. In order to capture the large fluctuations of data set, an exponent back propagation neural network (EBPNN) is introduced in the present work, which information is not only processed locally in each neural unit by computing the dot product between its input vector and its weight vector, but also processed by adding the dot product between its exponential type function of the input vector and its corresponding new weight vector. The proposed prediction model improves the activation function of the neural network, and makes an approach on cross-correlations forecasting with the particular input and output variables. The empirical research is performed in testing the forecasting effect of the EBPNN model and a comparison to back propagation neural network (BPNN). The empirical results show that the EBPNN is advantageous in increasing the predicting precision.},
  langid = {english},
  keywords = {Cross-correlation,Exponential type function,Financial time series,Forecast,Neural network},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\W5LGKWMB\\Mo et al. - 2016 - Exponent back propagation neural network forecasti.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Y3IN3BHN\\S0957417416000488.html}
}

@article{monteil_deep_2019,
  title = {Deep {{Learning}} to {{Scale}} up {{Time Series Traffic Prediction}}},
  author = {Monteil, Julien and Dekusar, Anton and Gambella, Claudio and Lassoued, Yassine and Mevissen, Martin},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.13042 [cs, stat]},
  eprint = {1911.13042},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {The transport literature is dense regarding short-term traffic predictions, up to the scale of 1 hour, yet less dense for long-term traffic predictions. The transport literature is also sparse when it comes to city-scale traffic predictions, mainly because of low data availability. The main question we try to answer in this work is to which extent the approaches used for short-term prediction at a link level can be scaled up for long-term prediction at a city scale. We investigate a city-scale traffic dataset with 14 weeks of speed observations collected every 15 minutes over 1098 segments in the hypercenter of Los Angeles, California. We look at a variety of machine learning and deep learning predictors for link-based predictions, and investigate ways to make such predictors scale up for larger areas, with brute force, clustering, and model design approaches. In particular we propose a novel deep learning spatio-temporal predictor inspired from recent works on recommender systems. We discuss the potential of including spatio-temporal features into the predictors, and conclude that modelling such features can be helpful for long-term predictions, while simpler predictors achieve very satisfactory performance for link-based and short-term forecasting. The trade-off is discussed not only in terms of prediction accuracy vs prediction horizon but also in terms of training time and model sizing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HPW82M4F\\Monteil et al. - 2019 - Deep Learning to Scale up Time Series Traffic Pred.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\KRJU2M8Q\\1911.html}
}

@article{montero-manso_principles_2021,
  title = {Principles and Algorithms for Forecasting Groups of Time Series: {{Locality}} and Globality},
  shorttitle = {Principles and Algorithms for Forecasting Groups of Time Series},
  author = {{Montero-Manso}, Pablo and Hyndman, Rob J.},
  year = {2021},
  month = jun,
  journal = {International Journal of Forecasting},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.03.004},
  urldate = {2021-08-25},
  abstract = {Global methods that fit a single forecasting method to all time series in a set have recently shown surprising accuracy, even when forecasting large groups of heterogeneous time series. We provide the following contributions that help understand the potential and applicability of global methods and how they relate to traditional local methods that fit a separate forecasting method to each series: \textbullet Global and local methods can produce the same forecasts without any assumptions about similarity of the series in the set.\textbullet The complexity of local methods grows with the size of the set while it remains constant for global methods. This result supports the recent evidence and provides principles for the design of new algorithms.\textbullet In an extensive empirical study, we show that purposely na\"ive algorithms derived from these principles show outstanding accuracy. In particular, global linear models provide competitive accuracy with far fewer parameters than the simplest of local methods.},
  langid = {english},
  keywords = {Cross-learning,Forecasting,Generalization,Global,Local,Pooled regression,Time series},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PFU33HZ4\\Montero-Manso and Hyndman - 2021 - Principles and algorithms for forecasting groups o.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4PF9SIZP\\S0169207021000558.html}
}

@article{moon_dynamic_2017,
  title = {{{DYNAMIC LINEAR PANEL REGRESSION MODELS WITH INTERACTIVE FIXED EFFECTS}}},
  author = {Moon, Hyungsik Roger and Weidner, Martin},
  year = {2017},
  month = feb,
  journal = {Econometric Theory},
  volume = {33},
  number = {1},
  pages = {158--195},
  issn = {0266-4666, 1469-4360},
  doi = {10.1017/S0266466615000328},
  urldate = {2020-02-24},
  abstract = {We analyze linear panel regression models with interactive fixed effects and predetermined regressors, for example lagged-dependent variables. The first-order asymptotic theory of the least squares (LS) estimator of the regression coefficients is worked out in the limit where both the cross-sectional dimension and the number of time periods become large. We find two sources of asymptotic bias of the LS estimator: bias due to correlation or heteroscedasticity of the idiosyncratic error term, and bias due to predetermined (as opposed to strictly exogenous) regressors. We provide a bias-corrected LS estimator. We also present bias-corrected versions of the three classical test statistics (Wald, LR, and LM test) and show their asymptotic distribution is a {$\chi$}2-distribution. Monte Carlo simulations show the bias correction of the LS estimator and of the test statistics also work well for finite sample sizes.},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WQ48KM6P\\Moon and Weidner - 2017 - DYNAMIC LINEAR PANEL REGRESSION MODELS WITH INTERA.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CSQEEAHP\\CE84629C05BB652892D7B7659A1D5CD5.html}
}

@inproceedings{mueen_extracting_2016,
  title = {Extracting {{Optimal Performance}} from {{Dynamic Time Warping}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '16},
  author = {Mueen, Abdullah and Keogh, Eamonn},
  year = {2016},
  pages = {2129--2130},
  publisher = {{ACM Press}},
  address = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2945383},
  urldate = {2019-04-17},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  keywords = {read - not in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ITZUVIDB\Mueen and Keogh - 2016 - Extracting Optimal Performance from Dynamic Time W.pdf}
}

@article{mukherjee_armdn_2018,
  title = {{{ARMDN}}: {{Associative}} and {{Recurrent Mixture Density Networks}} for {{eRetail Demand Forecasting}}},
  shorttitle = {{{ARMDN}}},
  author = {Mukherjee, Srayanta and Shankar, Devashish and Ghosh, Atin and Tathawadekar, Nilam and Kompalli, Pramod and Sarawagi, Sunita and Chaudhury, Krishnendu},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.03800 [cs, stat]},
  eprint = {1803.03800},
  primaryclass = {cs, stat},
  urldate = {2019-10-11},
  abstract = {Accurate demand forecasts can help on-line retail organizations better plan their supply-chain processes. The challenge, however, is the large number of associative factors that result in large, non-stationary shifts in demand, which traditional time series and regression approaches fail to model. In this paper, we propose a Neural Network architecture called AR-MDN, that simultaneously models associative factors, time-series trends and the variance in the demand. We first identify several causal features and use a combination of feature embeddings, MLP and LSTM to represent them. We then model the output density as a learned mixture of Gaussian distributions. The AR-MDN can be trained end-to-end without the need for additional supervision. We experiment on a dataset of an year's worth of data over tens-of-thousands of products from Flipkart. The proposed architecture yields a significant improvement in forecasting accuracy when compared with existing alternatives.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00007},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\P36EKW7Y\Mukherjee et al. - 2018 - ARMDN Associative and Recurrent Mixture Density N.pdf}
}

@article{muniain_probabilistic_2019,
  title = {Probabilistic {{Forecasting}} in {{Day-Ahead Electricity Markets}}: {{Simulating Peak}} and {{Off-Peak Prices}}},
  shorttitle = {Probabilistic {{Forecasting}} in {{Day-Ahead Electricity Markets}}},
  author = {Muniain, Peru and Ziel, Florian},
  year = {2019},
  month = dec,
  journal = {arXiv:1810.08418 [econ]},
  eprint = {1810.08418},
  primaryclass = {econ},
  urldate = {2020-02-17},
  abstract = {In this paper we include dependency structures for electricity price forecasting and forecasting evaluation. We work with off-peak and peak time series from the German-Austrian day-ahead price, hence we analyze bivariate data. We first estimate the mean of the two time series, and then in a second step we estimate the residuals. The mean equation is estimated by OLS and elastic net and the residuals are estimated by maximum likelihood. Our contribution is to include a bivariate jump component on a mean reverting jump diffusion model in the residuals. The models' forecasts are evaluated using four different criteria, including the energy score to measure whether the correlation structure between the time series is properly included or not. In the results it is observed that the models with bivariate jumps provide better results with the energy score, which means that it is important to consider this structure in order to properly forecast correlated time series.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TDZ5TPLX\\Muniain and Ziel - 2019 - Probabilistic Forecasting in Day-Ahead Electricity.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\C753EN65\\1810.html}
}

@inproceedings{murthy_deep_2016,
  title = {Deep {{Decision Network}} for {{Multi-class Image Classification}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Murthy, Venkatesh N. and Singh, Vivek and Chen, Terrence and Manmatha, R. and Comaniciu, Dorin},
  year = {2016},
  month = jun,
  pages = {2240--2248},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.246},
  urldate = {2019-08-30},
  abstract = {In this paper, we present a novel Deep Decision Network (DDN) that provides an alternative approach towards building an efficient deep learning network. During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. This results in a tree-like structured network driven by the data. The proposed method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others. DDN also has the ability to make early decisions thus making it suitable for timesensitive applications. We validate DDN on two publicly available benchmark datasets: CIFAR-10 and CIFAR-100 and it yields state-of-the-art classification performance on both the datasets. The proposed algorithm has no limitations to be applied to any generic classification problems.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PSLJDU7G\Murthy et al. - 2016 - Deep Decision Network for Multi-class Image Classi.pdf}
}

@article{nan_sentiment_2020,
  title = {Sentiment and {{Knowledge Based Algorithmic Trading}} with {{Deep Reinforcement Learning}}},
  author = {Nan, Abhishek and Perumal, Anandh and Zaiane, Osmar R.},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.09403 [cs]},
  eprint = {2001.09403},
  primaryclass = {cs},
  urldate = {2020-02-19},
  abstract = {Algorithmic trading, due to its inherent nature, is a difficult problem to tackle; there are too many variables involved in the real world which make it almost impossible to have reliable algorithms for automated stock trading. The lack of reliable labelled data that considers physical and physiological factors that dictate the ups and downs of the market, has hindered the supervised learning attempts for dependable predictions. To learn a good policy for trading, we formulate an approach using reinforcement learning which uses traditional time series stock price data and combines it with news headline sentiments, while leveraging knowledge graphs for exploiting news about implicit relationships.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CANIHEC6\\Nan et al. - 2020 - Sentiment and Knowledge Based Algorithmic Trading .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DISTYKNC\\2001.html}
}

@article{nasr_comprehensive_2018,
  title = {Comprehensive {{Privacy Analysis}} of {{Deep Learning}}: {{Stand-alone}} and {{Federated Learning}} under {{Passive}} and {{Active White-box Inference Attacks}}},
  shorttitle = {Comprehensive {{Privacy Analysis}} of {{Deep Learning}}},
  author = {Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.00910 [cs, stat]},
  eprint = {1812.00910},
  primaryclass = {cs, stat},
  urldate = {2019-11-12},
  abstract = {Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We perform a comprehensive analysis of white-box privacy inference attacks on deep learning models. We measure the privacy leakage by leveraging the final model parameters as well as the parameter updates during the training and finetuning processes. We design the attacks in the stand-alone and federated settings, with respect to passive and active inference attackers, and assuming different adversary prior knowledge.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZD4V4KD2\Nasr et al. - 2018 - Comprehensive Privacy Analysis of Deep Learning S.pdf}
}

@article{nau_mathematical_,
  title = {Mathematical Structure of {{ARIMA}} Models},
  author = {Nau, Robert},
  pages = {8},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6PRZVHRD\Nau - Mathematical structure of ARIMA models.pdf}
}

@article{navon_financial_2017,
  title = {Financial {{Time Series Prediction Using Deep Learning}}},
  author = {Navon, Ariel and Keller, Yosi},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.04174 [eess, q-fin]},
  eprint = {1711.04174},
  primaryclass = {eess, q-fin},
  urldate = {2019-03-19},
  abstract = {In this work we present a data-driven end-to-end Deep Learning approach for time series prediction, applied to financial time series. A Deep Learning scheme is derived to predict the temporal trends of stocks and ETFs in NYSE or NASDAQ. Our approach is based on a neural network (NN) that is applied to raw financial data inputs, and is trained to predict the temporal trends of stocks and ETFs. In order to handle commission-based trading, we derive an investment strategy that utilizes the probabilistic outputs of the NN, and optimizes the average return. The proposed scheme is shown to provide statistically significant accurate predictions of financial market trends, and the investment strategy is shown to be profitable under this challenging setup. The performance compares favorably with contemporary benchmarks along two-years of back-testing.},
  archiveprefix = {arxiv},
  keywords = {Electrical Engineering and Systems Science - Signal Processing,Quantitative Finance - Statistical Finance,read - not in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WABEKPT4\\Navon and Keller - 2017 - Financial Time Series Prediction Using Deep Learni.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UR76V9AZ\\1711.html}
}

@article{nazi_deep_2019,
  title = {A {{DEEP LEARNING FRAMEWORK FOR GRAPH PARTITIONING}}},
  author = {Nazi, Azade and Hang, Will and Goldie, Anna and Ravi, Sujith and Mirhoseini, Azalia},
  year = {2019},
  pages = {7},
  abstract = {Graph partitioning is the problem of dividing the nodes of a graph into balanced partitions while minimizing the edge cut across the partitions. Due to its combinatorial nature, many approximate solutions have been developed. We propose GAP, a Generalizable Approximate Partitioning framework that takes a deep learning approach to graph partitioning. We define a differentiable loss function that represents the partitioning objective. Unlike baselines that redo the optimization per graph, GAP is capable of generalization, allowing us to train models that produce performant partitions at inference time, even on unseen graphs. Furthermore, because we learn the representation of the graph while jointly optimizing for the partitioning loss function, GAP can be easily tuned for a variety of graph structures. We evaluate the performance of GAP on graphs of varying sizes and structures, including graphs of widely used machine learning models (e.g., ResNet, VGG, and Inception-V3), scale-free graphs, and random graphs.},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YNU9PMXS\Nazi et al. - 2019 - A DEEP LEARNING FRAMEWORK FOR GRAPH PARTITIONING.pdf}
}

@article{nejadettehad_shortterm_2019,
  title = {Short-Term {{Demand Forecasting}} for {{Online Car-hailing Services}} Using {{Recurrent Neural Networks}}},
  author = {Nejadettehad, Alireza and Mahini, Hamid and Bahrak, Behnam},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.10821 [cs, stat]},
  eprint = {1901.10821},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {Short-term traffic flow prediction is one of the crucial issues in intelligent transportation system, which is an important part of smart cities. Accurate predictions can enable both the drivers and the passengers to make better decisions about their travel route, departure time and travel origin selection, which can be helpful in traffic management. Multiple models and algorithms based on time series prediction and machine learning were applied to this issue and achieved acceptable results. Recently, the availability of sufficient data and computational power, motivates us to improve the prediction accuracy via deep-learning approaches. Recurrent neural networks have become one of the most popular methods for time series forecasting, however, due to the variety of these networks, the question that which type is the most appropriate one for this task remains unsolved. In this paper, we use three kinds of recurrent neural networks including simple RNN units, GRU and LSTM neural network to predict short-term traffic flow. The dataset from TAP30 Corporation is used for building the models and comparing RNNs with several well-known models, such as DEMA, LASSO and XGBoost. The results show that all three types of RNNs outperform the others, however, more simple RNNs such as simple recurrent units and GRU perform work better than LSTM in terms of accuracy and training time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - not in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3HWR4MAR\\Nejadettehad et al. - 2019 - Short-term Demand Forecasting for Online Car-haili.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZHXGCBM9\\1901.html}
}

@misc{nespoli_hierarchical_2020,
  title = {Hierarchical {{Demand Forecasting Benchmark}} for the {{Distribution Grid}}},
  author = {Nespoli, Lorenzo and Medici, Vasco and Lopatichki, Kristijan and Sossan, Fabrizio},
  year = {2020},
  month = apr,
  number = {arXiv:1910.03976},
  eprint = {1910.03976},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1910.03976},
  urldate = {2022-05-25},
  abstract = {We present a comparative study of different probabilistic forecasting techniques on the task of predicting the electrical load of secondary substations and cabinets located in a low voltage distribution grid, as well as their aggregated power profile. The methods are evaluated using standard KPIs for deterministic and probabilistic forecasts. We also compare the ability of different hierarchical techniques in improving the bottom level forecasters' performances. Both the raw and cleaned datasets, including meteorological data, are made publicly available to provide a standard benchmark for evaluating forecasting algorithms for demand-side management applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\JL57ELRN\\Nespoli et al. - 2020 - Hierarchical Demand Forecasting Benchmark for the .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EVD674WE\\1910.html}
}

@inproceedings{ning_modeling_2016,
  title = {Modeling {{Precursors}} for {{Event Forecasting}} via {{Nested Multi-Instance Learning}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ning, Yue and Muthiah, Sathappan and Rangwala, Huzefa and Ramakrishnan, Naren},
  year = {2016},
  series = {{{KDD}} '16},
  pages = {1095--1104},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2939672.2939802},
  urldate = {2019-04-16},
  abstract = {Forecasting large-scale societal events like civil unrest movements, disease outbreaks, and elections is an important and challenging problem. From the perspective of human analysts and policy makers, forecasting algorithms must not only make accurate predictions but must also provide supporting evidence, e.g., the causal factors related to the event of interest. We develop a novel multiple instance learning based approach that jointly tackles the problem of identifying evidence-based precursors and forecasts events into the future. Specifically, given a collection of streaming news articles from multiple sources we develop a nested multiple instance learning approach to forecast significant societal events such as protests. Using data from three countries in Latin America, we demonstrate how our approach is able to consistently identify news articles considered as precursors for protests. Our empirical evaluation demonstrates the strengths of our proposed approach in filtering candidate precursors, in forecasting the occurrence of events with a lead time advantage and in accurately predicting the characteristics of civil unrest events.},
  isbn = {978-1-4503-4232-2},
  keywords = {event detection,multi-instance learning,read - in related work,text mining},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2W3WWZ99\Ning et al. - 2016 - Modeling Precursors for Event Forecasting via Nest.pdf}
}

@article{noauthor_hierarchical_2009,
  title = {Hierarchical Forecasts for {{Australian}} Domestic Tourism},
  year = {2009},
  month = jan,
  journal = {International Journal of Forecasting},
  volume = {25},
  number = {1},
  pages = {146--166},
  publisher = {{Elsevier}},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2008.07.004},
  urldate = {2023-09-11},
  abstract = {In this paper we explore the hierarchical nature of tourism demand time series and produce short-term forecasts for Australian domestic tourism. The d\ldots},
  langid = {american},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\58VLBN3K\\2009 - Hierarchical forecasts for Australian domestic tou.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BZY2DV9G\\S0169207008000691.html}
}

@article{nolte_what_2019,
  title = {What Determines Forecasters' Forecasting Errors?},
  author = {Nolte, Ingmar and Nolte, Sandra and Pohlmeier, Winfried},
  year = {2019},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {Special {{Section}}: {{Supply Chain Forecasting}}},
  volume = {35},
  number = {1},
  pages = {11--24},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2018.07.007},
  urldate = {2019-03-19},
  abstract = {This paper contributes to the growing body of literature in macroeconomics and finance on expectation formation and information processing by analyzing the relationship between expectation formation at the individual level and the prediction of macroeconomic aggregates. Using information from business tendency surveys, we present a new approach of analyzing forecasters' qualitative forecasting errors. Based on a quantal response approach with misclassification, we define forecasters' qualitative mispredictions in terms of deviations from the qualitative rational expectation forecast, and relate them to the individual and macro factors that are driving these mispredictions. Our approach permits a detailed analysis of individual forecasting decisions, allowing for the introduction of individual and economy-wide determinants that affect the individual forecasting error process.},
  keywords = {Expectations,Forecasting errors,GLARMA,Misclassification,Tendency survey},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YSCF4TI8\\Nolte et al. - 2019 - What determines forecastersâ€™ forecasting errors.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EFYQ8PE8\\S0169207018301122.html}
}

@misc{novak_bayesian_2017,
  title = {A {{Bayesian Model}} for {{Forecasting Hierarchically Structured Time Series}}},
  author = {Novak, Julie and McGarvie, Scott and Garcia, Beatriz Etchegaray},
  year = {2017},
  month = nov,
  number = {arXiv:1711.04738},
  eprint = {1711.04738},
  primaryclass = {stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1711.04738},
  urldate = {2022-05-25},
  abstract = {An important task for any large-scale organization is to prepare forecasts of key performance metrics. Often these organizations are structured in a hierarchical manner and for operational reasons, projections of these metrics may have been obtained independently from one another at each level of the hierarchy by specialists focusing on certain areas within the business. There is no guarantee that when combined, these aggregates will be consistent with projections produced directly at other levels of the hierarchy. We propose a Bayesian hierarchical method that treats the initial forecasts as observed data which are then combined with prior information and historical predictive accuracy to infer a probability distribution of revised forecasts. When used to create point estimates, this method can reflect preferences for increased accuracy at specific levels in the hierarchy. We present simulated and real data studies to demonstrate when our approach results in improved inferences over alternative methods.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EUHMXDGB\\Novak et al. - 2017 - A Bayesian Model for Forecasting Hierarchically St.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IEGMGD9W\\1711.html}
}

@article{nystrup_temporal_2020,
  title = {Temporal Hierarchies with Autocorrelation for Load Forecasting},
  author = {Nystrup, Peter and Lindstr{\"o}m, Erik and Pinson, Pierre and Madsen, Henrik},
  year = {2020},
  month = feb,
  journal = {European Journal of Operational Research},
  volume = {280},
  number = {3},
  pages = {876--888},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2019.07.061},
  urldate = {2022-05-24},
  abstract = {We propose four different estimators that take into account the autocorrelation structure when reconciling forecasts in a temporal hierarchy. Combining forecasts from multiple temporal aggregation levels exploits information differences and mitigates model uncertainty, while reconciliation ensures a unified prediction that supports aligned decisions at different horizons. In previous studies, weights assigned to the forecasts were given by the structure of the hierarchy or the forecast error variances without considering potential autocorrelation in the forecast errors. Our first estimator considers the autocovariance matrix within each aggregation level. Since this can be difficult to estimate, we propose a second estimator that blends autocorrelation and variance information, but only requires estimation of the first-order autocorrelation coefficient at each aggregation level. Our third and fourth estimators facilitate information sharing between aggregation levels using robust estimates of the cross-correlation matrix and its inverse. We compare the proposed estimators in a simulation study and demonstrate their usefulness through an application to short-term electricity load forecasting in four price areas in Sweden. We find that by taking account of auto- and cross-covariances when reconciling forecasts, accuracy can be significantly improved uniformly across all frequencies and areas.},
  langid = {english},
  keywords = {Autocorrelation,Forecast combination,Forecasting,Reconciliation,Temporal aggregation},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FN4MQCM7\\Nystrup et al. - 2020 - Temporal hierarchies with autocorrelation for load.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\46WS2P9T\\S037722171930640X.html}
}

@misc{olivares_probabilistic_2022,
  title = {Probabilistic {{Hierarchical Forecasting}} with {{Deep Poisson Mixtures}}},
  author = {Olivares, Kin G. and Meetei, O. Nganba and Ma, Ruijun and Reddy, Rohan and Cao, Mengfei and Dicker, Lee},
  year = {2022},
  month = jan,
  number = {arXiv:2110.13179},
  eprint = {2110.13179},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2110.13179},
  urldate = {2022-05-25},
  abstract = {Hierarchical forecasting problems arise when time series have a natural group structure, and predictions at multiple levels of aggregation and disaggregation across the groups are needed. In such problems, it is often desired to satisfy the aggregation constraints in a given hierarchy, referred to as hierarchical coherence in the literature. Maintaining hierarchical coherence while producing accurate forecasts can be a challenging problem, especially in the case of probabilistic forecasting. We present a novel method capable of accurate and coherent probabilistic forecasts for hierarchical time series. We call it Deep Poisson Mixture Network (DPMN). It relies on the combination of neural networks and a statistical model for the joint distribution of the hierarchical multivariate time series structure. By construction, the model guarantees hierarchical coherence and provides simple rules for aggregation and disaggregation of the predictive distributions. We perform an extensive empirical evaluation comparing the DPMN to other state-of-the-art methods which produce hierarchically coherent probabilistic forecasts on multiple public datasets. Compared to existing coherent probabilistic models, we obtained a relative improvement in the overall Continuous Ranked Probability Score (CRPS) of 17.1\% on Australian domestic tourism data, 24.2 on the Favorita grocery sales dataset, and 6.9\% on a San Francisco Bay Area highway traffic dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZWTX44RX\\Olivares et al. - 2022 - Probabilistic Hierarchical Forecasting with Deep P.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PCZK2KF4\\2110.html}
}

@article{oliveira_assessing_2019,
  title = {Assessing the {{Performance}} of {{Hierarchical Forecasting Methods}} on the {{Retail Sector}}},
  author = {Oliveira, Jos{\'e} Manuel and Ramos, Patr{\'i}cia},
  year = {2019},
  month = apr,
  journal = {Entropy},
  volume = {21},
  number = {4},
  pages = {436},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/e21040436},
  urldate = {2021-02-04},
  abstract = {Retailers need demand forecasts at different levels of aggregation in order to support a variety of decisions along the supply chain. To ensure aligned decision-making across the hierarchy, it is essential that forecasts at the most disaggregated level add up to forecasts at the aggregate levels above. It is not clear if these aggregate forecasts should be generated independently or by using an hierarchical forecasting method that ensures coherent decision-making at the different levels but does not guarantee, at least, the same accuracy. To give guidelines on this issue, our empirical study investigates the relative performance of independent and reconciled forecasting approaches, using real data from a Portuguese retailer. We consider two alternative forecasting model families for generating the base forecasts; namely, state space models and ARIMA. Appropriate models from both families are chosen for each time-series by minimising the bias-corrected Akaike information criteria. The results show significant improvements in forecast accuracy, providing valuable information to support management decisions. It is clear that reconciled forecasts using the Minimum Trace Shrinkage estimator (MinT-Shrink) generally improve on the accuracy of the ARIMA base forecasts for all levels and for the complete hierarchy, across all forecast horizons. The accuracy gains generally increase with the horizon, varying between 1.7\% and 3.7\% for the complete hierarchy. It is also evident that the gains in forecast accuracy are more substantial at the higher levels of aggregation, which means that the information about the individual dynamics of the series, which was lost due to aggregation, is brought back again from the lower levels of aggregation to the higher levels by the reconciliation process, substantially improving the forecast accuracy over the base forecasts.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {ARIMA,entropy,hierarchical forecasting,information criteria,model selection,retail,state space models},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\I3IQ8AZJ\Oliveira and Ramos - 2019 - Assessing the Performance of Hierarchical Forecast.pdf}
}

@article{oord_conditional_2016,
  title = {Conditional {{Image Generation}} with {{PixelCNN Decoders}}},
  author = {van den Oord, Aaron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.05328 [cs]},
  eprint = {1606.05328},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-ofthe-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YS2YYBUB\Oord et al. - 2016 - Conditional Image Generation with PixelCNN Decoder.pdf}
}

@article{oord_pixel_2016,
  title = {Pixel {{Recurrent Neural Networks}}},
  author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  year = {2016},
  month = jan,
  journal = {arXiv:1601.06759 [cs]},
  eprint = {1601.06759},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast twodimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MIJFM5AN\Oord et al. - 2016 - Pixel Recurrent Neural Networks.pdf}
}

@inproceedings{oord_wavenet_2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  booktitle = {The 9th {{ISCA Speech Synthesis Workshop}}, {{Sunnyvale}}, {{CA}}, {{USA}}, 13-15 {{September}} 2016},
  author = {van den Oord, A{\"a}ron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew W. and Kavukcuoglu, Koray},
  year = {2016},
  pages = {125},
  publisher = {{ISCA}},
  urldate = {2020-01-17}
}

@article{oreshkin_metalearning_2020,
  title = {Meta-Learning Framework with Applications to Zero-Shot Time-Series Forecasting},
  author = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.02887 [cs, stat]},
  eprint = {2002.02887},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {Can meta-learning discover generic ways of processing time-series (TS) from a diverse dataset so as to greatly improve generalization on new TS coming from different datasets? This work provides positive evidence to demonstrate this using a broad meta-learning framework which we show subsumes many existing meta-learning algorithms as specific cases. We further identify via theoretical analysis the meta-learning adaptation mechanisms within N-BEATS, a recent neural TS forecasting model. Our meta-learning theory predicts that N-BEATS iteratively generates a subset of its task-specific parameters based on a given TS input, thus gradually expanding the expressive power of the architecture on-the-fly. Our empirical results emphasize the importance of meta-learning for successful zero-shot forecasting to new sources of TS, supporting the claim that it is viable to train a neural network on a source TS dataset and deploy it on a different target TS dataset without retraining, resulting in performance that is at least as good as that of state-of-practice univariate forecasting models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JFRUIWE2\Oreshkin et al. - 2020 - Meta-learning framework with applications to zero-.pdf}
}

@article{oreshkin_nbeats_2019,
  title = {N-{{BEATS}}: {{Neural}} Basis Expansion Analysis for Interpretable Time Series Forecasting},
  shorttitle = {N-{{BEATS}}},
  author = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  year = {2019},
  month = may,
  journal = {arXiv:1905.10437 [cs, stat]},
  eprint = {1905.10437},
  primaryclass = {cs, stat},
  urldate = {2019-10-11},
  abstract = {We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on the well-known M4 competition dataset containing 100k time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS, improving forecast accuracy by 11\% over a statistical benchmark and by 3\% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on the M4 dataset strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without loss in accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00001},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\99AQ744F\\Oreshkin et al. - 2019 - N-BEATS Neural basis expansion analysis for inter.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LW9P5L2M\\1905.html}
}

@misc{orlova_ensemble_2022,
  title = {Beyond {{Ensemble Averages}}: {{Leveraging Climate Model Ensembles}} for {{Subseasonal Forecasting}}},
  shorttitle = {Beyond {{Ensemble Averages}}},
  author = {Orlova, Elena and Liu, Haokun and Rossellini, Raphael and Cash, Benjamin and Willett, Rebecca},
  year = {2022},
  month = nov,
  number = {arXiv:2211.15856},
  eprint = {2211.15856},
  primaryclass = {physics},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.15856},
  urldate = {2023-03-07},
  abstract = {Producing high-quality forecasts of key climate variables such as temperature and precipitation on subseasonal time scales has long been a gap in operational forecasting. Recent studies have shown promising results using machine learning (ML) models to advance subseasonal forecasting (SSF), but several open questions remain. First, several past approaches use the average of an ensemble of physics-based forecasts as an input feature of these models. However, ensemble forecasts contain information that can aid prediction beyond only the ensemble mean. Second, past methods have focused on average performance, whereas forecasts of extreme events are far more important for planning and mitigation purposes. Third, climate forecasts correspond to a spatially-varying collection of forecasts, and different methods account for spatial variability in the response differently. Trade-offs between different approaches may be mitigated with model stacking. This paper describes the application of a variety of ML methods used to predict monthly average precipitation and two meter temperature using physics-based predictions (ensemble forecasts) and observational data such as relative humidity, pressure at sea level, or geopotential height, two weeks in advance for the whole continental United States. Regression, quantile regression, and tercile classification tasks using linear models, random forests, convolutional neural networks, and stacked models are considered. The proposed models outperform common baselines such as historical averages (or quantiles) and ensemble averages (or quantiles). This paper further includes an investigation of feature importance, trade-offs between using the full ensemble or only the ensemble average, and different modes of accounting for spatial variability.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Physics - Atmospheric and Oceanic Physics},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8RN7FZ38\\Orlova et al. - 2022 - Beyond Ensemble Averages Leveraging Climate Model.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ILZN9QI8\\2211.html}
}

@article{osborne_introduction_,
  title = {An {{Introduction}} to {{Fitting Gaussian Processes}} to {{Data}}},
  author = {Osborne, Michael},
  pages = {112},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\G4ZRN5PR\Osborne - An Introduction to Fitting Gaussian Processes to D.pdf}
}

@article{ostrovski_autoregressive_2018,
  title = {Autoregressive {{Quantile Networks}} for {{Generative Modeling}}},
  author = {Ostrovski, Georg and Dabney, Will and Munos, R{\'e}mi},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.05575 [cs, stat]},
  eprint = {1806.05575},
  primaryclass = {cs, stat},
  urldate = {2020-01-29},
  abstract = {We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherrypicked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RAKCIW2J\Ostrovski et al. - 2018 - Autoregressive Quantile Networks for Generative Mo.pdf}
}

@misc{ozaki_lightgbm_2020,
  title = {{{LightGBM Tuner}}: {{New Optuna Integration}} for {{Hyperparameter Optimization}}},
  shorttitle = {{{LightGBM Tuner}}},
  author = {Ozaki, Kohei},
  year = {2020},
  month = mar,
  journal = {Optuna},
  urldate = {2022-08-16},
  abstract = {The usage of LightGBM Tuner is straightforward. You use LightGBM Tuner by changing one import statement in your Python code.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YP2MG3B4\lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258.html}
}

@article{pan_hyperstnet_2018,
  title = {{{HyperST-Net}}: {{Hypernetworks}} for {{Spatio-Temporal Forecasting}}},
  shorttitle = {{{HyperST-Net}}},
  author = {Pan, Zheyi and Liang, Yuxuan and Zhang, Junbo and Yi, Xiuwen and Yu, Yong and Zheng, Yu},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.10889 [cs, stat]},
  eprint = {1809.10889},
  primaryclass = {cs, stat},
  urldate = {2019-04-03},
  abstract = {Spatio-temporal (ST) data, which represent multiple time series data corresponding to different spatial locations, are ubiquitous in real-world dynamic systems, such as air quality readings. Forecasting over ST data is of great importance but challenging as it is affected by many complex factors, including spatial characteristics, temporal characteristics and the intrinsic causality between them. In this paper, we propose a general framework (HyperST-Net) based on hypernetworks for deep ST models. More specifically, it consists of three major modules: a spatial module, a temporal module and a deduction module. Among them, the deduction module derives the parameter weights of the temporal module from the spatial characteristics, which are extracted by the spatial module. Then, we design a general form of HyperST layer as well as different forms for several basic layers in neural networks, including the dense layer (HyperST-Dense) and the convolutional layer (HyperST-Conv). Experiments on three types of real-world tasks demonstrate that the predictive models integrated with our framework achieve significant improvements, and outperform the state-of-the-art baselines as well.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3BFF3KTI\Pan et al. - 2018 - HyperST-Net Hypernetworks for Spatio-Temporal For.pdf}
}

@article{panagiotelis_forecast_2021,
  title = {Forecast Reconciliation: {{A}} Geometric View with New Insights on Bias Correction},
  shorttitle = {Forecast Reconciliation},
  author = {Panagiotelis, Anastasios and Athanasopoulos, George and Gamakumara, Puwasala and Hyndman, Rob J.},
  year = {2021},
  month = jan,
  journal = {International Journal of Forecasting},
  volume = {37},
  number = {1},
  pages = {343--359},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2020.06.004},
  urldate = {2022-05-24},
  abstract = {A geometric interpretation is developed for so-called reconciliation methodologies used to forecast time series that adhere to known linear constraints. In particular, a general framework is established that nests many existing popular reconciliation methods within the class of projections. This interpretation facilitates the derivation of novel theoretical results. First, reconciliation via projection is guaranteed to improve forecast accuracy with respect to a class of loss functions based on a generalised distance metric. Second, the Minimum Trace (MinT) method minimises expected loss for this same class of loss functions. Third, the geometric interpretation provides a new proof that forecast reconciliation using projections results in unbiased forecasts, provided that the initial base forecasts are also unbiased. Approaches for dealing with biased base forecasts are proposed. An extensive empirical study of Australian tourism flows demonstrates the theoretical results of the paper and shows that bias correction prior to reconciliation outperforms alternatives that only bias-correct or only reconcile forecasts.},
  langid = {english},
  keywords = {Elliptical distributions,Forecast reconciliation,High-dimensional time series,Projections,Scoring rules},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\VGD9UZJ7\\1-s2.0-S0169207020300911-main.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\W3G7EL7V\\S0169207020300911.html}
}

@techreport{panagiotelis_probabilistic_2020,
  title = {Probabilistic {{Forecast Reconciliation}}: {{Properties}}, {{Evaluation}} and {{Score Optimisation}}},
  shorttitle = {Probabilistic {{Forecast Reconciliation}}},
  author = {Panagiotelis, Anastasios and Gamakumara, Puwasala and Athanasopoulos, George and Hyndman, Rob J.},
  year = {2020},
  journal = {Monash Econometrics and Business Statistics Working Papers},
  number = {26/20},
  institution = {{Monash University, Department of Econometrics and Business Statistics}},
  urldate = {2021-10-06},
  abstract = {We develop a framework for prediction of multivariate data that follow some known linear constraints, such as the example where some variables are aggregates of others. This is particularly common when forecasting time series (predicting the future), but also arises in other types of prediction. For point prediction, an increasingly popular technique is reconciliation, whereby predictions are made for all series (so-called `base' predictions) and subsequently adjusted to ensure coherence with the constraints. This paper extends reconciliation from the setting of point prediction to probabilistic prediction. A novel definition of reconciliation is developed and used to construct densities and draw samples from a reconciled probabilistic prediction. In the elliptical case, it is proven that the true predictive distribution can be recovered from reconciliation even when the location and scale matrix of the base prediction are chosen arbitrarily. To find reconciliation weights, an objective function based on scoring rules is optimised. The energy and variogram scores are considered since the log score is improper in the context of comparing unreconciled to reconciled predictions, a result also proved in this paper. To account for the stochastic nature of the energy and variogram scores, optimisation is achieved using stochastic gradient descent. This method is shown to improve base predictions in simulation studies and in an empirical application, particularly when the base prediction models are severely misspecified. When misspecification is not too severe, extending popular reconciliation methods for point prediction can result in a similar performance to score optimisation via stochastic gradient descent. The methods described here are implemented in the ProbReco package for R.},
  langid = {english},
  keywords = {hierarchical time series,probabilistic forecasting,scoring rules,stochastic gradient descent},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\KCFAMSLI\\Panagiotelis et al. - 2020 - Probabilistic Forecast Reconciliation Properties,.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\X6RT74QW\\2020-26.html}
}

@article{papanicolaou_taylor_,
  title = {Taylor {{Approximation}} and the {{Delta Method}}},
  author = {Papanicolaou, Alex},
  pages = {6},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BLJCLX48\Papanicolaou - Taylor Approximation and the Delta Method.pdf}
}

@misc{paria_hierarchically_2021,
  title = {Hierarchically {{Regularized Deep Forecasting}}},
  author = {Paria, Biswajit and Sen, Rajat and Ahmed, Amr and Das, Abhimanyu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.07630},
  eprint = {2106.07630},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2106.07630},
  urldate = {2022-05-25},
  abstract = {Hierarchical forecasting is a key problem in many practical multivariate forecasting applications - the goal is to simultaneously predict a large number of correlated time series that are arranged in a pre-specified aggregation hierarchy. The main challenge is to exploit the hierarchical correlations to simultaneously obtain good prediction accuracy for time series at different levels of the hierarchy. In this paper, we propose a new approach for hierarchical forecasting which consists of two components. First, decomposing the time series along a global set of basis time series and modeling hierarchical constraints using the coefficients of the basis decomposition. And second, using a linear autoregressive model with coefficients that vary with time. Unlike past methods, our approach is scalable (inference for a specific time series only needs access to its own history) while also modeling the hierarchical structure via (approximate) coherence constraints among the time series forecasts. We experiment on several public datasets and demonstrate significantly improved overall performance on forecasts at different levels of the hierarchy, compared to existing state-of-the-art hierarchical models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4F7KZXQM\\Paria et al. - 2021 - Hierarchically Regularized Deep Forecasting.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\49GMPI7V\\2106.html}
}

@article{parisi_continual_2019,
  title = {Continual Lifelong Learning with Neural Networks: {{A}} Review},
  shorttitle = {Continual Lifelong Learning with Neural Networks},
  author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
  year = {2019},
  month = may,
  journal = {Neural Networks},
  volume = {113},
  pages = {54--71},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2019.01.012},
  urldate = {2019-05-01},
  abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
  keywords = {Catastrophic forgetting,Continual learning,Developmental systems,Lifelong learning,Memory consolidation,read - in related work},
  annotation = {00040},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\R5G6SC6D\\Parisi et al. - 2019 - Continual lifelong learning with neural networks .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FHN9ZRFF\\S0893608019300231.html}
}

@article{parisotto_actormimic_2015,
  title = {Actor-{{Mimic}}: {{Deep Multitask}} and {{Transfer Reinforcement Learning}}},
  shorttitle = {Actor-{{Mimic}}},
  author = {Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.06342 [cs]},
  eprint = {1511.06342},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed ``Actor-Mimic'', exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\8PV3PGUE\Parisotto et al. - 2015 - Actor-Mimic Deep Multitask and Transfer Reinforce.pdf}
}

@article{park_variational_2014,
  title = {Variational {{Bayesian}} Inference for Forecasting Hierarchical Time Series},
  author = {Park, Mijung and Nassar, Marcel},
  year = {2014},
  pages = {7},
  abstract = {In many real world data, time series are often hierarchically organized. Based on features such as products or geography, time series can be aggregated and disaggregated at several different levels. The so-called `hierarchical time series' are often forecast using simple top-town or bottomup approaches. In this paper, we build a probabilistic model that involves dynamically evolving latent variables to capture the proportion changes in time series at each hierarchy. We derive the variational Bayesian expectation maximisation (VBEM) algorithm under the new model. In our algorithm, we implement the posterior inference in a sequential manner that significantly decreases computational overhead common in large hierarchical time series data. Furthermore, unlike the standard EM algorithm that provides point estimates of model parameters, our algorithm yields the distribution over the model parameters, which give us an insight to which subset of features yields the proportion changes of the time series. Simulation results show that our method significantly outperforms other methods in prediction.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QLRBBJCH\Park and Nassar - 2014 - Variational Bayesian inference for forecasting hie.pdf}
}

@article{passalis_temporal_2019,
  title = {Temporal {{Logistic Neural Bag-of-Features}} for {{Financial Time}} Series {{Forecasting}} Leveraging {{Limit Order Book Data}}},
  author = {Passalis, Nikolaos and Tefas, Anastasios and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.08280 [cs, q-fin, stat]},
  eprint = {1901.08280},
  primaryclass = {cs, q-fin, stat},
  urldate = {2019-03-19},
  abstract = {Time series forecasting is a crucial component of many important applications, ranging from forecasting the stock markets to energy load prediction. The high-dimensionality, velocity and variety of the data collected in these applications pose significant and unique challenges that must be carefully addressed for each of them. In this work, a novel Temporal Logistic Neural Bag-of-Features approach, that can be used to tackle these challenges, is proposed. The proposed method can be effectively combined with deep neural networks, leading to powerful deep learning models for time series analysis. However, combining existing BoF formulations with deep feature extractors pose significant challenges: the distribution of the input features is not stationary, tuning the hyper-parameters of the model can be especially difficult and the normalizations involved in the BoF model can cause significant instabilities during the training process. The proposed method is capable of overcoming these limitations by a employing a novel adaptive scaling mechanism and replacing the classical Gaussian-based density estimation involved in the regular BoF model with a logistic kernel. The effectiveness of the proposed approach is demonstrated using extensive experiments on a large-scale financial time series dataset that consists of more than 4 million limit orders.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Finance - Computational Finance,read - not in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MWBCXZBC\\Passalis et al. - 2019 - Temporal Logistic Neural Bag-of-Features for Finan.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\2HW44X3M\\1901.html}
}

@incollection{paszke_pytorch_2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-02-10},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7WYNWGJI\\Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LWEP6Y4B\\9015-pytorch-an-imperative-style-high-performance-deep-learning-library.html}
}

@article{paul_deep_2021,
  title = {Deep {{Learning}} on a {{Data Diet}}: {{Finding Important Examples Early}} in {{Training}}},
  shorttitle = {Deep {{Learning}} on a {{Data Diet}}},
  author = {Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.07075 [cs]},
  eprint = {2107.07075},
  primaryclass = {cs},
  urldate = {2021-07-21},
  abstract = {The recent success of deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, on standard vision benchmarks, the initial loss gradient norm of individual training examples, averaged over several weight initializations, can be used to identify a smaller set of training data that is important for generalization. Furthermore, after only a few epochs of training, the information in gradient norms is reflected in the normed error--L2 distance between the predicted probabilities and one hot labels--which can be used to prune a significant fraction of the dataset without sacrificing test accuracy. Based on this, we propose data pruning methods which use only local information early in training, and connect them to recent work that prunes data by discarding examples that are rarely forgotten over the course of training. Our methods also shed light on how the underlying data distribution shapes the training dynamics: they rank examples based on their importance for generalization, detect noisy examples and identify subspaces of the model's data representation that are relatively stable over training.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GH5FG2LU\\Paul et al. - 2021 - Deep Learning on a Data Diet Finding Important Ex.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Q2SSETJG\\2107.html}
}

@article{pedregosa_scikit-learn_2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  issn = {1533-7928},
  urldate = {2023-10-19},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\8J4ER3AP\Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf}
}

@article{pesaran_estimation_2006,
  title = {Estimation and {{Inference}} in {{Large Heterogeneous Panels}} with a {{Multifactor Error Structure}}},
  author = {Pesaran, M. Hashem},
  year = {2006},
  journal = {Econometrica},
  volume = {74},
  number = {4},
  pages = {967--1012},
  issn = {1468-0262},
  doi = {10.1111/j.1468-0262.2006.00692.x},
  urldate = {2020-02-24},
  abstract = {This paper presents a new approach to estimation and inference in panel data models with a general multifactor error structure. The unobserved factors and the individual-specific errors are allowed to follow arbitrary stationary processes, and the number of unobserved factors need not be estimated. The basic idea is to filter the individual-specific regressors by means of cross-section averages such that asymptotically as the cross-section dimension (N) tends to infinity, the differential effects of unobserved common factors are eliminated. The estimation procedure has the advantage that it can be computed by least squares applied to auxiliary regressions where the observed regressors are augmented with cross-sectional averages of the dependent variable and the individual-specific regressors. A number of estimators (referred to as common correlated effects (CCE) estimators) are proposed and their asymptotic distributions are derived. The small sample properties of mean group and pooled CCE estimators are investigated by Monte Carlo experiments, showing that the CCE estimators have satisfactory small sample properties even under a substantial degree of heterogeneity and dynamics, and for relatively small values of N and T.},
  langid = {english},
  keywords = {common correlated effects,Cross-section dependence,estimation and inference,heterogeneity,large panels},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YMGAIYH9\\Pesaran - 2006 - Estimation and Inference in Large Heterogeneous Pa.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6Y2XNFDI\\j.1468-0262.2006.00692.html}
}

@article{pesevski_subspace_2018,
  title = {Subspace Clustering with the Multivariate-t Distribution},
  author = {Pesevski, Angelina and Franczak, Brian C. and McNicholas, Paul D.},
  year = {2018},
  month = sep,
  journal = {Pattern Recognition Letters},
  volume = {112},
  pages = {297--302},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2018.07.003},
  urldate = {2020-03-03},
  abstract = {Clustering procedures suitable for the analysis of very high-dimensional data are needed for many modern data sets. One approach, called high-dimensional data clustering (HDDC), uses a family of Gaussian mixture models for clustering. HDDC is based on the idea that high-dimensional data usually exists in lower-dimensional subspaces; as such, an intrinsic dimension for each sub-population of the observed data can be estimated and cluster analysis can be performed in this lower-dimensional subspace. As a result, only a fraction of the total number of parameters needs to be estimated. This family of models has gained attention due to its superior classification performance compared to other families of mixture models; however, it still suffers from the usual limitations of Gaussian mixture model-based approaches, e.g., these models are sensitive to outlying or spurious points. In this paper, a robust analog of the HDDC approach is proposed. This approach, which extends the HDDC procedure to the multivariate-t distribution, encompasses 28 models that rectify the aforementioned shortcoming of the HDDC procedure. Our tHDDC procedure is compared to the HDDC procedure using both simulated and real data sets, which includes an image reconstruction problem that arose from satellite imagery of the surface of Mars.},
  langid = {english},
  keywords = {Dimension reduction,EM algorithm,Finite mixture models,Multivariate- distribution,Subspace clustering},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TYXABSZW\\Pesevski et al. - 2018 - Subspace clustering with the multivariate-t distri.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LQSWW8ME\\S0167865518302848.html}
}

@inproceedings{peters_causal_2013,
  title = {Causal {{Inference}} on {{Time Series Using Restricted Structural Equation Models}}},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year = {2013},
  series = {{{NIPS}}'13},
  pages = {154--162},
  publisher = {{Curran Associates Inc.}},
  address = {{USA}},
  urldate = {2019-04-17},
  abstract = {Causal inference uses observational data to infer the causal structure of the data generating system. We study a class of restricted Structural Equation Models for time series that we call Time Series Models with Independent Noise (TiMINo). These models require independent residual time series, whereas traditional methods like Granger causality exploit the variance of residuals. This work contains two main contributions: (l) Theoretical: By restricting the model class (e.g. to additive noise) we provide general identifiability results. They cover lagged and instantaneous effects that can be nonlinear and unfaithful, and non-instantaneous feedbacks between the time series. (2) Practical: If there are no feedback loops between time series, we propose an algorithm based on non-linear independence tests of time series. We show empirically that when the data are causally insufficient or the model is misspecified, the method avoids incorrect answers. We extend the theoretical and the algorithmic part to situations in which the time series have been measured with different time delays. TiMINo is applied to artificial and real data and code is provided.},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7HGTVEZN\Peters et al. - Causal Inference on Time Series using Restricted S.pdf}
}

@article{petersen_differentiable_2021,
  title = {Differentiable {{Sorting Networks}} for {{Scalable Sorting}} and {{Ranking Supervision}}},
  author = {Petersen, Felix and Borgelt, Christian and Kuehne, Hilde and Deussen, Oliver},
  year = {2021},
  month = jul,
  journal = {arXiv:2105.04019 [cs]},
  eprint = {2105.04019},
  primaryclass = {cs},
  urldate = {2022-02-15},
  abstract = {Sorting and ranking supervision is a method for training neural networks end-to-end based on ordering constraints. That is, the ground truth order of sets of samples is known, while their absolute values remain unsupervised. For that, we propose differentiable sorting networks by relaxing their pairwise conditional swap operations. To address the problems of vanishing gradients and extensive blurring that arise with larger numbers of layers, we propose mapping activations to regions with moderate gradients. We consider odd-even as well as bitonic sorting networks, which outperform existing relaxations of the sorting operation. We show that bitonic sorting networks can achieve stable training on large input sets of up to 1024 elements.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\XVJQL56F\Petersen et al. - 2021 - Differentiable Sorting Networks for Scalable Sorti.pdf}
}

@article{petnehazi_qcnn_2019,
  title = {{{QCNN}}: {{Quantile Convolutional Neural Network}}},
  shorttitle = {{{QCNN}}},
  author = {Petneh{\'a}zi, G{\'a}bor},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.07978 [cs, q-fin, stat]},
  eprint = {1908.07978},
  primaryclass = {cs, q-fin, stat},
  urldate = {2020-02-17},
  abstract = {Convolutional neural networks can do time series forecasting. They can learn local patterns in time, and they can be modified to learn only from the history (ignoring the future) and to use a long look-back window when doing so. A further simple modification enables them to forecast not the mean, but arbitrary quantiles of the distribution. And one last thing to make this all work: the CNN forecaster's complexity and flexibility requires much data, that is, preferably multiple time series. When this is met, the proposed QCNN framework can be competitive. It is demonstrated on a financial problem of huge practical importance: Value at Risk forecasting. By contributing to the stability of financial systems, deep learning may find one further way to improve our lives.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Finance - Computational Finance,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\VMCXBQA9\\PetnehÃ¡zi - 2019 - QCNN Quantile Convolutional Neural Network.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZFV55IPE\\1908.html}
}

@article{petropoulos_another_2016,
  title = {Another Look at Estimators for Intermittent Demand},
  author = {Petropoulos, Fotios and Kourentzes, Nikolaos and Nikolopoulos, Konstantinos},
  year = {2016},
  month = nov,
  journal = {International Journal of Production Economics},
  series = {{{SI}}: {{ISIR}} 2014},
  volume = {181},
  pages = {154--161},
  issn = {0925-5273},
  doi = {10.1016/j.ijpe.2016.04.017},
  urldate = {2019-06-03},
  abstract = {In this paper we focus on forecasting for intermittent demand data. We propose a new aggregation framework for intermittent demand forecasting that performs aggregation over the demand volumes, in contrast to the standard framework that employs temporal (over time) aggregation. To achieve this we construct a transformed time series, the inverse intermittent demand series. The new algorithm is expected to work best on erratic and lumpy demand, as a result of the variance reduction of the non-zero demands. The improvement in forecasting performance is empirically demonstrated through an extensive evaluation in more than 8000time series of two well-researched spare parts data sets from the automotive and defence sectors. Furthermore, a simulation is performed so as to provide a stock-control evaluation. The proposed framework could find popularity among practitioners given its suitability when dealing with clump sizes. As such it could be used in conjunction with existing popular forecasting methods for intermittent demand as an exception handling mechanism when certain types of demand are observed.},
  keywords = {Decomposition,Intermittent demand,Temporal aggregation,Variance reduction},
  annotation = {00016},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TVPVC2TB\\Petropoulos et al. - 2016 - Another look at estimators for intermittent demand.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\R4NQRA4A\\S092552731630041X.html}
}

@article{petropoulos_forecasting_2022,
  title = {Forecasting: Theory and Practice},
  shorttitle = {Forecasting},
  author = {Petropoulos, Fotios and Apiletti, Daniele and Assimakopoulos, Vassilios and Babai, Mohamed Zied and Barrow, Devon K. and Ben Taieb, Souhaib and Bergmeir, Christoph and Bessa, Ricardo J. and Bijak, Jakub and Boylan, John E. and Browell, Jethro and Carnevale, Claudio and Castle, Jennifer L. and Cirillo, Pasquale and Clements, Michael P. and Cordeiro, Clara and Cyrino Oliveira, Fernando Luiz and De Baets, Shari and Dokumentov, Alexander and Ellison, Joanne and Fiszeder, Piotr and Franses, Philip Hans and Frazier, David T. and Gilliland, Michael and G{\"o}n{\"u}l, M. Sinan and Goodwin, Paul and Grossi, Luigi and {Grushka-Cockayne}, Yael and Guidolin, Mariangela and Guidolin, Massimo and Gunter, Ulrich and Guo, Xiaojia and Guseo, Renato and Harvey, Nigel and Hendry, David F. and Hollyman, Ross and Januschowski, Tim and Jeon, Jooyoung and Jose, Victor Richmond R. and Kang, Yanfei and Koehler, Anne B. and Kolassa, Stephan and Kourentzes, Nikolaos and Leva, Sonia and Li, Feng and Litsiou, Konstantia and Makridakis, Spyros and Martin, Gael M. and Martinez, Andrew B. and Meeran, Sheik and Modis, Theodore and Nikolopoulos, Konstantinos and {\"O}nkal, Dilek and Paccagnini, Alessia and Panagiotelis, Anastasios and Panapakidis, Ioannis and Pav{\'i}a, Jose M. and Pedio, Manuela and Pedregal, Diego J. and Pinson, Pierre and Ramos, Patr{\'i}cia and Rapach, David E. and Reade, J. James and {Rostami-Tabar}, Bahman and Rubaszek, Micha{\l} and Sermpinis, Georgios and Shang, Han Lin and Spiliotis, Evangelos and Syntetos, Aris A. and Talagala, Priyanga Dilini and Talagala, Thiyanga S. and Tashman, Len and Thomakos, Dimitrios and Thorarinsdottir, Thordis and Todini, Ezio and Trapero Arenas, Juan Ram{\'o}n and Wang, Xiaoqian and Winkler, Robert L. and Yusupova, Alisa and Ziel, Florian},
  year = {2022},
  month = jan,
  journal = {International Journal of Forecasting},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.11.001},
  urldate = {2022-05-24},
  abstract = {Forecasting has always been at the forefront of decision making and planning. The uncertainty that surrounds the future is both exciting and challenging, with individuals and organisations seeking to minimise risks and maximise utilities. The large number of forecasting applications calls for a diverse set of forecasting methods to tackle real-life challenges. This article provides a non-systematic review of the theory and the practice of forecasting. We provide an overview of a wide range of theoretical, state-of-the-art models, methods, principles, and approaches to prepare, produce, organise, and evaluate forecasts. We then demonstrate how such theoretical concepts are applied in a variety of real-life contexts. We do not claim that this review is an exhaustive list of methods and applications. However, we wish that our encyclopedic presentation will offer a point of reference for the rich work that has been undertaken over the last decades, with some key insights for the future of forecasting theory and practice. Given its encyclopedic nature, the intended mode of reading is non-linear. We offer cross-references to allow the readers to navigate through the various topics. We complement the theoretical concepts and applications covered by large lists of free or open-source software implementations and publicly-available databases.},
  langid = {english},
  keywords = {Applications,Encyclopedia,Methods,Prediction,Principles,Review,Time series},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\EXX7I6PC\Petropoulos et al. - 2022 - Forecasting theory and practice.pdf}
}

@misc{petropoulos_model_2021,
  title = {Model Combinations through Revised Base-Rates},
  author = {Petropoulos, Fotios and Spiliotis, Evangelos and Panagiotelis, Anastasios},
  year = {2021},
  month = apr,
  number = {arXiv:2103.16157},
  eprint = {2103.16157},
  primaryclass = {stat},
  publisher = {{arXiv}},
  urldate = {2022-05-24},
  abstract = {Standard selection criteria for forecasting models focus on information that is calculated for each series independently, disregarding the general tendencies and performances of the candidate models. In this paper, we propose a new way to statistical model selection and model combination that incorporates the base-rates of the candidate forecasting models, which are then revised so that the per-series information is taken into account. We examine two schemes that are based on the precision and sensitivity information from the contingency table of the base rates. We apply our approach on pools of exponential smoothing models and a large number of real time series and we show that our schemes work better than standard statistical benchmarks. We discuss the connection of our approach to other cross-learning approaches and offer insights regarding implications for theory and practice.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Applications,Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HYBL6AU2\Petropoulos et al. - 2021 - Model combinations through revised base-rates.pdf}
}

@article{peyre_computational_2019,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2019},
  month = feb,
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {11},
  number = {5-6},
  pages = {355--607},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000073},
  urldate = {2019-10-02},
  abstract = {Computational Optimal Transport},
  langid = {english},
  annotation = {00199},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GF37XLTG\\PeyrÃ© and Cuturi - 2019 - Computational Optimal Transport.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\T7KN2XRL\\MAL-073.html}
}

@misc{philipps_interpreting_2022,
  type = {{{SSRN Scholarly Paper}}},
  title = {Interpreting {{Expectiles}}},
  author = {Philipps, Collin},
  year = {2022},
  month = jun,
  number = {3881402},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.3881402},
  urldate = {2023-10-10},
  abstract = {This article establishes how expectiles should be understood. An expectile is the minimizer of an asymmetric least squares criterion, making it a weighted average. This also means that an expectile is the conditional mean of the distribution under special circumstances. Specifically, an expectile of a distribution is a value that would be the mean if values above it were more likely to occur than they are. Expectiles summarize distributions in a manner comparable to quantiles, but quantiles are expectiles in location models. The reverse is true in special cases. Expectiles are m-estimators, m-quantiles, and Lp-quantiles, families which connect them to the majority of statistics commonly in use.},
  langid = {english},
  keywords = {Expectile Regression,Generalized Quantile Regression},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FUDAFJQL\Philipps - 2022 - Interpreting Expectiles.pdf}
}

@article{pletnev_graph_2020,
  title = {Graph {{Neural Networks}} for {{Model Recommendation}} Using {{Time Series Data}}},
  author = {Pletnev, Aleksandr and {Rivera-Castro}, Rodrigo and Burnaev, Evgeny},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.03474 [stat]},
  eprint = {2009.03474},
  primaryclass = {stat},
  urldate = {2021-01-07},
  abstract = {Time series prediction aims to predict future values to help stakeholders make proper strategic decisions. This problem is relevant in all industries and areas, ranging from financial data to demand to forecast. However, it remains challenging for practitioners to select the appropriate model to use for forecasting tasks. With this in mind, we present a model architecture based on Graph Neural Networks to provide model recommendations for time series forecasting. We validate our approach on three relevant datasets and compare it against more than sixteen techniques. Our study shows that the proposed method performs better than target baselines and state of the art, including meta-learning. The results show the relevancy and suitability of GNN as methods for model recommendations in time series forecasting.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DL4APVDZ\\Pletnev et al. - 2020 - Graph Neural Networks for Model Recommendation usi.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XZX3YDKQ\\2009.html}
}

@article{pliakos_global_2018,
  title = {Global Multi-Output Decision Trees for Interaction Prediction},
  author = {Pliakos, Konstantinos and Geurts, Pierre and Vens, Celine},
  year = {2018},
  month = sep,
  journal = {Machine Learning},
  volume = {107},
  number = {8-10},
  pages = {1257--1281},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-018-5700-x},
  urldate = {2019-09-04},
  abstract = {Interaction data are characterized by two sets of objects, each described by their own set of features. They are often modeled as networks and the values of interest are the possible interactions between two instances, represented usually as a matrix. Here, a novel global decision tree learning method is proposed, where multi-output decision trees are constructed over the global interaction setting, addressing the problem of interaction prediction as a multi-label classification task. More specifically, the tree is constructed by splitting the interaction matrix both row-wise and column-wise, incorporating this way both interaction dataset features in the learning procedure. Experiments are conducted across several heterogeneous interaction datasets from the biomedical domain. The experimental results indicate the superiority of the proposed method against other decision tree approaches in terms of predictive accuracy, model size and computational efficiency. The performance is boosted by fully exploiting the multi-output structure of the model. We conclude that the proposed method should be considered in interaction prediction tasks, especially where interpretable models are desired.},
  langid = {english},
  annotation = {00002},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZHZ5UZYL\Pliakos et al. - 2018 - Global multi-output decision trees for interaction.pdf}
}

@incollection{plis_rateagnostic_2015,
  title = {Rate-{{Agnostic}} ({{Causal}}) {{Structure Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Plis, Sergey and Danks, David and Freeman, Cynthia and Calhoun, Vince},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {3303--3311},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  keywords = {read - in related work},
  annotation = {00013},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZTWNQ4BW\\Plis et al. - 2015 - Rate-Agnostic (Causal) Structure Learning.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\T58CSIFD\\5689-rate-agnostic-causal-structure-learning.html}
}

@article{popov_neural_2019,
  title = {Neural {{Oblivious Decision Ensembles}} for {{Deep Learning}} on {{Tabular Data}}},
  author = {Popov, Sergei and Morozov, Stanislav and Babenko, Artem},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.06312 [cs, stat]},
  eprint = {1909.06312},
  primaryclass = {cs, stat},
  urldate = {2019-09-20},
  abstract = {Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\4LF5EHKL\Popov et al. - 2019 - Neural Oblivious Decision Ensembles for Deep Learn.pdf}
}

@article{potts_incremental_2005,
  title = {Incremental {{Learning}} of {{Linear Model Trees}}},
  author = {Potts, Duncan and Sammut, Claude},
  year = {2005},
  month = nov,
  journal = {Machine Learning},
  volume = {61},
  number = {1},
  pages = {5--48},
  issn = {1573-0565},
  doi = {10.1007/s10994-005-1121-8},
  urldate = {2019-09-05},
  abstract = {A linear model tree is a decision tree with a linear functional model in each leaf. Previous model tree induction algorithms have been batch techniques that operate on the entire training set. However there are many situations when an incremental learner is advantageous. In this article a new batch model tree learner is described with two alternative splitting rules and a stopping rule. An incremental algorithm is then developed that has many similarities with the batch version but is able to process examples one at a time. An online pruning rule is also developed. The incremental training time for an example is shown to only depend on the height of the tree induced so far, and not on the number of previous examples. The algorithms are evaluated empirically on a number of standard datasets, a simple test function and three dynamic domains ranging from a simple pendulum to a complex 13 dimensional flight simulator. The new batch algorithm is compared with the most recent batch model tree algorithms and is seen to perform favourably overall. The new incremental model tree learner compares well with an alternative online function approximator. In addition it can sometimes perform almost as well as the batch model tree algorithms, highlighting the effectiveness of the incremental implementation.},
  langid = {english},
  keywords = {incremental learning,linear regression trees,model trees,online learning},
  annotation = {00100},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\AAG742MX\Potts and Sammut - 2005 - Incremental Learning of Linear Model Trees.pdf}
}

@article{prasad_deep_2014,
  title = {Deep {{Recurrent Neural Networks}} for {{Time Series Prediction}}},
  author = {Prasad, Sharat C. and Prasad, Piyush},
  year = {2014},
  month = jul,
  journal = {arXiv:1407.5949 [cs]},
  eprint = {1407.5949},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Ability of deep networks to extract high level features and of recurrent networks to perform time-series inference have been studied. In view of universality of one hidden layer network at approximating functions under weak constraints, the benefit of multiple layers is to enlarge the space of dynamical systems approximated or, given the space, reduce the number of units required for a certain error. Traditionally shallow networks with manually engineered features are used, back-propagation extent is limited to one and attempt to choose a large number of hidden units to satisfy the Markov condition is made. In case of Markov models, it has been shown that many systems need to be modeled as higher order. In the present work, we present deep recurrent networks with longer backpropagation through time extent as a solution to modeling systems that are high order and to predicting ahead. We study epileptic seizure suppression electro-stimulator. Extraction of manually engineered complex features and prediction employing them has not allowed small low-power implementations as, to avoid possibility of surgery, extraction of any features that may be required has to be included. In this solution, a recurrent neural network performs both feature extraction and prediction. We prove analytically that adding hidden layers or increasing backpropagation extent increases the rate of decrease of approximation error. A Dynamic Programming (DP) training procedure employing matrix operations is derived. DP and use of matrix operations makes the procedure efficient particularly when using data-parallel computing. The simulation studies show the geometry of the parameter space, that the network learns the temporal structure, that parameters converge while model output displays same dynamic behavior as the system and greater than .99 Average Detection Rate on all real seizure data tried.},
  archiveprefix = {arxiv},
  keywords = {{62M45, 82C32, 92B20},C.1.3,Computer Science - Neural and Evolutionary Computing,F.1.1,I.2.6,I.5.1,read - not in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9BRKVINJ\\Prasad and Prasad - 2014 - Deep Recurrent Neural Networks for Time Series Pre.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\923QNX94\\1407.html}
}

@inproceedings{pratt_fcnn_2017,
  title = {{{FCNN}}: {{Fourier Convolutional Neural Networks}}},
  shorttitle = {{{FCNN}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Pratt, Harry and Williams, Bryan and Coenen, Frans and Zheng, Yalin},
  editor = {Ceci, Michelangelo and Hollm{\'e}n, Jaakko and Todorovski, Ljup{\v c}o and Vens, Celine and D{\v z}eroski, Sa{\v s}o},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {786--798},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-71249-9_47},
  abstract = {The Fourier domain is used in computer vision and machine learning as image analysis tasks in the Fourier domain are analogous to spatial domain methods but are achieved using different operations. Convolutional Neural Networks (CNNs) use machine learning to achieve state-of-the-art results with respect to many computer vision tasks. One of the main limiting aspects of CNNs is the computational cost of updating a large number of convolution parameters. Further, in the spatial domain, larger images take exponentially longer than smaller image to train on CNNs due to the operations involved in convolution methods. Consequently, CNNs are often not a viable solution for large image computer vision tasks. In this paper a Fourier Convolution Neural Network (FCNN) is proposed whereby training is conducted entirely within the Fourier domain. The advantage offered is that there is a significant speed up in training time without loss of effectiveness. Using the proposed approach larger images can therefore be processed within viable computation time. The FCNN is fully described and evaluated. The evaluation was conducted using the benchmark Cifar10 and MNIST datasets, and a bespoke fundus retina image dataset. The results demonstrate that convolution in the Fourier domain gives a significant speed up without adversely affecting accuracy. For simplicity the proposed FCNN concept is presented in the context of a basic CNN architecture, however, the FCNN concept has the potential to improve the speed of any neural network system involving convolution.},
  isbn = {978-3-319-71249-9},
  langid = {english},
  keywords = {Convolutional Neural Network (CNNs),Fourier Convolution,Fourier Domain,Large Image,Spatial Kernel}
}

@article{proskuryakov_intelligent_2017,
  title = {Intelligent {{System}} for {{Time Series Forecasting}}},
  author = {Proskuryakov, A.},
  year = {2017},
  month = jan,
  journal = {Procedia Computer Science},
  series = {{{XII International Symposium Intelligent Systems}} 2016, {{INTELS}} 2016, 5-7 {{October}} 2016, {{Moscow}}, {{Russia}}},
  volume = {103},
  pages = {363--369},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2017.01.122},
  urldate = {2021-02-04},
  abstract = {The paper presents a mathematical model of processing and forecasting time series data. The mathematical model based on the methods of artificial neural networks and preliminary data processing using wavelet transforms discribed.},
  langid = {english},
  keywords = {artificial neural networks,forecasting,time series,wavelet transform},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HY9M7U27\\Proskuryakov - 2017 - Intelligent System for Time Series Forecasting.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4EDX4SP9\\S1877050917301230.html}
}

@inproceedings{putra_vlsi_2013,
  title = {{{VLSI Design}} of {{Parallel Sorter}} Based on {{Modified PCM Algorithm}} and {{Batcher}}'s {{Odd-Even Mergesort}}},
  author = {Putra, Rachmad Vidya W.},
  year = {2013},
  month = jun,
  doi = {10.1109/ICTSS.2013.6588108},
  abstract = {Data sorting is an important process in digital signal processing. There were many researches related to data sorting, two of them were about partition and concurrent merging (PCM) algorithm and Batcher's odd-even mergesort network. PCM algorithm will decompose the data in several groups and sort them in two phases, quicksort and mergesort. We captured and modified the idea of PCM algorithm by eliminating unnecessary processes which can be handled directly by Batcher's odd-even mergesort architecture. VLSI design of this parallel sorter is low complexity. It has 2k+1 clock cycles latency, which k represents the number of iterative steps for each kind of sorter block (odd or even). This design has been synthesized for FPGA Altera Cyclone II EP2C35F672C6 as target board.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\NM5PX775\Putra - 2013 - VLSI Design of Parallel Sorter based on Modified P.pdf}
}

@article{qin_comparison_2019,
  title = {Comparison of {{Deep}} Learning Models on Time Series Forecasting : A Case Study of {{Dissolved Oxygen Prediction}}},
  shorttitle = {Comparison of {{Deep}} Learning Models on Time Series Forecasting},
  author = {Qin, Hongqian},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.08414 [cs, eess, stat]},
  eprint = {1911.08414},
  primaryclass = {cs, eess, stat},
  urldate = {2020-02-17},
  abstract = {Deep learning has achieved impressive prediction performance in the field of sequence learning recently. Dissolved oxygen prediction, as a kind of time-series forecasting, is suitable for this technique. Although many researchers have developed hybrid models or variant models based on deep learning techniques, there is no comprehensive and sound comparison among the deep learning models in this field currently. Plus, most previous studies focused on one-step forecasting by using a small data set. As the convenient access to high-frequency data, this paper compares multi-step deep learning forecasting by using walk-forward validation. Specifically, we test Convolutional Neural Network (CNN), Temporal Convolutional Network (TCN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional Recurrent Neural Network (BiRNN) based on the realtime data recorded automatically at a fixed observation point in the Yangtze River from 2012 to 2016. By comparing the average accumulated statistical metrics of root mean square error (RMSE), mean absolute error (MAE), and coefficient of determination in each time step, We find for multi-step time series forecasting, the average performance of each time step does not decrease linearly. GRU outperforms other models with significant advantages.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Applications,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\DU2M4YNW\Qin - 2019 - Comparison of Deep learning models on time series .pdf}
}

@article{qin_dualstage_2017,
  title = {A {{Dual-Stage Attention-Based Recurrent Neural Network}} for {{Time Series Prediction}}},
  author = {Qin, Yao and Song, Dongjin and Chen, Haifeng and Cheng, Wei and Jiang, Guofei and Cottrell, Garrison},
  year = {2017},
  month = apr,
  journal = {arXiv:1704.02971 [cs, stat]},
  eprint = {1704.02971},
  primaryclass = {cs, stat},
  urldate = {2019-04-02},
  abstract = {The Nonlinear autoregressive exogenous (NARX) model, which predicts the current value of a time series based upon its previous values as well as the current and past values of multiple driving (exogenous) series, has been studied for decades. Despite the fact that various NARX models have been developed, few of them can capture the long-term temporal dependencies appropriately and select the relevant driving series to make predictions. In this paper, we propose a dual-stage attention-based recurrent neural network (DA-RNN) to address these two issues. In the first stage, we introduce an input attention mechanism to adaptively extract relevant driving series (a.k.a., input features) at each time step by referring to the previous encoder hidden state. In the second stage, we use a temporal attention mechanism to select relevant encoder hidden states across all time steps. With this dual-stage attention scheme, our model can not only make predictions effectively, but can also be easily interpreted. Thorough empirical studies based upon the SML 2010 dataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can outperform state-of-the-art methods for time series prediction.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\KZYMG732\Qin et al. - 2017 - A Dual-Stage Attention-Based Recurrent Neural Netw.pdf}
}

@article{quinlan_learning_,
  title = {{{LEARNING WITH CONTINUOUS CLASSES}}},
  author = {Quinlan, J R},
  pages = {6},
  abstract = {Some empirical learning tasks are concerned with predicting values rather than the more familiar categories. This paper describes a new system, m5, that constructs tree-based piecewise linear models. Four case studies are presented in which m5 is compared to other methods.},
  langid = {english},
  annotation = {02425},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\UEA8WVPY\Quinlan - LEARNING WITH CONTINUOUS CLASSES.pdf}
}

@article{rabanser_failing_2019,
  title = {Failing {{Loudly}}: {{An Empirical Study}} of {{Methods}} for {{Detecting Dataset Shift}}},
  shorttitle = {Failing {{Loudly}}},
  author = {Rabanser, Stephan and G{\"u}nnemann, Stephan and Lipton, Zachary C.},
  year = {2019},
  month = oct,
  journal = {arXiv:1810.11953 [cs, stat]},
  eprint = {1810.11953},
  primaryclass = {cs, stat},
  urldate = {2020-05-12},
  abstract = {We might hope that when faced with unexpected inputs, well-designed software systems would fire off warnings. Machine learning (ML) systems, however, which depend strongly on properties of their inputs (e.g. the i.i.d. assumption), tend to fail silently. This paper explores the problem of building ML systems that fail loudly, investigating methods for detecting dataset shift, identifying exemplars that most typify the shift, and quantifying shift malignancy. We focus on several datasets and various perturbations to both covariates and label distributions with varying magnitudes and fractions of data affected. Interestingly, we show that across the dataset shifts that we explore, a two-sample-testing-based approach, using pre-trained classifiers for dimensionality reduction, performs best. Moreover, we demonstrate that domain-discriminating approaches tend to be helpful for characterizing shifts qualitatively and determining if they are harmful.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VN7XG3NX\Rabanser et al. - 2019 - Failing Loudly An Empirical Study of Methods for .pdf}
}

@misc{rabanser_intrinsic_2022,
  title = {Intrinsic {{Anomaly Detection}} for {{Multi-Variate Time Series}}},
  author = {Rabanser, Stephan and Januschowski, Tim and Rasul, Kashif and Borchert, Oliver and Kurle, Richard and Gasthaus, Jan and {Bohlke-Schneider}, Michael and Papernot, Nicolas and Flunkert, Valentin},
  year = {2022},
  month = jun,
  number = {arXiv:2206.14342},
  eprint = {2206.14342},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-31},
  abstract = {We introduce a novel, practically relevant variation of the anomaly detection problem in multi-variate time series: intrinsic anomaly detection. It appears in diverse practical scenarios ranging from DevOps to IoT, where we want to recognize failures of a system that operates under the influence of a surrounding environment. Intrinsic anomalies are changes in the functional dependency structure between time series that represent an environment and time series that represent the internal state of a system that is placed in said environment. We formalize this problem, provide under-studied public and new purpose-built data sets for it, and present methods that handle intrinsic anomaly detection. These address the short-coming of existing anomaly detection methods that cannot differentiate between expected changes in the system's state and unexpected ones, i.e., changes in the system that deviate from the environment's influence. Our most promising approach is fully unsupervised and combines adversarial learning and time series representation learning, thereby addressing problems such as label sparsity and subjectivity, while allowing to navigate and improve notoriously problematic anomaly detection data sets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\N7CKB5XX\\Rabanser et al. - 2022 - Intrinsic Anomaly Detection for Multi-Variate Time.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\833NCAAU\\2206.html}
}

@article{rabe_selfattention_2021,
  title = {Self-Attention {{Does Not Need}} \${{O}}(N\^2)\$ {{Memory}}},
  author = {Rabe, Markus N. and Staats, Charles},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.05682 [cs]},
  eprint = {2112.05682},
  primaryclass = {cs},
  urldate = {2021-12-14},
  abstract = {We present a very simple algorithm for attention that requires \$O(1)\$ memory with respect to sequence length and an extension to self-attention that requires \$O(\textbackslash log n)\$ memory. This is in contrast with the frequently stated belief that self-attention requires \$O(n\^2)\$ memory. While the time complexity is still \$O(n\^2)\$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires \$O(\textbackslash sqrt\{n\})\$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5CSY7TIG\\Rabe and Staats - 2021 - Self-attention Does Not Need $O(n^2)$ Memory.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HFTJZBE2\\2112.html}
}

@article{racine_consistent_2000,
  title = {Consistent Cross-Validatory Model-Selection for Dependent Data: Hv-Block Cross-Validation},
  shorttitle = {Consistent Cross-Validatory Model-Selection for Dependent Data},
  author = {Racine, Jeff},
  year = {2000},
  month = nov,
  journal = {Journal of Econometrics},
  volume = {99},
  number = {1},
  pages = {39--61},
  issn = {03044076},
  doi = {10.1016/S0304-4076(00)00030-0},
  urldate = {2019-04-10},
  abstract = {This paper considers the impact of Shao's (1993) recent results regarding the asymptotic inconsistency of model selection via leave-one-out cross-validation on h-block cross-validation, a cross-validatory method for dependent data proposed by Burman, Chow and Nolan (1994, Journal of Time Series Analysis 13, 189\vphantom\{\}207). It is shown that h-block cross-validation is inconsistent in the sense of Shao (1993, Journal of American Statistical Association 88(422), 486\vphantom\{\}495) and therefore is not asymptotically optimal. A modi"cation of the h-block method, dubbed \&hv-block' cross-validation, is proposed which is asymptotically optimal. The proposed approach is consistent for general stationary observations in the sense that the probability of selecting the model with the best predictive ability converges to 1 as the total number of observations approaches in"nity. This extends existing results and yields a new approach which contains leavesopneec-ioaul tccarsoesss.-vAaplipdlaictaiotnio,nlesaavree-nTc-oonustidcerroesds.-valid2a0t0io0n,Ealsnedvihe-rblSoccikenccreosSs.-Ava. liAdlaltiroignhatss reserved.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZER542FU\Racine - 2000 - Consistent cross-validatory model-selection for de.pdf}
}

@article{radford_unsupervised_2015,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.06434 [cs]},
  eprint = {1511.06434},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FID5H9NQ\Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf}
}

@incollection{rajasekaran_randomized_1990,
  title = {Randomized Parallel Selection},
  booktitle = {Foundations of {{Software Technology}} and {{Theoretical Computer Science}}},
  author = {Rajasekaran, Sanguthevar},
  editor = {Goos, G. and Hartmanis, J. and Barstow, D. and Brauer, W. and Brinch Hansen, P. and Gries, D. and Luckham, D. and Moler, C. and Pnueli, A. and Seegm{\"u}ller, G. and Stoer, J. and Wirth, N. and Nori, Kesav V. and Veni Madhavan, C. E.},
  year = {1990},
  volume = {472},
  pages = {215--224},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-53487-3_46},
  urldate = {2022-02-15},
  abstract = {We show that selection on an input of size N can be performed on a P-node hypercube (P = N/(log N)) in time O(n/P) with high probability, provided each node can process all the incident edges in one unit of time (this model is called the parallel model and has been assumed by previous researchers (e.g.,[17])). This result is important in view of a lower bound of Plaxton that implies selection takes {$\Omega$}((N/P)loglog P+log P) time on a P-node hypercube if each node can process only one edge at a time (this model is referred to as the sequential model).},
  isbn = {978-3-540-53487-7 978-3-540-46313-9},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2PZYBEZW\Rajasekaran - 1990 - Randomized parallel selection.pdf}
}

@inproceedings{ralaivola_dynamical_2003,
  title = {Dynamical {{Modeling}} with {{Kernels}} for {{Nonlinear Time Series Prediction}}},
  booktitle = {Proceedings of the 16th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Ralaivola, Liva and {d'Alch{\'e}-Buc}, Florence},
  year = {2003},
  series = {{{NIPS}}'03},
  pages = {129--136},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  urldate = {2019-04-17},
  abstract = {We consider the question of predicting nonlinear time series. Kernel Dynamical Modeling (KDM), a new method based on kernels, is proposed as an extension to linear dynamical models. The kernel trick is used twice: first, to learn the parameters of the model, and second, to compute preimages of the time series predicted in the feature space by means of Support Vector Regression. Our model shows strong connection with the classic Kalman Filter model, with the kernel feature space as hidden state space. Kernel Dynamical Modeling is tested against two benchmark time series and achieves high quality predictions.},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\NJJ9GADN\Ralaivola and D'alchÃ©-buc - Dynamical Modeling with Kernels for Nonlinear Time.pdf}
}

@article{ramachandra_causal_2019,
  title = {Causal Inference for Climate Change Events from Satellite Image Time Series Using Computer Vision and Deep Learning},
  author = {Ramachandra, Vikas},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.11492 [cs, stat]},
  eprint = {1910.11492},
  primaryclass = {cs, stat},
  urldate = {2020-02-19},
  abstract = {We propose a method for causal inference using satellite image time series, in order to determine the treatment effects of interventions which impact climate change, such as deforestation. Simply put, the aim is to quantify the 'before versus after' effect of climate related human driven interventions, such as urbanization; as well as natural disasters, such as hurricanes and forest fires. As a concrete example, we focus on quantifying forest tree cover change/ deforestation due to human led causes. The proposed method involves the following steps. First, we uae computer vision and machine learning/deep learning techniques to detect and quantify forest tree coverage levels over time, at every time epoch. We then look at this time series to identify changepoints. Next, we estimate the expected (forest tree cover) values using a Bayesian structural causal model and projecting/forecasting the counterfactual. This is compared to the values actually observed post intervention, and the difference in the two values gives us the effect of the intervention (as compared to the non intervention scenario, i.e. what would have possibly happened without the intervention). As a specific use case, we analyze deforestation levels before and after the hyperinflation event (intervention) in Brazil (which ended in 1993-94), for the Amazon rainforest region, around Rondonia, Brazil. For this deforestation use case, using our causal inference framework can help causally attribute change/reduction in forest tree cover and increasing deforestation rates due to human activities at various points in time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3ZMD4Z5S\\Ramachandra - 2019 - Causal inference for climate change events from sa.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\US77UNHH\\1910.html}
}

@article{ramachandran_searching_2017,
  title = {Searching for {{Activation Functions}}},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.05941 [cs]},
  eprint = {1710.05941},
  primaryclass = {cs},
  urldate = {2020-01-30},
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, f (x) = x {$\cdot$} sigmoid({$\beta$}x), which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\% for Mobile NASNet-A and 0.6\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7K2JHPRG\Ramachandran et al. - 2017 - Searching for Activation Functions.pdf}
}

@inproceedings{rangapuram_coherent_2023,
  title = {Coherent {{Probabilistic Forecasting}} of {{Temporal Hierarchies}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Rangapuram, Syama Sundar and Kapoor, Shubham and Nirwan, Rajbir Singh and Mercado, Pedro and Januschowski, Tim and Wang, Yuyang and {Bohlke-Schneider}, Michael},
  year = {2023},
  month = apr,
  pages = {9362--9376},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-05-31},
  abstract = {Forecasts at different time granularities are required in practice for addressing various business problems starting from short-term operational to medium-term tactical and to long-term strategic planning. These forecasting problems are usually treated independently by learning different ML models which results in forecasts that are not consistent with the temporal aggregation structure, leading to inefficient decision making. Some of the recent work addressed this problem, however, it uses a post-hoc reconciliation strategy, which results in sub-optimal results and cannot produce probabilistic forecasts. In this paper, we present a global model that produces coherent, probabilistic forecasts for different time granularities by learning joint embeddings for the different aggregation levels with graph neural networks and temporal reconciliation. Temporal reconciliation not only enables consistent decisions for business problems across different planning horizons but also improves the quality of forecasts at finer time granularities. A thorough empirical evaluation illustrates the benefits of the proposed method.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\XYNHWZ56\Rangapuram et al. - 2023 - Coherent Probabilistic Forecasting of Temporal Hie.pdf}
}

@incollection{rangapuram_deep_2018,
  title = {Deep {{State Space Models}} for {{Time Series Forecasting}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Rangapuram, Syama Sundar and Seeger, Matthias W and Gasthaus, Jan and Stella, Lorenzo and Wang, Yuyang and Januschowski, Tim},
  year = {2018},
  pages = {7785--7794},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-29},
  keywords = {read - in related work},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PEFVUPCV\\Rangapuram et al. - 2018 - Deep State Space Models for Time Series Forecastin.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\SP9J28JS\\8004-deep-state-space-models-for-time-series-forecasting.html}
}

@inproceedings{rangapuram_endtoend_2021,
  title = {End-to-{{End Learning}} of {{Coherent Probabilistic Forecasts}} for {{Hierarchical Time Series}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Rangapuram, Syama Sundar and Werner, Lucien D. and Benidis, Konstantinos and Mercado, Pedro and Gasthaus, Jan and Januschowski, Tim},
  year = {2021},
  month = jul,
  pages = {8832--8843},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-10-06},
  abstract = {This paper presents a novel approach for hierarchical time series forecasting that produces coherent, probabilistic forecasts without requiring any explicit post-processing reconciliation. Unlike the state-of-the-art, the proposed method simultaneously learns from all time series in the hierarchy and incorporates the reconciliation step into a single trainable model. This is achieved by applying the reparameterization trick and casting reconciliation as an optimization problem with a closed-form solution. These model features make end-to-end learning of hierarchical forecasts possible, while accomplishing the challenging task of generating forecasts that are both probabilistic and coherent. Importantly, our approach also accommodates general aggregation constraints including grouped and temporal hierarchies. An extensive empirical evaluation on real-world hierarchical datasets demonstrates the advantages of the proposed approach over the state-of-the-art.},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7HMQCJTQ\\Rangapuram et al. - 2021 - End-to-End Learning of Coherent Probabilistic Fore.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XEQZD8TW\\Rangapuram et al. - 2021 - End-to-End Learning of Coherent Probabilistic Fore.pdf}
}

@article{rashmi_dart_,
  title = {{{DART}}: {{Dropouts}} Meet {{Multiple Additive Regression Trees}}},
  author = {Rashmi, K V and {Gilad-Bachrach}, Ran},
  pages = {9},
  langid = {english},
  annotation = {00032},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TEI9K6JH\Rashmi and Gilad-Bachrach - DART Dropouts meet Multiple Additive Regression T.pdf}
}

@inproceedings{rasul_autoregressive_2021,
  title = {Autoregressive {{Denoising Diffusion Models}} for {{Multivariate Probabilistic Time Series Forecasting}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Rasul, Kashif and Seward, Calvin and Schuster, Ingmar and Vollgraf, Roland},
  year = {2021},
  month = jul,
  pages = {8857--8868},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-03-20},
  abstract = {In this work, we propose TimeGrad, an autoregressive model for multivariate probabilistic time series forecasting which samples from the data distribution at each time step by estimating its gradient. To this end, we use diffusion probabilistic models, a class of latent variable models closely connected to score matching and energy-based methods. Our model learns gradients by optimizing a variational bound on the data likelihood and at inference time converts white noise into a sample of the distribution of interest through a Markov chain using Langevin sampling. We demonstrate experimentally that the proposed autoregressive denoising diffusion model is the new state-of-the-art multivariate probabilistic forecasting method on real-world data sets with thousands of correlated dimensions. We hope that this method is a useful tool for practitioners and lays the foundation for future research in this area.},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\F8T2Z5C3\\Rasul et al. - 2021 - Autoregressive Denoising Diffusion Models for Mult.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\JDDIN572\\Rasul et al. - 2021 - Autoregressive Denoising Diffusion Models for Mult.pdf}
}

@inproceedings{rasul_multivariate_2022,
  title = {Multivariate {{Probabilistic Time Series Forecasting}} via {{Conditioned Normalizing Flows}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Rasul, Kashif and Sheikh, Abdul-Saboor and Schuster, Ingmar and Bergmann, Urs M. and Vollgraf, Roland},
  year = {2022},
  month = feb,
  urldate = {2023-03-20},
  abstract = {Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multi-variate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multi-variate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\IX6UP9WG\Rasul et al. - 2022 - Multivariate Probabilistic Time Series Forecasting.pdf}
}

@article{rayskin_dynamical_2020,
  title = {Dynamical Systems' Models for the Prediction of Multi-Variable Time Series. {{Wikipedia}}'s Traffic Example},
  author = {Rayskin, Victoria},
  year = {2020},
  month = jan,
  journal = {arXiv:1912.06939 [cs, math]},
  eprint = {1912.06939},
  primaryclass = {cs, math},
  urldate = {2020-02-17},
  abstract = {The models VAR, ARIMA, Holt-Winters, are frequently used for short-term forecasts of multivariate time series. In this paper we consider models constructed with the help of dynamical systems that have relatively simple limiting behavior. Switching between different trajectories of the phase portrait, we obtain a high precision prediction. Moreover, the dynamical system approach provides the global qualitative picture of the model's phase portrait, and allows us to discuss multidimensional patterns and long-term properties of the process. The simple limiting behavior allows us to associate different trends with different process's realization scenarios that can be influenced by externalities. We demonstrate these ideas using the examples of the Wikipedia's traffic of Readers, Contributors and Edits. First, we consider the two-dimensional model, predicting the traffic of Readers and Edits. The prediction precision is higher than the two-dimensional VAR prediction. Different trends (corresponding to different fixed points) can be associated with different platform's incentives. Then, adding the Contributors data, we discuss the three-dimensional model (more precise than the three-dimensional VAR). It provides a more accurate short-term prediction of Edits than the two-dimensional dynamic model. The global picture shows that the number of new Edits tends to decline in the future, while the number of new Contributors and Readers will grow in the long run.},
  archiveprefix = {arxiv},
  keywords = {{34N, 62H, 91B84, 62M10},Computer Science - Social and Information Networks,G.3,Mathematics - Dynamical Systems},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Z9WBFVKN\\Rayskin - 2020 - Dynamical systems' models for the prediction of mu.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3MUGBZS6\\1912.html}
}

@article{rehman_individual_2019,
  title = {Individual and Combination Approaches to Forecasting Hierarchical Time Series with Correlated Data: An Empirical Study},
  shorttitle = {Individual and Combination Approaches to Forecasting Hierarchical Time Series with Correlated Data},
  author = {Rehman, Hakeem-Ur and Wan, Guohua and Ullah, Azmat and Shaukat, Badiea},
  year = {2019},
  month = jul,
  journal = {Journal of Management Analytics},
  volume = {6},
  number = {3},
  pages = {231--249},
  publisher = {{Taylor \& Francis}},
  issn = {2327-0012},
  doi = {10.1080/23270012.2019.1629342},
  urldate = {2021-02-04},
  abstract = {Hierarchical time series arise in manufacturing and service industries when the products or services have the hierarchical structure, and top-down and bottom-up methods are commonly used to forecast the hierarchical time series. One of the critical factors that affect the performance of the two methods is the correlation between the data series. This study attempts to resolve the problem and shows that the top-down method performs better when data have high positive correlation compared to high negative correlation and combination of forecasting methods may be the best solution when there is no evidence of the correlationship. We conduct the computational experiments using 240 monthly data series from the `Industrial' category of the M3-Competition and test twelve combination methods for the hierarchical data series. The results show that the regression-based, VAR-COV and the Rank-based methods perform better compared to the other methods.},
  keywords = {combination forecasting methods,correlation,hierarchical time series,individual forecasting methods},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\KGZAYR4E\23270012.2019.html}
}

@article{richins_ai_2020,
  title = {{{AI Tax}}: {{The Hidden Cost}} of {{AI Data Center Applications}}},
  shorttitle = {{{AI Tax}}},
  author = {Richins, Daniel and Doshi, Dharmisha and Blackmore, Matthew and Nair, Aswathy Thulaseedharan and Pathapati, Neha and Patel, Ankit and Daguman, Brainard and Dobrijalowski, Daniel and Illikkal, Ramesh and Long, Kevin and Zimmerman, David and Reddi, Vijay Janapa},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.10571 [cs]},
  eprint = {2007.10571},
  primaryclass = {cs},
  urldate = {2020-09-04},
  abstract = {DANIEL RICHINS, The University of Texas at Austin and Intel DHARMISHA DOSHI, Intel MATTHEW BLACKMORE, Intel ASWATHY THULASEEDHARAN NAIR, Intel NEHA PATHAPATI, Intel ANKIT PATEL, Intel BRAINARD DAGUMAN, Intel DANIEL DOBRIJALOWSKI, Intel RAMESH ILLIKKAL, Intel KEVIN LONG, Intel DAVID ZIMMERMAN, Intel VIJAY JANAPA REDDI, Harvard University and The University of Texas at Austin Artificial intelligence and machine learning are experiencing widespread adoption in industry and academia. This has been driven by rapid advances in the applications and accuracy of AI through increasingly complex algorithms and models; this, in turn, has spurred research into specialized hardware AI accelerators. Given the rapid pace of advances, it is easy to forget that they are often developed and evaluated in a vacuum without considering the full application environment. This paper emphasizes the need for a holistic, end-to-end analysis of AI workloads and reveals the ``AI tax.'' We deploy and characterize Face Recognition in an edge data center. The application is an AI-centric edge video analytics application built using popular open source infrastructure and ML tools. Despite using state-of-the-art AI and ML algorithms, the application relies heavily on preand post-processing code. As AI-centric applications benefit from the acceleration promised by accelerators, we find they impose stresses on the hardware and software infrastructure: storage and network bandwidth become major bottlenecks with increasing AI acceleration. By specializing for AI applications, we show that a purpose-built edge data center can be designed for the stresses of accelerated AI at 15\% lower TCO than one derived from homogeneous servers and infrastructure. CCS Concepts: \textbullet{} Computing methodologies \textrightarrow{} Machine learning; Artificial intelligence; \textbullet{} Software and its engineering \textrightarrow{} Software performance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {C.4,{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Performance,I.2},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\4SU4R2CG\Richins et al. - 2020 - AI Tax The Hidden Cost of AI Data Center Applicat.pdf}
}

@article{rigby_generalized_2005,
  title = {Generalized Additive Models for Location, Scale and Shape},
  author = {Rigby, R. A. and Stasinopoulos, D. M.},
  year = {2005},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {54},
  number = {3},
  pages = {507--554},
  issn = {1467-9876},
  doi = {10.1111/j.1467-9876.2005.00510.x},
  urldate = {2020-11-18},
  abstract = {Summary. A general class of statistical models for a univariate response variable is presented which we call the generalized additive model for location, scale and shape (GAMLSS). The model assumes independent observations of the response variable y given the parameters, the explanatory variables and the values of the random effects. The distribution for the response variable in the GAMLSS can be selected from a very general family of distributions including highly skew or kurtotic continuous and discrete distributions. The systematic part of the model is expanded to allow modelling not only of the mean (or location) but also of the other parameters of the distribution of y, as parametric and/or additive nonparametric (smooth) functions of explanatory variables and/or random-effects terms. Maximum (penalized) likelihood estimation is used to fit the (non)parametric models. A Newton\textendash Raphson or Fisher scoring algorithm is used to maximize the (penalized) likelihood. The additive terms in the model are fitted by using a backfitting algorithm. Censored data are easily incorporated into the framework. Five data sets from different fields of application are analysed to emphasize the generality of the GAMLSS class of models.},
  langid = {english},
  keywords = {Beta\textendash binomial distribution,Box\textendash Cox transformation,Centile estimation,Cubic smoothing splines,Generalized linear mixed model,LMS method,Negative binomial distribution,Non-normality,Nonparametric models,Overdispersion,Penalized likelihood,Random effects,Skewness and kurtosis},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\E7K3AYN2\\Rigby and Stasinopoulos - 2005 - Generalized additive models for location, scale an.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Z69RSM8R\\j.1467-9876.2005.00510.html}
}

@article{roberts_giniregularized_2017,
  title = {Gini-Regularized {{Optimal Transport}} with an {{Application}} to {{Spatio-Temporal Forecasting}}},
  author = {Roberts, Lucas and Razoumov, Leo and Su, Lin and Wang, Yuyang},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.02512 [cs, stat]},
  eprint = {1712.02512},
  primaryclass = {cs, stat},
  urldate = {2019-10-02},
  abstract = {Rapidly growing product lines and services require a finer-granularity forecast that considers geographic locales. However the open question remains, how to assess the quality of a spatio-temporal forecast? In this manuscript we introduce a metric to evaluate spatio-temporal forecasts. This metric is based on an Opti- mal Transport (OT) problem. The metric we propose is a constrained OT objec- tive function using the Gini impurity function as a regularizer. We demonstrate through computer experiments both the qualitative and the quantitative charac- teristics of the Gini regularized OT problem. Moreover, we show that the Gini regularized OT problem converges to the classical OT problem, when the Gini regularized problem is considered as a function of \{\textbackslash lambda\}, the regularization parame-ter. The convergence to the classical OT solution is faster than the state-of-the-art Entropic-regularized OT[Cuturi, 2013] and results in a numerically more stable algorithm.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00004},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7W5DTIVX\Roberts et al. - 2017 - Gini-regularized Optimal Transport with an Applica.pdf}
}

@incollection{robinson_nonstationary_2009,
  title = {Non-Stationary Dynamic {{Bayesian}} Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}} 21},
  author = {Robinson, Joshua W. and Hartemink, Alexander J.},
  editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
  year = {2009},
  pages = {1369--1376},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\AI3KHBBT\\Robinson and Hartemink - 2009 - Non-stationary dynamic Bayesian networks.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RRE45XG7\\3571-non-stationary-dynamic-bayesian-networks.html}
}

@inproceedings{romano_conformalized_2019,
  title = {Conformalized {{Quantile Regression}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-07},
  abstract = {Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\IJNGHTDG\Romano et al. - 2019 - Conformalized Quantile Regression.pdf}
}

@article{ross_reduction_,
  title = {A {{Reduction}} of {{Imitation Learning}} and {{Structured Prediction}} to {{No-Regret Online Learning}}},
  author = {Ross, St{\'e}phane and Gordon, Geoffrey J and Bagnell, J Andrew},
  pages = {9},
  abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches (Daum\'e III et al., 2009; Ross and Bagnell, 2010) provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
  langid = {english},
  annotation = {00786},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5CA5DMMP\Ross et al. - A Reduction of Imitation Learning and Structured P.pdf}
}

@inproceedings{rotabulo_neural_2014,
  title = {Neural {{Decision Forests}} for {{Semantic Image Labelling}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rota Bulo, Samuel and Kontschieder, Peter},
  year = {2014},
  month = jun,
  pages = {81--88},
  publisher = {{IEEE}},
  address = {{Columbus, OH}},
  doi = {10.1109/CVPR.2014.18},
  urldate = {2019-08-30},
  abstract = {In this work we present Neural Decision Forests, a novel approach to jointly tackle data representation- and discriminative learning within randomized decision trees. Recent advances of deep learning architectures demonstrate the power of embedding representation learning within the classifier \textendash{} An idea that is intuitively supported by the hierarchical nature of the decision forest model where the input space is typically left unchanged during training and testing. We bridge this gap by introducing randomized MultiLayer Perceptrons (rMLP) as new split nodes which are capable of learning non-linear, data-specific representations and taking advantage of them by finding optimal predictions for the emerging child nodes. To prevent overfitting, we i) randomly select the image data fed to the input layer, ii) automatically adapt the rMLP topology to meet the complexity of the data arriving at the node and iii) introduce an 1-norm based regularization that additionally sparsifies the network. The key findings in our experiments on three different semantic image labelling datasets are consistently improved results and significantly compressed trees compared to conventional classification trees.},
  isbn = {978-1-4799-5118-5},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5G3LAJTS\Bulo_Neural_Decision_Forests_2014_CVPR_paper.pdf}
}

@book{roth_multivariate_2013,
  title = {On the {{Multivariate}} t {{Distribution}}},
  author = {Roth, Michael},
  year = {2013},
  month = apr,
  abstract = {This technical report summarizes a number of results for the multivariate t distribution which can exhibit heavier tails than the Gaussian distribution. It is shown how t random variables can be generated, the probability density function (pdf) is derived, and marginal and conditional densities of partitioned t random vectors are presented. Moreover, a brief comparison with the multivariate Gaussian distribution is provided. The derivations of several results are given in an extensive appendix.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QEL35X34\Roth - 2013 - On the Multivariate t Distribution.pdf}
}

@incollection{rubanova_latent_2019,
  title = {Latent {{Ordinary Differential Equations}} for {{Irregularly-Sampled Time Series}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Rubanova, Yulia and Chen, Tian Qi and Duvenaud, David K},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {5320--5330},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-03-12},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\KZXYWIZA\\Rubanova et al. - 2019 - Latent Ordinary Differential Equations for Irregul.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UNFCGQBN\\8773-latent-ordinary-differential-equations-for-irregularly-sampled-time-series.html}
}

@article{rusu_policy_2015,
  title = {Policy {{Distillation}}},
  author = {Rusu, Andrei A. and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.06295 [cs]},
  eprint = {1511.06295},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PM6ZDW7S\Rusu et al. - 2015 - Policy Distillation.pdf}
}

@article{rusu_progressive_2016,
  title = {Progressive {{Neural Networks}}},
  author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.04671 [cs]},
  eprint = {1606.04671},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Learning to solve complex sequences of tasks\textemdash while both leveraging transfer and avoiding catastrophic forgetting\textemdash remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QJ6XVPWT\Rusu et al. - 2016 - Progressive Neural Networks.pdf}
}

@incollection{ryabko_reducing_2012,
  title = {Reducing Statistical Time-Series Problems to Binary Classification},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Ryabko, Daniil and Mary, Jeremie},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {2060--2068},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XS9G69HU\\Ryabko and Mary - 2012 - Reducing statistical time-series problems to binar.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\KC6NM5FS\\4510-reducing-statistical-time-series-problems-to-binary-classification.html}
}

@article{sagheer_unsupervised_2019,
  title = {Unsupervised {{Pre-training}} of a {{Deep LSTM-based Stacked Autoencoder}} for {{Multivariate Time Series Forecasting Problems}}},
  author = {Sagheer, Alaa and Kotb, Mostafa},
  year = {2019},
  month = dec,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {19038},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-55320-6},
  urldate = {2021-02-04},
  abstract = {Currently, most real-world time series datasets are multivariate and are rich in dynamical information of the underlying system. Such datasets are attracting much attention; therefore, the need for accurate modelling of such high-dimensional datasets is increasing. Recently, the deep architecture of the recurrent neural network (RNN) and its variant long short-term memory (LSTM) have been proven to be more accurate than traditional statistical methods in modelling time series data. Despite the reported advantages of the deep LSTM model, its performance in modelling multivariate time series (MTS) data has not been satisfactory, particularly when attempting to process highly non-linear and long-interval MTS datasets. The reason is that the supervised learning approach initializes the neurons randomly in such recurrent networks, disabling the neurons that ultimately must properly learn the latent features of the correlated variables included in the MTS dataset. In this paper, we propose a pre-trained LSTM-based stacked autoencoder (LSTM-SAE) approach in an unsupervised learning fashion to replace the random weight initialization strategy adopted in deep LSTM recurrent networks. For evaluation purposes, two different case studies that include real-world datasets are investigated, where the performance of the proposed approach compares favourably with the deep LSTM approach. In addition, the proposed approach outperforms several reference models investigating the same case studies. Overall, the experimental results clearly show that the unsupervised pre-training approach improves the performance of deep LSTM and leads to better and faster convergence than other models.},
  copyright = {2019 The Author(s)},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7EX8R25F\Sagheer and Kotb - 2019 - Unsupervised Pre-training of a Deep LSTM-based Sta.pdf}
}

@article{saleh_ridge_2015,
  title = {Ridge {{Autoregression Estimation}}: {{LS Method}}},
  shorttitle = {Ridge {{Autoregression Estimation}}},
  author = {Saleh, A. K. MD Ehsanes and Ghania, Amal F.},
  year = {2015},
  month = aug,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {44},
  number = {15},
  pages = {3303--3320},
  issn = {0361-0926},
  doi = {10.1080/03610926.2013.857866},
  urldate = {2019-09-06},
  abstract = {In an AR (p)-model, least-squares estimation of the parameters is considered when it is suspected that the parameters may belong to a linear subspace and the estimated covariance matrix is ill-conditioned. Accordingly, we define five estimators and study their properties in an asymptotic setup to discover dominance properties based on asymptotic distributional bias (ADB), MSE (ADMSE) matrices, and under quadratic risks (ADQR).},
  annotation = {00002},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3TW8TY6M\03610926.2013.html}
}

@incollection{salimans_weight_2016,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  shorttitle = {Weight {{Normalization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Salimans, Tim and Kingma, Durk P},
  year = {2016},
  pages = {901--909},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-01-17},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DRKSU3YB\\Salimans and Kingma - 2016 - Weight Normalization A Simple Reparameterization .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HTGCZ2CG\\6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-net.html}
}

@article{salinas_deepar_2019,
  title = {{{DeepAR}}: {{Probabilistic}} Forecasting with Autoregressive Recurrent Networks},
  shorttitle = {{{DeepAR}}},
  author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
  year = {2019},
  month = oct,
  journal = {International Journal of Forecasting},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.07.001},
  urldate = {2020-01-17},
  abstract = {Probabilistic forecasting, i.e., estimating a time series' future probability distribution given its past, is a key enabler for optimizing business processes. In retail businesses, for example, probabilistic demand forecasts are crucial for having the right inventory available at the right time and in the right place. This paper proposes DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an autoregressive recurrent neural network model on a large number of related time series. We demonstrate how the application of deep learning techniques to forecasting can overcome many of the challenges that are faced by widely-used classical approaches to the problem. By means of extensive empirical evaluations on several real-world forecasting datasets, we show that our methodology produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.},
  langid = {english},
  keywords = {Big data,Deep learning,Demand forecasting,Neural networks,Probabilistic forecasting},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MAEUARWZ\\Salinas et al. - 2019 - DeepAR Probabilistic forecasting with autoregress.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BKDHYUJZ\\S0169207019301888.html}
}

@incollection{salinas_highdimensional_2019,
  title = {High-Dimensional Multivariate Forecasting with Low-Rank {{Gaussian Copula Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Salinas, David and {Bohlke-Schneider}, Michael and Callot, Laurent and Medico, Roberto and Gasthaus, Jan},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {6827--6837},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-02-18},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CGCDSGLS\\Salinas et al. - 2019 - High-dimensional multivariate forecasting with low.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\J6RQP8EP\\8907-high-dimensional-multivariate-forecasting-with-low-rank-gaussian-copula-processes.html}
}

@inproceedings{salinas_highdimensional_2019a,
  title = {High-Dimensional Multivariate Forecasting with Low-Rank {{Gaussian Copula Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Salinas, David and {Bohlke-Schneider}, Michael and Callot, Laurent and Medico, Roberto and Gasthaus, Jan},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-06},
  abstract = {Predicting the dependencies between observations from multiple time series is critical for applications such as anomaly detection, financial risk management, causal analysis, or demand forecasting. However, the computational and numerical difficulties of estimating time-varying and high-dimensional covariance matrices often limits existing methods to handling at most a few hundred dimensions or requires making strong assumptions on the dependence between series. We propose to combine an RNN-based time series model with a Gaussian copula process output model with a low-rank covariance structure to reduce the computational complexity and handle non-Gaussian marginal distributions. This permits to drastically reduce the number of parameters and consequently allows the modeling of time-varying correlations of thousands of time series. We show on several real-world datasets that our method provides significant accuracy improvements over state-of-the-art baselines and perform an ablation study analyzing the contributions of the different components of our model.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JEBLVAKS\Salinas et al. - 2019 - High-dimensional multivariate forecasting with low.pdf}
}

@misc{salinas_optimizing_2023,
  title = {Optimizing {{Hyperparameters}} with {{Conformal Quantile Regression}}},
  author = {Salinas, David and Golebiowski, Jacek and Klein, Aaron and Seeger, Matthias and Archambeau, Cedric},
  year = {2023},
  month = may,
  number = {arXiv:2305.03623},
  eprint = {2305.03623},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-31},
  abstract = {Many state-of-the-art hyperparameter optimization (HPO) algorithms rely on model-based optimizers that learn surrogate models of the target function to guide the search. Gaussian processes are the de facto surrogate model due to their ability to capture uncertainty but they make strong assumptions about the observation noise, which might not be warranted in practice. In this work, we propose to leverage conformalized quantile regression which makes minimal assumptions about the observation noise and, as a result, models the target function in a more realistic and robust fashion which translates to quicker HPO convergence on empirical benchmarks. To apply our method in a multi-fidelity setting, we propose a simple, yet effective, technique that aggregates observed results across different resource levels and outperforms conventional methods across many empirical tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EXMRJZ3Q\\Salinas et al. - 2023 - Optimizing Hyperparameters with Conformal Quantile.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZRRZAXE6\\2305.html}
}

@article{santoro_oneshot_2016,
  title = {One-Shot {{Learning}} with {{Memory-Augmented Neural Networks}}},
  author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  year = {2016},
  month = may,
  journal = {arXiv:1605.06065 [cs]},
  eprint = {1605.06065},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of ``one-shot learning.'' Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory locationbased focusing mechanisms.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LR52MTN5\Santoro et al. - 2016 - One-shot Learning with Memory-Augmented Neural Net.pdf}
}

@misc{saucecat_boosting_2017,
  title = {Boosting Algorithm: {{GBM}}},
  shorttitle = {Boosting Algorithm},
  author = {SauceCat},
  year = {2017},
  month = may,
  journal = {Medium},
  urldate = {2019-08-06},
  abstract = {This article continues the previous post Boosting algorithm: AdaBoost. This time we will turn to GBM (Gradient Boosting Machine).},
  howpublished = {https://towardsdatascience.com/boosting-algorithm-gbm-97737c63daa3},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YQ9SDLJE\boosting-algorithm-gbm-97737c63daa3.html}
}

@article{schaer_demand_2019,
  title = {Demand Forecasting with User-Generated Online Information},
  author = {Schaer, Oliver and Kourentzes, Nikolaos and Fildes, Robert},
  year = {2019},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {Special {{Section}}: {{Supply Chain Forecasting}}},
  volume = {35},
  number = {1},
  pages = {197--212},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2018.03.005},
  urldate = {2019-03-19},
  abstract = {Recently, there has been substantial research on the augmentation of aggregate forecasts with individual consumer data from internet platforms, such as search traffic or social network shares. Although the majority of studies have reported increases in accuracy, many exhibit design weaknesses, including a lack of adequate benchmarks or rigorous evaluation. Furthermore, their usefulness over the product life-cycle has not been investigated, even though this may change, as consumers may search initially for pre-purchase information, but later for after-sales support. This study begins by reviewing the relevant literature, then attempts to support the key findings using two forecasting case studies. Our findings are in stark contrast to those in the previous literature, as we find that established univariate forecasting benchmarks, such as exponential smoothing, consistently perform better those that include online information. Our research underlines the need for a thorough forecast evaluation and argues that the usefulness of online platform data for supporting operational decisions may be limited.},
  keywords = {Electronic word-of-mouth,Google trends,Leading indicators,Product life-cycle,Search traffic,Social media},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\U8MRY9SR\\Schaer et al. - 2019 - Demand forecasting with user-generated online info.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\X46JV8LE\\S0169207018300505.html}
}

@article{schafer_shrinkage_2005,
  title = {A {{Shrinkage Approach}} to {{Large-Scale Covariance Matrix Estimation}} and {{Implications}} for {{Functional Genomics}}},
  author = {Sch{\"a}fer, Juliane and Strimmer, Korbinian},
  year = {2005},
  month = jan,
  journal = {Statistical Applications in Genetics and Molecular Biology},
  volume = {4},
  number = {1},
  issn = {1544-6115, 2194-6302},
  doi = {10.2202/1544-6115.1175},
  urldate = {2022-06-30},
  abstract = {Inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. Clearly, the widely used standard covariance and correlation estimators are ill-suited for this purpose. As statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the Ledoit-Wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2A98KN2Q\SchÃ¤fer and Strimmer - 2005 - A Shrinkage Approach to Large-Scale Covariance Mat.pdf}
}

@article{schelter_challenges_,
  title = {On {{Challenges}} in {{Machine Learning Model Management}}},
  author = {Schelter, Sebastian and Biessmann, Felix and Januschowski, Tim and Salinas, David and Seufert, Stephan and Szarvas, Gyuri},
  pages = {11},
  abstract = {The training, maintenance, deployment, monitoring, organization and documentation of machine learning (ML) models \textendash{} in short model management \textendash{} is a critical task in virtually all production ML use cases. Wrong model management decisions can lead to poor performance of a ML system and can result in high maintenance cost. As both research on infrastructure as well as on algorithms is quickly evolving, there is a lack of understanding of challenges and best practices for ML model management. Therefore, this field is receiving increased attention in recent years, both from the data management as well as from the ML community. In this paper, we discuss a selection of ML use cases, develop an overview over conceptual, engineering, and data-processing related challenges arising in the management of the corresponding ML models, and point out future research directions.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5C7AC5A5\Schelter et al. - On Challenges in Machine Learning Model Management.pdf}
}

@article{schlegel_rigorous_2019,
  title = {Towards a {{Rigorous Evaluation}} of {{XAI Methods}} on {{Time Series}}},
  author = {Schlegel, Udo and Arnout, Hiba and {El-Assady}, Mennatallah and Oelke, Daniela and Keim, Daniel A.},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.07082 [cs]},
  eprint = {1909.07082},
  primaryclass = {cs},
  urldate = {2019-11-12},
  abstract = {Explainable Artificial Intelligence (XAI) methods are typically deployed to explain and debug black-box machine learning models. However, most proposed XAI methods are black-boxes themselves and designed for images. Thus, they rely on visual interpretability to evaluate and prove explanations. In this work, we apply XAI methods previously used in the image and text-domain on time series. We present a methodology to test and evaluate various XAI methods on time series by introducing new verification techniques to incorporate the temporal dimension. We further conduct preliminary experiments to assess the quality of selected XAI method explanations with various verification methods on a range of datasets and inspecting quality metrics on it. We demonstrate that in our initial experiments, SHAP works robust for all models, but others like DeepLIFT, LRP, and Saliency Maps work better with specific architectures.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7AZ56MTT\Schlegel et al. - 2019 - Towards a Rigorous Evaluation of XAI Methods on Ti.pdf}
}

@article{schmidheiny_panel_,
  title = {Panel {{Data}}: {{Fixed}} and {{Random Effects}}},
  author = {Schmidheiny, Kurt},
  pages = {8},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\9RUMLFFQ\Schmidheiny - Panel Data Fixed and Random Eï¬€ects.pdf}
}

@inproceedings{schubert_numerically_2018,
  title = {Numerically Stable Parallel Computation of (Co-)Variance},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Scientific}} and {{Statistical Database Management}}},
  author = {Schubert, Erich and Gertz, Michael},
  year = {2018},
  month = jul,
  series = {{{SSDBM}} '18},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3221269.3223036},
  urldate = {2022-10-24},
  abstract = {With the advent of big data, we see an increasing interest in computing correlations in huge data sets with both many instances and many variables. Essential descriptive statistics such as the variance, standard deviation, covariance, and correlation can suffer from a numerical instability known as "catastrophic cancellation" that can lead to problems when naively computing these statistics with a popular textbook equation. While this instability has been discussed in the literature already 50 years ago, we found that even today, some high-profile tools still employ the instable version. In this paper, we study a popular incremental technique originally proposed by Welford, which we extend to weighted covariance and correlation. We also discuss strategies for further improving numerical precision, how to compute such statistics online on a data stream, with exponential aging, with missing data, and a batch parallelization for both high performance and numerical precision. We demonstrate when the numerical instability arises, and the performance of different approaches under these conditions. We showcase applications from the classic computation of variance as well as advanced applications such as stock market analysis with exponentially weighted moving models and Gaussian mixture modeling for cluster analysis that all benefit from this approach.},
  isbn = {978-1-4503-6505-5},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\W2R65IHQ\Schubert and Gertz - 2018 - Numerically stable parallel computation of (co-)va.pdf}
}

@article{schulman_trust_,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John},
  pages = {9},
  abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QQIVZHZK\Schulman - Trust Region Policy Optimization.pdf}
}

@article{scott_predicting_2013,
  title = {Predicting the {{Present}} with {{Bayesian Structural Time Series}}},
  author = {Scott, Steven L and Varian, Hal},
  year = {2013},
  pages = {21},
  abstract = {This article describes a system for short term forecasting based on an ensemble prediction that averages over different combinations of predictors. The system combines a structural time series model for the target series with regression component capturing the contributions of contemporaneous search query data. A spike-and-slab prior on the regression coefficients induces sparsity, dramatically reducing the size of the regression problem. Our system averages over potential contributions from a very large set of models and gives easily digested reports of which coefficients are likely to be important. We illustrate with applications to initial claims for unemployment benefits and to retail sales. Although our exposition focuses on using search engine data to forecast economic time series, the underlying statistical methods can be applied to more general short term forecasting with large numbers of contemporaneous predictors.},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\Y2T8A4RV\Scott and Varian - Predicting the Present with Bayesian Structural Ti.pdf}
}

@article{seeger_approximate_2017,
  title = {Approximate {{Bayesian Inference}} in {{Linear State Space Models}} for {{Intermittent Demand Forecasting}} at {{Scale}}},
  author = {Seeger, Matthias and Rangapuram, Syama and Wang, Yuyang and Salinas, David and Gasthaus, Jan and Januschowski, Tim and Flunkert, Valentin},
  year = {2017},
  month = sep,
  journal = {arXiv:1709.07638 [cs, stat]},
  eprint = {1709.07638},
  primaryclass = {cs, stat},
  urldate = {2019-04-24},
  abstract = {We present a scalable and robust Bayesian inference method for linear state space models. The method is applied to demand forecasting in the context of a large e-commerce platform, paying special attention to intermittent and bursty target statistics. Inference is approximated by the Newton-Raphson algorithm, reduced to linear-time Kalman smoothing, which allows us to operate on several orders of magnitude larger problems than previous related work. In a study on large real-world sales datasets, our method outperforms competing approaches on fast and medium moving items.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00002},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\XPLZFAPQ\Seeger et al. - 2017 - Approximate Bayesian Inference in Linear State Spa.pdf}
}

@incollection{seeger_bayesian_2016,
  title = {Bayesian {{Intermittent Demand Forecasting}} for {{Large Inventories}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Seeger, Matthias W and Salinas, David and Flunkert, Valentin},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {4646--4654},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-24},
  keywords = {read - in related work},
  annotation = {00017},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\AT7SDICB\\Seeger et al. - 2016 - Bayesian Intermittent Demand Forecasting for Large.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\A25MV3Q2\\6313-bayesian-intermittent-demand-forecasting-for-large-inventories.html}
}

@incollection{sen_think_2019,
  title = {Think {{Globally}}, {{Act Locally}}: {{A Deep Neural Network Approach}} to {{High-Dimensional Time Series Forecasting}}},
  shorttitle = {Think {{Globally}}, {{Act Locally}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Sen, Rajat and Yu, Hsiang-Fu and Dhillon, Inderjit S},
  year = {2019},
  pages = {4838--4847},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-01-17},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\W6YJKS8B\\Sen et al. - 2019 - Think Globally, Act Locally A Deep Neural Network.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\P4HKPFTX\\8730-think-globally-act-locally-a-deep-neural-network-approach-to-high-dimensional-time-series-.html}
}

@inproceedings{sesia_conformal_2021,
  title = {Conformal {{Prediction}} Using {{Conditional Histograms}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sesia, Matteo and Romano, Yaniv},
  year = {2021},
  volume = {34},
  pages = {6304--6315},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-07},
  abstract = {This paper develops a conformal method to compute prediction intervals for non-parametric regression that can automatically adapt to skewed data. Leveraging black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, it translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the black-box model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives, including conformalized quantile regression and other distributional conformal prediction approaches.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6NLFWNQH\Sesia and Romano - 2021 - Conformal Prediction using Conditional Histograms.pdf}
}

@article{sezer_algorithmic_2018,
  title = {Algorithmic Financial Trading with Deep Convolutional Neural Networks: {{Time}} Series to Image Conversion Approach},
  shorttitle = {Algorithmic Financial Trading with Deep Convolutional Neural Networks},
  author = {Sezer, Omer Berat and Ozbayoglu, Ahmet Murat},
  year = {2018},
  month = sep,
  journal = {Applied Soft Computing},
  volume = {70},
  pages = {525--538},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2018.04.024},
  urldate = {2021-01-21},
  abstract = {Computational intelligence techniques for financial trading systems have always been quite popular. In the last decade, deep learning models start getting more attention, especially within the image processing community. In this study, we propose a novel algorithmic trading model CNN-TA using a 2-D convolutional neural network based on image processing properties. In order to convert financial time series into 2-D images, 15 different technical indicators each with different parameter selections are utilized. Each indicator instance generates data for a 15 day period. As a result, 15\,\texttimes\,15 sized 2-D images are constructed. Each image is then labeled as Buy, Sell or Hold depending on the hills and valleys of the original time series. The results indicate that when compared with the Buy \& Hold Strategy and other common trading systems over a long out-of-sample period, the trained model provides better results for stocks and ETFs.},
  langid = {english},
  keywords = {Algorithmic trading,Convolutional neural networks,Deep learning,Financial forecasting,Stock market,Technical analysis},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WCDEDQ4V\Sezer and Ozbayoglu - 2018 - Algorithmic financial trading with deep convolutio.pdf}
}

@article{sezer_financial_2019,
  title = {Financial {{Trading Model}} with {{Stock Bar Chart Image Time Series}} with {{Deep Convolutional Neural Networks}}},
  author = {Sezer, Omer Berat and Ozbayoglu, Ahmet Murat},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.04610 [cs, stat]},
  eprint = {1903.04610},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {Even though computational intelligence techniques have been extensively utilized in financial trading systems, almost all developed models use the time series data for price prediction or identifying buy-sell points. However, in this study we decided to use 2-D stock bar chart images directly without introducing any additional time series associated with the underlying stock. We propose a novel algorithmic trading model CNN-BI (Convolutional Neural Network with Bar Images) using a 2-D Convolutional Neural Network. We generated 2-D images of sliding windows of 30-day bar charts for Dow 30 stocks and trained a deep Convolutional Neural Network (CNN) model for our algorithmic trading model. We tested our model separately between 2007-2012 and 2012-2017 for representing different market conditions. The results indicate that the model was able to outperform Buy and Hold strategy, especially in trendless or bear markets. Since this is a preliminary study and probably one of the first attempts using such an unconventional approach, there is always potential for improvement. Overall, the results are promising and the model might be integrated as part of an ensemble trading model combined with different strategies.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FTNEG87T\\Sezer and Ozbayoglu - 2019 - Financial Trading Model with Stock Bar Chart Image.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UX9XAYDZ\\1903.html}
}

@article{sezer_financial_2019a,
  title = {Financial {{Time Series Forecasting}} with {{Deep Learning}} : {{A Systematic Literature Review}}: 2005-2019},
  shorttitle = {Financial {{Time Series Forecasting}} with {{Deep Learning}}},
  author = {Sezer, Omer Berat and Gudelek, Mehmet Ugur and Ozbayoglu, Ahmet Murat},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.13288 [cs, q-fin, stat]},
  eprint = {1911.13288},
  primaryclass = {cs, q-fin, stat},
  urldate = {2020-02-17},
  abstract = {Financial time series forecasting is, without a doubt, the top choice of computational intelligence for finance researchers from both academia and financial industry due to its broad implementation areas and substantial impact. Machine Learning (ML) researchers came up with various models and a vast number of studies have been published accordingly. As such, a significant amount of surveys exist covering ML for financial time series forecasting studies. Lately, Deep Learning (DL) models started appearing within the field, with results that significantly outperform traditional ML counterparts. Even though there is a growing interest in developing models for financial time series forecasting research, there is a lack of review papers that were solely focused on DL for finance. Hence, our motivation in this paper is to provide a comprehensive literature review on DL studies for financial time series forecasting implementations. We not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, commodity forecasting, but also grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Long-Short Term Memory (LSTM). We also tried to envision the future for the field by highlighting the possible setbacks and opportunities, so the interested researchers can benefit.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,I.1.2,Quantitative Finance - Computational Finance,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3NN4EQJL\\Sezer et al. - 2019 - Financial Time Series Forecasting with Deep Learni.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\QFM2NUTS\\1911.html}
}

@book{shalev-shwartz_understanding_2014,
  title = {Understanding {{Machine Learning}}: {{From Theory}} to {{Algorithms}}},
  shorttitle = {Understanding {{Machine Learning}}},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781107298019},
  urldate = {2019-09-12},
  isbn = {978-1-107-29801-9},
  langid = {english},
  annotation = {01477},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FCF6DRM3\Shalev-Shwartz and Ben-David - 2014 - Understanding Machine Learning From Theory to Alg.pdf}
}

@inproceedings{shanbhag_efficient_2018,
  title = {Efficient {{Top-K Query Processing}} on {{Massively Parallel Hardware}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Shanbhag, Anil and Pirk, Holger and Madden, Samuel},
  year = {2018},
  month = may,
  series = {{{SIGMOD}} '18},
  pages = {1557--1570},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3183713.3183735},
  urldate = {2022-01-10},
  abstract = {A common operation in many data analytics workloads is to find the top-k items, i.e., the largest or smallest operations according to some sort order (implemented via LIMIT or ORDER BY expressions in SQL). A naive implementation of top-k is to sort all of the items and then return the first k, but this does much more work than needed. Although efficient implementations for top-k have been explored on traditional multi-core processors, there has been no prior systematic study of top-k implementations on GPUs, despite open requests for such implementations in GPU-based frameworks like TensorFlow and ArrayFire. In this work, we present several top-k algorithms for GPUs, including a new algorithm based on bitonic sort called bitonic top-k. The bitonic top-k algorithm is up to a factor of \textbackslash new15x faster than sort and 4x faster than a variety of other possible implementations for values of k up to 256. We also develop a cost model to predict the performance of several of our algorithms, and show that it accurately predicts actual performance on modern GPUs.},
  isbn = {978-1-4503-4703-7},
  keywords = {bitonic top-k,top-k algorithms for gpu}
}

@article{shcherbakov_survey_2013,
  title = {A {{Survey}} of {{Forecast Error Measures}}},
  author = {Shcherbakov, Maxim Vladimirovich and Brebels, Adriaan and Shcherbakova, Nataliya Lvovna and Tyukov, Anton Pavlovich and Janovsky, Timur Alexandrovich and Kamaev, Valeriy Anatol'evich},
  year = {2013},
  pages = {6},
  abstract = {This article reviews the common used forecast error measurements. All error measurements have been joined in the seven groups: absolute forecasting errors, measures based on percentage errors, symmetric errors, measures based on relative errors, scaled errors, relative measures and other error measures. The formulas are presented and drawbacks are discussed for every accuracy measurements. To reduce the impact of outliers, an Integral Normalized Mean Square Error have been proposed. Due to the fact that each error measure has the disadvantages that can lead to inaccurate evaluation of the forecasting results, it is impossible to choose only one measure, the recommendations for selecting the appropriate error measurements are given.},
  langid = {english},
  annotation = {00126},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZU8HUL4S\Shcherbakov et al. - 2013 - A Survey of Forecast Error Measures.pdf}
}

@inproceedings{shen_endtoend_2018,
  title = {End-to-{{End Time Series Imputation}} via {{Residual Short Paths}}},
  booktitle = {Asian {{Conference}} on {{Machine Learning}}},
  author = {Shen, Lifeng and Ma, Qianli and Li, Sen},
  year = {2018},
  month = nov,
  pages = {248--263},
  urldate = {2019-04-17},
  abstract = {Time series imputation (replacing  missing data) plays an important role in time series analysis due to missing values in real world data. How to recover missing values and model the underlying dyn...},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8TPAD9TM\\Shen et al. - 2018 - End-to-End Time Series Imputation via Residual Sho.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\X4JHXYIQ\\shen18a.html}
}

@article{shi_compositional_2020,
  title = {Compositional {{Embeddings Using Complementary Partitions}} for {{Memory-Efficient Recommendation Systems}}},
  author = {Shi, Hao-Jun Michael and Mudigere, Dheevatsa and Naumov, Maxim and Yang, Jiyan},
  year = {2020},
  month = aug,
  journal = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  eprint = {1909.02107},
  pages = {165--175},
  doi = {10.1145/3394486.3403059},
  urldate = {2021-12-16},
  abstract = {Modern deep learning-based recommendation systems exploit hundreds to thousands of different categorical features, each with millions of different categories ranging from clicks to posts. To respect the natural diversity within the categorical data, embeddings map each category to a unique dense representation within an embedded space. Since each categorical feature could take on as many as tens of millions of different possible categories, the embedding tables form the primary memory bottleneck during both training and inference. We propose a novel approach for reducing the embedding size in an end-to-end fashion by exploiting complementary partitions of the category set to produce a unique embedding vector for each category without explicit definition. By storing multiple smaller embedding tables based on each complementary partition and combining embeddings from each table, we define a unique embedding for each category at smaller memory cost. This approach may be interpreted as using a specific fixed codebook to ensure uniqueness of each category's representation. Our experimental results demonstrate the effectiveness of our approach over the hashing trick for reducing the size of the embedding tables in terms of model loss and accuracy, while retaining a similar reduction in the number of parameters.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QM4ETS7S\Shi et al. - 2020 - Compositional Embeddings Using Complementary Parti.pdf}
}

@inproceedings{shi_compositional_2020a,
  title = {Compositional {{Embeddings Using Complementary Partitions}} for {{Memory-Efficient Recommendation Systems}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Shi, Hao-Jun Michael and Mudigere, Dheevatsa and Naumov, Maxim and Yang, Jiyan},
  year = {2020},
  month = aug,
  eprint = {1909.02107},
  primaryclass = {cs, stat},
  pages = {165--175},
  doi = {10.1145/3394486.3403059},
  urldate = {2023-05-05},
  abstract = {Modern deep learning-based recommendation systems exploit hundreds to thousands of different categorical features, each with millions of different categories ranging from clicks to posts. To respect the natural diversity within the categorical data, embeddings map each category to a unique dense representation within an embedded space. Since each categorical feature could take on as many as tens of millions of different possible categories, the embedding tables form the primary memory bottleneck during both training and inference. We propose a novel approach for reducing the embedding size in an end-to-end fashion by exploiting complementary partitions of the category set to produce a unique embedding vector for each category without explicit definition. By storing multiple smaller embedding tables based on each complementary partition and combining embeddings from each table, we define a unique embedding for each category at smaller memory cost. This approach may be interpreted as using a specific fixed codebook to ensure uniqueness of each category's representation. Our experimental results demonstrate the effectiveness of our approach over the hashing trick for reducing the size of the embedding tables in terms of model loss and accuracy, while retaining a similar reduction in the number of parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\J3UKLX2Z\\Shi et al. - 2020 - Compositional Embeddings Using Complementary Parti.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZSTJE49T\\1909.html}
}

@inproceedings{shi_user_2020,
  title = {Beyond {{User Embedding Matrix}}: {{Learning}} to {{Hash}} for {{Modeling Large-Scale Users}} in {{Recommendation}}},
  shorttitle = {Beyond {{User Embedding Matrix}}},
  booktitle = {Proceedings of the 43rd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Shi, Shaoyun and Ma, Weizhi and Zhang, Min and Zhang, Yongfeng and Yu, Xinxing and Shan, Houzhi and Liu, Yiqun and Ma, Shaoping},
  year = {2020},
  month = jul,
  pages = {319--328},
  publisher = {{ACM}},
  address = {{Virtual Event China}},
  doi = {10.1145/3397271.3401119},
  urldate = {2021-12-15},
  abstract = {Modeling large scale and rare-interaction users are the two major challenges in recommender systems, which derives big gaps between researches and applications. Facing to millions or even billions of users, it is hard to store and leverage personalized preferences with a user embedding matrix in real scenarios. And many researches pay attention to users with rich histories, while users with only one or several interactions are the biggest part in real systems. Previous studies make efforts to handle one of the above issues but rarely tackle efficiency and cold-start problems together. In this work, a novel user preference representation called Preference Hash (PreHash) is proposed to model large scale users, including rare-interaction ones. In PreHash, a series of buckets are generated based on users' historical interactions. Users with similar preferences are assigned into the same buckets automatically, including warm and cold ones. Representations of the buckets are learned accordingly. Contributing to the designed hash buckets, only limited parameters are stored, which saves a lot of memory for more efficient modeling. Furthermore, when new interactions are made by a user, his buckets and representations will be dynamically updated, which enables more effective understanding and modeling of the user. It is worth mentioning that PreHash is flexible to work with various recommendation algorithms by taking the place of previous user embedding matrices. We combine it with multiple state-of-the-art recommendation methods and conduct various experiments. Comparative results on public datasets show that it not only improves the recommendation performance but also significantly reduces the number of model parameters. To summarize, PreHash has achieved significant improvements in both efficiency and effectiveness for recommender systems.},
  isbn = {978-1-4503-8016-4},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\WCYCXMXK\Shi et al. - 2020 - Beyond User Embedding Matrix Learning to Hash for.pdf}
}

@article{shi_virtualtaobao_2018,
  title = {Virtual-{{Taobao}}: {{Virtualizing Real-world Online Retail Environment}} for {{Reinforcement Learning}}},
  shorttitle = {Virtual-{{Taobao}}},
  author = {Shi, Jing-Cheng and Yu, Yang and Da, Qing and Chen, Shi-Yong and Zeng, An-Xiang},
  year = {2018},
  month = may,
  journal = {arXiv:1805.10000 [cs]},
  eprint = {1805.10000},
  primaryclass = {cs},
  urldate = {2020-05-25},
  abstract = {Applying reinforcement learning in physical-world tasks is extremely challenging. It is commonly infeasible to sample a large number of trials, as required by current reinforcement learning methods, in a physical environment. This paper reports our project on using reinforcement learning for better commodity search in Taobao, one of the largest online retail platforms and meanwhile a physical environment with a high sampling cost. Instead of training reinforcement learning in Taobao directly, we present our approach: first we build Virtual Taobao, a simulator learned from historical customer behavior data through the proposed GAN-SD (GAN for Simulating Distributions) and MAIL (multi-agent adversarial imitation learning), and then we train policies in Virtual Taobao with no physical costs in which ANC (Action Norm Constraint) strategy is proposed to reduce over-fitting. In experiments, Virtual Taobao is trained from hundreds of millions of customers' records, and its properties are compared with the real environment. The results disclose that Virtual Taobao faithfully recovers important properties of the real environment. We also show that the policies trained in Virtual Taobao can have significantly superior online performance to the traditional supervised approaches. We hope our work could shed some light on reinforcement learning applications in complex physical environments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FUKQGDHP\\Shi et al. - 2018 - Virtual-Taobao Virtualizing Real-world Online Ret.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Y3BB4XIN\\1805.html}
}

@article{shih_temporal_2018,
  title = {Temporal {{Pattern Attention}} for {{Multivariate Time Series Forecasting}}},
  author = {Shih, Shun-Yao and Sun, Fan-Keng and Lee, Hung-yi},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.04206 [cs, stat]},
  eprint = {1809.04206},
  primaryclass = {cs, stat},
  urldate = {2019-04-03},
  abstract = {Forecasting of multivariate time series data, for instance the prediction of electricity consumption, solar power production, and polyphonic piano pieces, has numerous valuable applications. However, complex and non-linear interdependencies between time steps and series complicate this task. To obtain accurate prediction, it is crucial to model long-term dependency in time series data, which can be achieved by recurrent neural networks (RNNs) with an attention mechanism. The typical attention mechanism reviews the information at each previous time step and selects relevant information to help generate the outputs; however, it fails to capture temporal patterns across multiple time steps. In this paper, we propose using a set of filters to extract time-invariant temporal patterns, similar to transforming time series data into its ``frequency domain''. Then we propose a novel attention mechanism to select relevant time series, and use its frequency domain information for multivariate forecasting. We apply the proposed model on several real-world tasks and achieve state-of-the-art performance in all of these with a single exception. Our source code is available at https://github.com/gantheory/TPA-LSTM.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\KQ2AZ5UR\Shih et al. - 2018 - Temporal Pattern Attention for Multivariate Time S.pdf}
}

@article{shiratori_prediction_2020,
  title = {Prediction of Hierarchical Time Series Using Structured Regularization and Its Application to Artificial Neural Networks},
  author = {Shiratori, Tomokaze and Kobayashi, Ken and Takano, Yuichi},
  year = {2020},
  month = nov,
  journal = {PLOS ONE},
  volume = {15},
  number = {11},
  eprint = {2007.15159},
  primaryclass = {cs, stat},
  pages = {e0242099},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0242099},
  urldate = {2022-05-25},
  abstract = {This paper discusses the prediction of hierarchical time series, where each upper-level time series is calculated by summing appropriate lower-level time series. Forecasts for such hierarchical time series should be coherent, meaning that the forecast for an upper-level time series equals the sum of forecasts for corresponding lower-level time series. Previous methods for making coherent forecasts consist of two phases: first computing base (incoherent) forecasts and then reconciling those forecasts based on their inherent hierarchical structure. With the aim of improving time series predictions, we propose a structured regularization method for completing both phases simultaneously. The proposed method is based on a prediction model for bottom-level time series and uses a structured regularization term to incorporate upper-level forecasts into the prediction model. We also develop a backpropagation algorithm specialized for application of our method to artificial neural networks for time series prediction. Experimental results using synthetic and real-world datasets demonstrate the superiority of our method in terms of prediction accuracy and computational efficiency.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8BAMQH3C\\Shiratori et al. - 2020 - Prediction of hierarchical time series using struc.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\VDSIMJB9\\2007.html}
}

@inproceedings{shokoohi-yekta_discovery_2015,
  title = {Discovery of {{Meaningful Rules}} in {{Time Series}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '15},
  author = {{Shokoohi-Yekta}, Mohammad and Chen, Yanping and Campana, Bilson and Hu, Bing and Zakaria, Jesin and Keogh, Eamonn},
  year = {2015},
  pages = {1085--1094},
  publisher = {{ACM Press}},
  address = {{Sydney, NSW, Australia}},
  doi = {10.1145/2783258.2783306},
  urldate = {2019-04-17},
  abstract = {The ability to make predictions about future events is at the heart of much of science; so, it is not surprising that prediction has been a topic of great interest in the data mining community for the last decade. Most of the previous work has attempted to predict the future based on the current value of a stream. However, for many problems the actual values are irrelevant, whereas the shape of the current time series pattern may foretell the future. The handful of research efforts that consider this variant of the problem have met with limited success. In particular, it is now understood that most of these efforts allow the discovery of spurious rules. We believe the reason why rule discovery in real-valued time series has failed thus far is because most efforts have more or less indiscriminately applied the ideas of symbolic stream rule discovery to real-valued rule discovery. In this work, we show why these ideas are not directly suitable for rule discovery in time series. Beyond our novel definitions/representations, which allow for meaningful and extendable specifications of rules, we further show novel algorithms that allow us to quickly discover high quality rules in very large datasets that accurately predict the occurrence of future events.},
  isbn = {978-1-4503-3664-2},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BEAG3D7A\Shokoohi-Yekta et al. - 2015 - Discovery of Meaningful Rules in Time Series.pdf}
}

@article{siddiqui_tsviz_2019,
  title = {{{TSViz}}: {{Demystification}} of {{Deep Learning Models}} for {{Time-Series Analysis}}},
  shorttitle = {{{TSViz}}},
  author = {Siddiqui, Shoaib Ahmed and Mercier, Dominik and Munir, Mohsin and Dengel, Andreas and Ahmed, Sheraz},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  eprint = {1802.02952},
  pages = {67027--67040},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2912823},
  urldate = {2019-10-06},
  abstract = {This paper presents a novel framework for demystification of convolutional deep learning models for time-series analysis. This is a step towards making informed/explainable decisions in the domain of time-series, powered by deep learning. There have been numerous efforts to increase the interpretability of image-centric deep neural network models, where the learned features are more intuitive to visualize. Visualization in time-series domain is much more complicated as there is no direct interpretation of the filters and inputs as compared to the image modality. In addition, little or no concentration has been devoted for the development of such tools in the domain of time-series in the past. TSViz provides possibilities to explore and analyze a network from different dimensions at different levels of abstraction which includes identification of parts of the input that were responsible for a prediction (including per filter saliency), importance of different filters present in the network for a particular prediction, notion of diversity present in the network through filter clustering, understanding of the main sources of variation learnt by the network through inverse optimization, and analysis of the network's robustness against adversarial noise. As a sanity check for the computed influence values, we demonstrate results regarding pruning of neural networks based on the computed influence information. These representations allow to understand the network features so that the acceptability of deep networks for time-series data can be enhanced. This is extremely important in domains like finance, industry 4.0, self-driving cars, health-care, counter-terrorism etc., where reasons for reaching a particular prediction are equally important as the prediction itself. We assess the proposed framework for interpretability with a set of desirable properties essential for any method in this direction.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\V2KV6VHJ\Siddiqui et al. - 2019 - TSViz Demystification of Deep Learning Models for.pdf}
}

@inproceedings{siffer_anomaly_2017,
  title = {Anomaly {{Detection}} in {{Streams}} with {{Extreme Value Theory}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Siffer, Alban and Fouque, Pierre-Alain and Termier, Alexandre and Largouet, Christine},
  year = {2017},
  series = {{{KDD}} '17},
  pages = {1067--1075},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3097983.3098144},
  urldate = {2019-04-16},
  abstract = {Anomaly detection in time series has attracted considerable attention due to its importance in many real-world applications including intrusion detection, energy management and finance. Most approaches for detecting outliers rely on either manually set thresholds or assumptions on the distribution of data according to Chandola, Banerjee and Kumar. Here, we propose a new approach to detect outliers in streaming univariate time series based on Extreme Value Theory that does not require to hand-set thresholds and makes no assumption on the distribution: the main parameter is only the risk, controlling the number of false positives. Our approach can be used for outlier detection, but more generally for automatically setting thresholds, making it useful in wide number of situations. We also experiment our algorithms on various real-world datasets which confirm its soundness and efficiency.},
  isbn = {978-1-4503-4887-4},
  keywords = {extreme value theory,outliers in time series,read - in related work,streaming},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3BV7DP58\Siffer et al. - 2017 - Anomaly Detection in Streams with Extreme Value Th.pdf}
}

@article{sigrist_gradient_2020,
  title = {Gradient and {{Newton}} Boosting for Classification and Regression},
  author = {Sigrist, Fabio},
  year = {2020},
  month = oct,
  journal = {Expert Systems with Applications},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2020.114080},
  urldate = {2020-12-07},
  abstract = {Boosting algorithms are frequently used in applied data science and in research. To date, the distinction between boosting with either gradient descent or second-order Newton updates is often not made in both applied and methodological research, and it is thus implicitly assumed that the difference is irrelevant. The goal of this article is to clarify this situation. In particular, we present gradient and Newton boosting, as well as a hybrid variant of the two, in a unified framework. We compare these boosting algorithms with trees as base learners using various datasets and loss functions. Our experiments show that Newton boosting outperforms gradient and hybrid gradient-Newton boosting in terms of predictive accuracy on the majority of datasets. We also present evidence that the reason for this is not faster convergence of Newton boosting. In addition, we introduce a novel tuning parameter for tree-based Newton boosting which is interpretable and important for predictive accuracy.},
  langid = {english},
  keywords = {Boosting,Ensembles,Supervised learning,Trees},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HX69HLCT\\Sigrist - 2020 - Gradient and Newton boosting for classification an.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6NP4TXGB\\S0957417420308381.html}
}

@article{sigtermans_framework_2020,
  title = {Towards a {{Framework}} for {{Observational Causality From Time Series}}: {{When Shannon Meets Turing}}},
  shorttitle = {Towards a {{Framework}} for {{Observational Causality From Time Series}}},
  author = {Sigtermans, David},
  year = {2020},
  month = feb,
  journal = {arXiv:1909.13559 [cs, math]},
  eprint = {1909.13559},
  primaryclass = {cs, math},
  urldate = {2020-02-17},
  abstract = {We propose a novel tensor-based formalism for inferring causal structures from time series. An information theoretical analysis of transfer entropy, shows that transfer entropy results from transmission of information over a set of communication channels. Tensors are the mathematical equivalents of these multi-channel causal channels. A multi-channel causal channel is a generalization of a discrete memoryless channel. Investigation of a system comprising three variables shows that in our formalism, bivariate analysis suffices to differentiate between direct and indirect relations. For this to be true, we have to combine the output of multi-channel causal channels with the output of single-channel causal channels. We can understand this result when we consider the role of noise. Subsequent transmission of information over noisy channels can never result in less noisy transmission overall. This implies that a Data Processing Inequality exists for transfer entropy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Theory},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9RF8WZGT\\Sigtermans - 2020 - Towards a Framework for Observational Causality Fr.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7K8QY4FB\\1909.html}
}

@article{silva_generalized_2020,
  title = {Generalized {{Autoregressive Neural Network Models}}},
  author = {Silva, Renato Rodrigues},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.05676 [stat]},
  eprint = {2002.05676},
  primaryclass = {stat},
  urldate = {2020-02-17},
  abstract = {A time series is a sequence of observations taken sequentially in time [4, 15, 26]. The autoregressive integrated moving average is a class of the model more used for time series data.However, this class of model has two critical limitations. It fits well only Gaussian data with the linear structure of correlation. Here, I present a new model named as generalized autoregressive neural networks, GARNN. The GARNN is an extension of the generalized linear model where the mean marginal depends on the lagged values via the inclusion of the neural network in the link function. A practical application of the model is shown using a well-known poliomyelitis case number, originally analyzed by [29].},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MAU8KFWY\Silva - 2020 - Generalized Autoregressive Neural Network Models.pdf}
}

@misc{simhadri_results_2022,
  title = {Results of the {{NeurIPS}}'21 {{Challenge}} on {{Billion-Scale Approximate Nearest Neighbor Search}}},
  author = {Simhadri, Harsha Vardhan and Williams, George and Aum{\"u}ller, Martin and Douze, Matthijs and Babenko, Artem and Baranchuk, Dmitry and Chen, Qi and Hosseini, Lucas and Krishnaswamy, Ravishankar and Srinivasa, Gopal and Subramanya, Suhas Jayaram and Wang, Jingdong},
  year = {2022},
  month = may,
  number = {arXiv:2205.03763},
  eprint = {2205.03763},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-24},
  abstract = {Despite the broad range of algorithms for Approximate Nearest Neighbor Search, most empirical evaluations of algorithms have focused on smaller datasets, typically of 1 million points\textasciitilde\textbackslash citep\{Benchmark\}. However, deploying recent advances in embedding based techniques for search, recommendation and ranking at scale require ANNS indices at billion, trillion or larger scale. Barring a few recent papers, there is limited consensus on which algorithms are effective at this scale vis-\textbackslash `a-vis their hardware cost. This competition compares ANNS algorithms at billion-scale by hardware cost, accuracy and performance. We set up an open source evaluation framework and leaderboards for both standardized and specialized hardware. The competition involves three tracks. The standard hardware track T1 evaluates algorithms on an Azure VM with limited DRAM, often the bottleneck in serving billion-scale indices, where the embedding data can be hundreds of GigaBytes in size. It uses FAISS\textasciitilde\textbackslash citep\{Faiss17\} as the baseline. The standard hardware track T2 additional allows inexpensive SSDs in addition to the limited DRAM and uses DiskANN\textasciitilde\textbackslash citep\{DiskANN19\} as the baseline. The specialized hardware track T3 allows any hardware configuration, and again uses FAISS as the baseline. We compiled six diverse billion-scale datasets, four newly released for this competition, that span a variety of modalities, data types, dimensions, deep learning models, distance functions and sources. The outcome of the competition was ranked leaderboards of algorithms in each track based on recall at a query throughput threshold. Additionally, for track T3, separate leaderboards were created based on recall as well as cost-normalized and power-normalized query throughput.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Databases,Computer Science - Machine Learning,Computer Science - Performance},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8LFRT6Q3\\Simhadri et al. - 2022 - Results of the NeurIPS'21 Challenge on Billion-Sca.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\M3DWVWHI\\2205.html}
}

@article{simonyan_very_2014,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2014},
  month = sep,
  journal = {arXiv:1409.1556 [cs]},
  eprint = {1409.1556},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 \texttimes{} 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16\textendash 19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MPATP35I\Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf}
}

@incollection{sindhwani_block_2010,
  title = {Block {{Variable Selection}} in {{Multivariate Regression}} and {{High-dimensional Causal Inference}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 23},
  author = {Sindhwani, Vikas and Lozano, Aurelie C},
  editor = {Lafferty, J. D. and Williams, C. K. I. and {Shawe-Taylor}, J. and Zemel, R. S. and Culotta, A.},
  year = {2010},
  pages = {1486--1494},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\2G5IQ5VP\\Sindhwani and Lozano - 2010 - Block Variable Selection in Multivariate Regressio.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\L5JELZW5\\3993-block-variable-selection-in-multivariate-regression-and-high-dimensional-causal-inference.html}
}

@article{smeekes_highdimensional_2019,
  title = {High-{{Dimensional Forecasting}} in the {{Presence}} of {{Unit Roots}} and {{Cointegration}}},
  author = {Smeekes, Stephan and Wijler, Etienne},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.10552 [econ]},
  eprint = {1911.10552},
  primaryclass = {econ},
  urldate = {2020-02-17},
  abstract = {We investigate how the possible presence of unit roots and cointegration affects forecasting with Big Data. As most macroeoconomic time series are very persistent and may contain unit roots, a proper handling of unit roots and cointegration is of paramount importance for macroeconomic forecasting. The high-dimensional nature of Big Data complicates the analysis of unit roots and cointegration in two ways. First, transformations to stationarity require performing many unit root tests, increasing room for errors in the classification. Second, modelling unit roots and cointegration directly is more difficult, as standard high-dimensional techniques such as factor models and penalized regression are not directly applicable to (co)integrated data and need to be adapted. We provide an overview of both issues and review methods proposed to address these issues. These methods are also illustrated with two empirical applications.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\F6KA3X9I\\Smeekes and Wijler - 2019 - High-Dimensional Forecasting in the Presence of Un.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EPA5P7RR\\1911.html}
}

@article{snyder_forecasting_2012,
  title = {Forecasting the Intermittent Demand for Slow-Moving Inventories: {{A}} Modelling Approach},
  shorttitle = {Forecasting the Intermittent Demand for Slow-Moving Inventories},
  author = {Snyder, Ralph D. and Ord, J. Keith and Beaumont, Adrian},
  year = {2012},
  month = apr,
  journal = {International Journal of Forecasting},
  volume = {28},
  number = {2},
  pages = {485--496},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2011.03.009},
  urldate = {2019-06-03},
  abstract = {Organizations with large-scale inventory systems typically have a large proportion of items for which demand is intermittent and low volume. We examine various different approaches to demand forecasting for such products, paying particular attention to the need for inventory planning over a multi-period lead-time when the underlying process may be non-stationary. This emphasis leads to the consideration of prediction distributions for processes with time-dependent parameters. A wide range of possible distributions could be considered, but we focus upon the Poisson (as a widely used benchmark), the negative binomial (as a popular extension of the Poisson), and a hurdle shifted Poisson (which retains Croston's notion of a Bernoulli process for the occurrence of active demand periods). We also develop performance measures which are related to the entire prediction distribution, rather than focusing exclusively upon point predictions. The three models are compared using data on the monthly demand for 1046 automobile parts, provided by a US automobile manufacturer. We conclude that inventory planning should be based upon dynamic models using distributions that are more flexible than the traditional Poisson scheme.},
  keywords = {Croston's method,Exponential smoothing,Hurdle shifted Poisson distribution,Intermittent demand,Inventory control,Prediction likelihood,State space models},
  annotation = {00062},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9SMA2ZKS\\Snyder et al. - 2012 - Forecasting the intermittent demand for slow-movin.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\QAIIRIRK\\S0169207011000781.html}
}

@article{so_evolved_2019,
  title = {The {{Evolved Transformer}}},
  author = {So, David R. and Liang, Chen and Le, Quoc V.},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.11117 [cs, stat]},
  eprint = {1901.11117},
  primaryclass = {cs, stat},
  urldate = {2019-09-04},
  abstract = {Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 English-German translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments -- the Evolved Transformer -- demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT'14 English-German; at smaller sizes, it achieves the same quality as the original "big" Transformer with 37.6\% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {00011},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\V4XGF7N5\\So et al. - 2019 - The Evolved Transformer.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Q2RVFQPE\\1901.html}
}

@misc{sousa_improved_2022,
  title = {Improved Conformalized Quantile Regression},
  author = {Sousa, Martim and Tom{\'e}, Ana Maria and Moreira, Jos{\'e}},
  year = {2022},
  month = nov,
  number = {arXiv:2207.02808},
  eprint = {2207.02808},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.02808},
  urldate = {2023-03-07},
  abstract = {Conformalized quantile regression is a procedure that inherits the advantages of conformal prediction and quantile regression. That is, we use quantile regression to estimate the true conditional quantile and then apply a conformal step on a calibration set to ensure marginal coverage. In this way, we get adaptive prediction intervals that account for heteroscedasticity. However, the aforementioned conformal step lacks adaptiveness as described in (Romano et al., 2019). To overcome this limitation, instead of applying a single conformal step after estimating conditional quantiles with quantile regression, we propose to cluster the explanatory variables weighted by their permutation importance with an optimized k-means and apply k conformal steps. To show that this improved version outperforms the classic version of conformalized quantile regression and is more adaptive to heteroscedasticity, we extensively compare the prediction intervals of both in open datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5XG9EEPN\\Sousa et al. - 2022 - Improved conformalized quantile regression.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YZX3VNXF\\2207.html}
}

@article{spiegel_model_2017,
  title = {Model Selection in Semiparametric Expectile Regression},
  author = {Spiegel, Elmar and Sobotka, Fabian and Kneib, Thomas},
  year = {2017},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {11},
  number = {2},
  pages = {3008--3038},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1935-7524, 1935-7524},
  doi = {10.1214/17-EJS1307},
  urldate = {2021-10-13},
  abstract = {Ordinary least squares regression focuses on the expected response and strongly depends on the assumption of normally distributed errors for inferences. An approach to overcome these restrictions is expectile regression, where no distributional assumption is made but rather the whole distribution of the response is described in terms of covariates. This is similar to quantile regression, but expectiles provide a convenient generalization of the arithmetic mean while quantiles are a generalization of the median. To analyze more complex data structures where purely linear predictors are no longer sufficient, semiparametric regression methods have been introduced for both ordinary least squares and expectile regression. However, with increasing complexity of the data and the regression structure, the selection of the true covariates and their effects becomes even more important than in standard regression models. Therefore we introduce several approaches depending on selection criteria and shrinkage methods to perform model selection in semiparametric expectile regression. Moreover, we propose a joint approach for model selection based on several asymmetries simultaneously to deal with the special feature that expectile regression estimates the complete distribution of the response. Furthermore, to distinguish between linear and smooth predictors, we split nonlinear effects into the purely linear trend and the deviation from this trend. All selection methods are compared with the benchmark of functional gradient descent boosting in a simulation study and applied to determine the relevant covariates when studying childhood malnutrition in Peru.},
  keywords = {boosting,expectiles,least asymmetrically weighted squares,Model selection,non-negative garrote,semiparametric regression},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\8I7FITZ3\Spiegel et al. - 2017 - Model selection in semiparametric expectile regres.pdf}
}

@article{spiliotis_generalizing_2020,
  title = {Generalizing the {{Theta}} Method for Automatic Forecasting},
  author = {Spiliotis, Evangelos and Assimakopoulos, Vassilios and Makridakis, Spyros},
  year = {2020},
  month = jul,
  journal = {European Journal of Operational Research},
  volume = {284},
  number = {2},
  pages = {550--558},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2020.01.007},
  urldate = {2021-08-30},
  abstract = {The Theta method became popular due to its superior performance in the M3 forecasting competition. Since then, although it has been shown that Theta provides accurate forecasts for various types of data, being a solid benchmark to beat, limited research has been conducted to exploit its full potential and generalize its reach. This paper examines three extensions on Theta's framework to boost its performance. This includes (i) considering both linear and non-linear trends, (ii) allowing to adjust the slope of such trends, and (iii) introducing a multiplicative expression of the underlying forecasting model along with the existing, additive one. The proposed modifications transform Theta into a generalized forecasting algorithm, suitable for automatic time series predictions. The proposed algorithm is evaluated using the series of the M, M3, and M4 competitions. Such an evaluation shows that the proposed approach produces more accurate forecasts than the original, classic Theta, both in terms of point forecasts and prediction intervals, and is also more accurate than other well-known methods for yearly series.},
  langid = {english},
  keywords = {Automatic model selection,Forecasting,M competitions,Theta method,Time series},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TQ9UTN9K\\Spiliotis et al. - 2020 - Generalizing the Theta method for automatic foreca.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\84BXG56C\\S0377221720300242.html}
}

@article{spiliotis_hierarchical_2021a,
  title = {Hierarchical Forecast Reconciliation with Machine Learning},
  author = {Spiliotis, Evangelos and Abolghasemi, Mahdi and Hyndman, Rob J. and Petropoulos, Fotios and Assimakopoulos, Vassilios},
  year = {2021},
  month = nov,
  journal = {Applied Soft Computing},
  volume = {112},
  pages = {107756},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2021.107756},
  urldate = {2022-05-24},
  abstract = {Over the last 15 years, studies on hierarchical forecasting have moved away from single-level approaches towards proposing linear combination approaches across multiple levels of the hierarchy. Such combinations offer coherent reconciled forecasts, improved forecasting performance and aligned decision-making. This paper proposes a novel hierarchical forecasting approach based on machine learning. The proposed method allows for non-linear combinations of the base forecasts, thus being more general than linear approaches. We structurally combine the objectives of improved post-sample empirical forecasting accuracy and coherence. Due to its non-linear nature, our approach selectively combines the base forecasts in a direct and automated way without requiring that the complete information must be used for producing reconciled forecasts for each series and level. The proposed method is evaluated both in terms of accuracy and bias using two different data sets coming from the tourism and retail industries. Our results suggest that the proposed method gives superior point forecasts than existing approaches, especially when the series comprising the hierarchy are not characterized by the same patterns.},
  langid = {english},
  keywords = {Forecasting,Hierarchies,Non-linear coherence,Time series},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FHNP92DY\\Spiliotis et al. - 2021 - Hierarchical forecast reconciliation with machine .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6PPCSXGF\\S1568494621006773.html}
}

@article{sprangers_parameter-efficient_2023,
  title = {Parameter-Efficient Deep Probabilistic Forecasting},
  author = {Sprangers, Olivier and Schelter, Sebastian and {de Rijke}, Maarten},
  year = {2023},
  month = jan,
  journal = {International Journal of Forecasting},
  volume = {39},
  number = {1},
  pages = {332--345},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.11.011},
  urldate = {2023-10-18},
  abstract = {Probabilistic time series forecasting is crucial in many application domains, such as retail, ecommerce, finance, and biology. With the increasing availability of large volumes of data, a number of neural architectures have been proposed for this problem. In particular, Transformer-based methods achieve state-of-the-art performance on real-world benchmarks. However, these methods require a large number of parameters to be learned, which imposes high memory requirements on the computational resources for training such models. To address this problem, we introduce a novel bidirectional temporal convolutional network that requires an order of magnitude fewer parameters than a common Transformer-based approach. Our model combines two temporal convolutional networks: the first network encodes future covariates of the time series, whereas the second network encodes past observations and covariates. We jointly estimate the parameters of an output distribution via these two networks. Experiments on four real-world datasets show that our method performs on par with four state-of-the-art probabilistic forecasting methods, including a Transformer-based approach and WaveNet, on two point metrics (sMAPE and NRMSE) as well as on a set of range metrics (quantile loss percentiles) in the majority of cases. We also demonstrate that our method requires significantly fewer parameters than Transformer-based methods, which means that the model can be trained faster with significantly lower memory requirements, which as a consequence reduces the infrastructure cost for deploying these models.},
  keywords = {Efficiency in forecasting methods,Large-scale forecasting Forecasting with neural networks,Probabilistic forecasting,Temporal convolutional network},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\2GH78LPI\\Sprangers et al. - 2023 - Parameter-efficient deep probabilistic forecasting.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WSIEDTPJ\\S0169207021001850.html}
}

@article{sprangers_parameterefficient_2022,
  title = {Parameter-Efficient Deep Probabilistic Forecasting},
  author = {Sprangers, Olivier and Schelter, Sebastian and {de Rijke}, Maarten},
  year = {2022},
  month = jan,
  journal = {International Journal of Forecasting},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2021.11.011},
  urldate = {2022-01-17},
  abstract = {Probabilistic time series forecasting is crucial in many application domains, such as retail, ecommerce, finance, and biology. With the increasing availability of large volumes of data, a number of neural architectures have been proposed for this problem. In particular, Transformer-based methods achieve state-of-the-art performance on real-world benchmarks. However, these methods require a large number of parameters to be learned, which imposes high memory requirements on the computational resources for training such models. To address this problem, we introduce a novel bidirectional temporal convolutional network that requires an order of magnitude fewer parameters than a common Transformer-based approach. Our model combines two temporal convolutional networks: the first network encodes future covariates of the time series, whereas the second network encodes past observations and covariates. We jointly estimate the parameters of an output distribution via these two networks. Experiments on four real-world datasets show that our method performs on par with four state-of-the-art probabilistic forecasting methods, including a Transformer-based approach and WaveNet, on two point metrics (sMAPE and NRMSE) as well as on a set of range metrics (quantile loss percentiles) in the majority of cases. We also demonstrate that our method requires significantly fewer parameters than Transformer-based methods, which means that the model can be trained faster with significantly lower memory requirements, which as a consequence reduces the infrastructure cost for deploying these models.},
  langid = {english},
  keywords = {Efficiency in forecasting methods,Large-scale forecasting Forecasting with neural networks,Probabilistic forecasting,Temporal convolutional network},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8AJ5FPH7\\Sprangers et al. - 2022 - Parameter-efficient deep probabilistic forecasting.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3BUB29AX\\S0169207021001850.html}
}

@inproceedings{sprangers_probabilistic_2021,
  title = {Probabilistic {{Gradient Boosting Machines}} for {{Large-Scale Probabilistic Regression}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Sprangers, Olivier and Schelter, Sebastian and {de Rijke}, Maarten},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {1510--1520},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3447548.3467278},
  urldate = {2021-10-09},
  abstract = {Gradient Boosting Machines (GBM) are hugely popular for solving tabular data problems. However, practitioners are not only interested in point predictions, but also in probabilistic predictions in order to quantify the uncertainty of the predictions. Creating such probabilistic predictions is difficult with existing GBM-based solutions: they either require training multiple models or they become too computationally expensive to be useful for large-scale settings. We propose Probabilistic Gradient Boosting Machines (PGBM), a method to create probabilistic predictions with a single ensemble of decision trees in a computationally efficient manner. PGBM approximates the leaf weights in a decision tree as a random variable, and approximates the mean and variance of each sample in a dataset via stochastic tree ensemble update equations. These learned moments allow us to subsequently sample from a specified distribution after training. We empirically demonstrate the advantages of PGBM compared to existing state-of-the-art methods: (i) PGBM enables probabilistic estimates without compromising on point performance in a single model, (ii) PGBM learns probabilistic estimates via a single model only (and without requiring multi-parameter boosting), and thereby offers a speedup of up to several orders of magnitude over existing state-of-the-art methods on large datasets, and (iii) PGBM achieves accurate probabilistic estimates in tasks with complex differentiable loss functions, such as hierarchical time series problems, where we observed up to 10\% improvement in point forecasting performance and up to 300\% improvement in probabilistic forecasting performance.},
  isbn = {978-1-4503-8332-5},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\XPNZ5HSE\Sprangers et al. - 2021 - Probabilistic Gradient Boosting Machines for Large.pdf}
}

@article{srinivas_curl_2020,
  title = {{{CURL}}: {{Contrastive Unsupervised Representations}} for {{Reinforcement Learning}}},
  shorttitle = {{{CURL}}},
  author = {Srinivas, Aravind and Laskin, Michael and Abbeel, Pieter},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.04136 [cs, stat]},
  eprint = {2004.04136},
  primaryclass = {cs, stat},
  urldate = {2020-05-25},
  abstract = {We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs offpolicy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.6x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency and performance of methods that use state-based features.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\AGMMMZJX\Srinivas et al. - 2020 - CURL Contrastive Unsupervised Representations for.pdf}
}

@article{srivastava_dropout_,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  pages = {30},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ``thinned'' networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MG294P4R\Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf}
}

@article{srivastava_learning_2020,
  title = {Learning to {{Forecast}} and {{Forecasting}} to {{Learn}} from the {{COVID-19 Pandemic}}},
  author = {Srivastava, Ajitesh and Prasanna, Viktor K.},
  year = {2020},
  month = may,
  journal = {arXiv:2004.11372 [cs, q-bio, stat]},
  eprint = {2004.11372},
  primaryclass = {cs, q-bio, stat},
  urldate = {2020-05-12},
  abstract = {Accurate forecasts of COVID-19 is central to resource management and building strategies to deal with the epidemic. We propose a heterogeneous infection rate model with human mobility for epidemic modeling, a preliminary version of which we have successfully used during DARPA Grand Challenge 2014. By linearizing the model and using weighted least squares, our model is able to quickly adapt to changing trends and provide extremely accurate predictions of confirmed cases at the level of countries and states of the United States. We show that during the earlier part of the epidemic, using travel data increases the predictions. Training the model to forecast also enables learning characteristics of the epidemic. In particular, we show that changes in model parameters over time can help us quantify how well a state or a country has responded to the epidemic. The variations in parameters also allow us to forecast different scenarios such as what would happen if we were to disregard social distancing suggestions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Populations and Evolution,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YZRHH6GW\Srivastava and Prasanna - 2020 - Learning to Forecast and Forecasting to Learn from.pdf}
}

@article{staerman_area_2019,
  title = {The {{Area}} of the {{Convex Hull}} of {{Sampled Curves}}: A {{Robust Functional Statistical Depth Measure}}},
  shorttitle = {The {{Area}} of the {{Convex Hull}} of {{Sampled Curves}}},
  author = {Staerman, Guillaume and Mozharovskyi, Pavlo and Cl{\'e}men{\c c}on, Stephan},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.04085 [cs, math, stat]},
  eprint = {1910.04085},
  primaryclass = {cs, math, stat},
  urldate = {2020-02-27},
  abstract = {With the ubiquity of sensors in the IoT era, statistical observations are becoming increasingly available in the form of massive (multivariate) time-series. Formulated as unsupervised anomaly detection tasks, an abundance of applications like aviation safety management, the health monitoring of complex infrastructures or fraud detection can now rely on such functional data, acquired and stored with an ever finer granularity. The concept of statistical depth, which reflects centrality of an arbitrary observation w.r.t. a statistical population may play a crucial role in this regard, anomalies corresponding to observations with 'small' depth. Supported by sound theoretical and computational developments in the recent decades, it has proven to be extremely useful, in particular in functional spaces. However, most approaches documented in the literature consist in evaluating independently the centrality of each point forming the time series and consequently exhibit a certain insensitivity to possible shape changes. In this paper, we propose a novel notion of functional depth based on the area of the convex hull of sampled curves, capturing gradual departures from centrality, even beyond the envelope of the data, in a natural fashion. We discuss practical relevance of commonly imposed axioms on functional depths and investigate which of them are satisfied by the notion of depth we promote here. Estimation and computational issues are also addressed and various numerical experiments provide empirical evidence of the relevance of the approach proposed.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\D47ITFB5\\Staerman et al. - 2019 - The Area of the Convex Hull of Sampled Curves a R.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UVNW3UMJ\\1910.html}
}

@article{staneski_truncated_,
  title = {The {{Truncated Cauchy Distribution}}: {{Estimation}} of {{Parameters}} and {{Application}} to {{Stock Returns}}},
  author = {Staneski, Paul G},
  pages = {107},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VPLWNKDM\Staneski - The Truncated Cauchy Distribution Estimation of P.pdf}
}

@inproceedings{stankeviciute_conformal_2021,
  title = {Conformal {{Time-series Forecasting}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Stankeviciute, Kamile and M. Alaa, Ahmed and {van der Schaar}, Mihaela},
  year = {2021},
  volume = {34},
  pages = {6216--6228},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-06},
  abstract = {Current approaches for multi-horizon time series forecasting using recurrent neural networks (RNNs) focus on issuing point estimates, which is insufficient for decision-making in critical application domains where an uncertainty estimate is also required. Existing approaches for uncertainty quantification in RNN-based time-series forecasts are limited as they may require significant alterations to the underlying model architecture, may be computationally complex, may be difficult to calibrate, may incur high sample complexity, and may not provide theoretical guarantees on frequentist coverage. In this paper, we extend the inductive conformal prediction framework to the time-series forecasting setup, and propose a lightweight algorithm to address all of the above limitations, providing uncertainty estimates with theoretical guarantees for any multi-horizon forecast predictor and any dataset with minimal exchangeability assumptions. We demonstrate the effectiveness of our approach by comparing it with existing benchmarks on a variety of synthetic and real-world datasets.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\DN45N4YI\Stankeviciute et al. - 2021 - Conformal Time-series Forecasting.pdf}
}

@article{stolzenburg_power_2020,
  title = {The {{Power}} of {{Linear Recurrent Neural Networks}}},
  author = {Stolzenburg, Frieder and Litz, Sandra and Michael, Olivia and Obst, Oliver},
  year = {2020},
  month = mar,
  journal = {arXiv:1802.03308 [cs]},
  eprint = {1802.03308},
  primaryclass = {cs},
  urldate = {2020-03-11},
  abstract = {Recurrent neural networks are a powerful means to cope with time series. We show how a type of linearly activated recurrent neural networks, which we call predictive neural networks, can approximate any time-dependent function f(t) given by a number of function values. The approximation can effectively be learned by simply solving a linear equation system; no backpropagation or similar methods are needed. Furthermore, the network size can be reduced by taking only most relevant components. Thus, in contrast to others, our approach not only learns network weights but also the network architecture. The networks have interesting properties: They end up in ellipse trajectories in the long run and allow the prediction of further values and compact representations of functions. We demonstrate this by several experiments, among them multiple superimposed oscillators (MSO), robotic soccer, and predicting stock prices. Predictive neural networks outperform the previous state-of-the-art for the MSO task with a minimal number of units.},
  archiveprefix = {arxiv},
  keywords = {15A06,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YU5L4QK9\\Stolzenburg et al. - 2020 - The Power of Linear Recurrent Neural Networks.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\2XSN96TK\\1802.html}
}

@unpublished{stratigakos_endtoend_2022,
  title = {End-to-End {{Learning}} for {{Hierarchical Forecasting}} of {{Renewable Energy Production}} with {{Missing Values}}},
  author = {Stratigakos, Akylas and Van Der Meer, Dennis and Camal, Simon and Kariniotakis, Georges},
  year = {2022},
  month = jan,
  urldate = {2022-04-08},
  abstract = {Power systems feature an inherent hierarchical structure. Ensuring that forecasts across a hierarchy are coherent presents an important challenge in energy forecasting. In this context, proposed reconciliation or end-to-end learning approaches assume coherent historical observations by construction; this assumption, however, is often violated in practice due to equipment failures. This work proposes an end-to-end learning approach for hierarchical forecasting that directly handles missing values. First, we show that a class of off-the-shelf machine learning algorithms already leads to coherent hierarchical forecasts. Next, we describe a conditional stochastic optimization approach based on prescriptive trees for end-to-end learning with missing values, without imputation or disregarding of quality observations. We validate the proposed approach in two case studies comprising 60 wind turbines and 20 photovoltaic parks, respectively. The empirical results show that end-to-end learning outperforms twostep reconciliation approaches and that the proposed solution mitigates the adverse effect of missing values.},
  keywords = {artificial intelligence,digitalisation,hierarchical forecasting,missing data,renewable energy forecasting,smart grids},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\UXHEIY6T\Stratigakos et al. - 2022 - End-to-end Learning for Hierarchical Forecasting o.pdf}
}

@article{strubell_energy_2020,
  title = {Energy and {{Policy Considerations}} for {{Modern Deep Learning Research}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {09},
  pages = {13693--13696},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i09.7123},
  urldate = {2020-09-04},
  abstract = {The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\B4EXGAIQ\\Strubell et al. - 2020 - Energy and Policy Considerations for Modern Deep L.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\42H2K7CJ\\7123.html}
}

@article{sukhbaatar_augmenting_2019,
  title = {Augmenting {{Self-attention}} with {{Persistent Memory}}},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Lample, Guillaume and Jegou, Herve and Joulin, Armand},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.01470 [cs, stat]},
  eprint = {1907.01470},
  primaryclass = {cs, stat},
  urldate = {2019-10-06},
  abstract = {Transformer networks have lead to important progress in language modeling and machine translation. These models include two consecutive modules, a feedforward layer and a self-attention layer. The latter allows the network to capture long term dependencies and are often regarded as the key ingredient in the success of Transformers. Building upon this intuition, we propose a new model that solely consists of attention layers. More precisely, we augment the self-attention layers with persistent memory vectors that play a similar role as the feed-forward layer. Thanks to these vectors, we can remove the feed-forward layer without degrading the performance of a transformer. Our evaluation shows the benefits brought by our model on standard character and word level language modeling benchmarks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FJ9M446Z\Sukhbaatar et al. - 2019 - Augmenting Self-attention with Persistent Memory.pdf}
}

@article{sukhbaatar_endtoend_,
  title = {End-{{To-End Memory Networks}}},
  author = {Sukhbaatar, Sainbayar},
  pages = {9},
  abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\N45YVU92\Sukhbaatar - End-To-End Memory Networks.pdf}
}

@article{sun_bayesian_2019,
  title = {Bayesian {{Temporal Factorization}} for {{Multidimensional Time Series Prediction}}},
  author = {Sun, Lijun and Chen, Xinyu},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.06366 [cs, stat]},
  eprint = {1910.06366},
  primaryclass = {cs, stat},
  urldate = {2020-02-19},
  abstract = {Large-scale and multidimensional spatiotemporal data sets are becoming ubiquitous in many real-world applications such as monitoring urban traffic and air quality. Making predictions on these time series has become a critical challenge due to not only the large-scale and high-dimensional nature but also the considerable amount of missing data. In this paper, we propose a Bayesian temporal factorization (BTF) framework for modeling multidimensional time series---in particular spatiotemporal data---in the presence of missing values. By integrating low-rank matrix/tensor factorization and vector autoregressive (VAR) process into a single probabilistic graphical model, this framework can characterize both global and local consistencies in large-scale time series data. The graphical model allows us to effectively perform probabilistic predictions and produce uncertainty estimates without imputing those missing values. We develop efficient Gibbs sampling algorithms for model inference and test the proposed BTF framework on several real-world spatiotemporal data sets for both missing data imputation and short-term/long-term rolling prediction tasks. The numerical experiments demonstrate the superiority of the proposed BTF approaches over many state-of-the-art techniques.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MVFBV2BE\\Sun and Chen - 2019 - Bayesian Temporal Factorization for Multidimension.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GWD82SU3\\1910.html}
}

@misc{sun_copula_2022,
  title = {Copula {{Conformal Prediction}} for {{Multi-step Time Series Forecasting}}},
  author = {Sun, Sophia and Yu, Rose},
  year = {2022},
  month = dec,
  number = {arXiv:2212.03281},
  eprint = {2212.03281},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.03281},
  urldate = {2023-03-07},
  abstract = {Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper we propose a Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Applications},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\PUJMZTGF\\Sun and Yu - 2022 - Copula Conformal Prediction for Multi-step Time Se.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WBP6WUAL\\2212.html}
}

@article{sun_copula_2023,
  title = {Copula {{Conformal Prediction}} for {{Multi-step Time Series Forecasting}}},
  author = {Sun, Sophia Huiwen and Yu, Rose},
  year = {2023},
  month = feb,
  urldate = {2023-03-07},
  abstract = {Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper we propose a Copula-based Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\A2JUGVZI\Sun and Yu - 2023 - Copula Conformal Prediction for Multi-step Time Se.pdf}
}

@article{sun_neupde_2019,
  title = {{{NeuPDE}}: {{Neural Network Based Ordinary}} and {{Partial Differential Equations}} for {{Modeling Time-Dependent Data}}},
  shorttitle = {{{NeuPDE}}},
  author = {Sun, Yifan and Zhang, Linan and Schaeffer, Hayden},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.03190 [cs, stat]},
  eprint = {1908.03190},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {We propose a neural network based approach for extracting models from dynamic data using ordinary and partial differential equations. In particular, given a time-series or spatio-temporal dataset, we seek to identify an accurate governing system which respects the intrinsic differential structure. The unknown governing model is parameterized by using both (shallow) multilayer perceptrons and nonlinear differential terms, in order to incorporate relevant correlations between spatio-temporal samples. We demonstrate the approach on several examples where the data is sampled from various dynamical systems and give a comparison to recurrent networks and other data-discovery methods. In addition, we show that for MNIST and Fashion MNIST, our approach lowers the parameter cost as compared to other deep neural networks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\IUWWDKJS\Sun et al. - 2019 - NeuPDE Neural Network Based Ordinary and Partial .pdf}
}

@article{sun_stochastic_2019,
  title = {Stochastic {{Prediction}} of {{Multi-Agent Interactions}} from {{Partial Observations}}},
  author = {Sun, Chen and Karlsson, Per and Wu, Jiajun and Tenenbaum, Joshua B. and Murphy, Kevin},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.09641 [cs, stat]},
  eprint = {1902.09641},
  primaryclass = {cs, stat},
  urldate = {2020-02-24},
  abstract = {We present a method that learns to integrate temporal information, from a learned dynamics model, with ambiguous visual information, from a learned vision model, in the context of interacting agents. Our method is based on a graph-structured variational recurrent neural network (Graph-VRNN), which is trained end-to-end to infer the current state of the (partially observed) world, as well as to forecast future states. We show that our method outperforms various baselines on two sports datasets, one based on real basketball trajectories, and one generated by a soccer game engine.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\KA73GGZ5\Sun et al. - 2019 - Stochastic Prediction of Multi-Agent Interactions .pdf}
}

@misc{sung_deep_2019,
  title = {Deep {{Learning}} Papers Reading Roadmap for Anyone Who Are Eager to Learn This Amazing Tech!: Floodsung/{{Deep-Learning-Papers-Reading-Roadmap}}},
  shorttitle = {Deep {{Learning}} Papers Reading Roadmap for Anyone Who Are Eager to Learn This Amazing Tech!},
  author = {Sung, Flood},
  year = {2019},
  month = mar,
  urldate = {2019-03-19}
}

@article{sutskever_importance_,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  author = {Sutskever, Ilya and Martens, James and Dahl, George},
  pages = {9},
  abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7CKFTHDX\Sutskever et al. - On the importance of initialization and momentum i.pdf}
}

@article{sutskever_sequence_2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  year = {2014},
  month = sep,
  journal = {arXiv:1409.3215 [cs]},
  eprint = {1409.3215},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\G2MK28WB\Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf}
}

@article{syntetos_accuracy_2005,
  title = {The Accuracy of Intermittent Demand Estimates},
  author = {Syntetos, Aris A. and Boylan, John E.},
  year = {2005},
  month = apr,
  journal = {International Journal of Forecasting},
  volume = {21},
  number = {2},
  pages = {303--314},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2004.10.001},
  urldate = {2019-06-03},
  abstract = {Intermittent demand appears sporadically, with some time periods showing no demand at all. In this paper, four forecasting methods, Simple Moving Average (SMA, 13 periods), Single Exponential Smoothing (SES), Croston's method, and a new method (based on Croston's approach) recently developed by the authors, are compared on 3000 real intermittent demand data series from the automotive industry. The mean signed and relative geometric root-mean-square errors are shown to meet the theoretical and practical requirements of intermittent demand, as do the Percentage Better and Percentage Best summary statistics based on these measures. These measures are subsequently applied in a simulation experiment. The out-of-sample comparison results indicate superior performance of the new method. In addition, the results show that the mean signed error is not strongly scale dependent and the relative geometric root-mean-square error is a well-behaved accuracy measure for intermittent demand.},
  keywords = {Accuracy measures,Croston's method,Demand forecasting,Exponential smoothing,Forecasting competition,Intermittent demand},
  annotation = {00336},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TXVWDWT5\\Syntetos and Boylan - 2005 - The accuracy of intermittent demand estimates.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WVDH3YNF\\S0169207004000792.html}
}

@inproceedings{szegedy_going_2015,
  title = {Going Deeper with Convolutions},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  month = jun,
  pages = {1--9},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298594},
  urldate = {2019-03-19},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\L9BH57FD\Szegedy et al. - 2015 - Going deeper with convolutions.pdf}
}

@article{tadayon_comprehensive_2020,
  title = {Comprehensive {{Analysis}} of {{Time Series Forecasting Using Neural Networks}}},
  author = {Tadayon, Manie and Iwashita, Yumi},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.09547 [cs, stat]},
  eprint = {2001.09547},
  primaryclass = {cs, stat},
  urldate = {2020-02-19},
  abstract = {Time series forecasting has gained lots of attention recently; this is because many real-world phenomena can be modeled as time series. The massive volume of data and recent advancements in the processing power of the computers enable researchers to develop more sophisticated machine learning algorithms such as neural networks to forecast the time series data. In this paper, we propose various neural network architectures to forecast the time series data using the dynamic measurements; moreover, we introduce various architectures on how to combine static and dynamic measurements for forecasting. We also investigate the importance of performing techniques such as anomaly detection and clustering on forecasting accuracy. Our results indicate that clustering can improve the overall prediction time as well as improve the forecasting performance of the neural network. Furthermore, we show that feature-based clustering can outperform the distance-based clustering in terms of speed and efficiency. Finally, our results indicate that adding more predictors to forecast the target variable will not necessarily improve the forecasting accuracy.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FJADJEUH\Tadayon and Iwashita - 2020 - Comprehensive Analysis of Time Series Forecasting .pdf}
}

@article{taieb_boosting_,
  title = {Boosting Multi-Step Autoregressive Forecasts},
  author = {Taieb, Souhaib Ben and Hyndman, Rob J},
  pages = {10},
  abstract = {Multi-step forecasts can be produced recursively by iterating a one-step model, or directly using a specific model for each horizon. Choosing between these two strategies is not an easy task since it involves a trade-off between bias and estimation variance over the forecast horizon. Using a nonlinear machine learning model makes the tradeoff even more difficult. To address this issue, we propose a new forecasting strategy which boosts traditional recursive linear forecasts with a direct strategy using a boosting autoregression procedure at each horizon. First, we investigate the performance of the proposed strategy in terms of bias and variance decomposition of the error using simulated time series. Then, we evaluate the proposed strategy on real-world time series from two forecasting competitions. Overall, we obtain excellent performance with respect to the standard forecasting strategies.},
  langid = {english},
  annotation = {00021},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YEG92UMH\Taieb and Hyndman - Boosting multi-step autoregressive forecasts.pdf}
}

@inproceedings{taieb_coherent_2017,
  title = {Coherent {{Probabilistic Forecasts}} for {{Hierarchical Time Series}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Taieb, Souhaib Ben and Taylor, James W. and Hyndman, Rob J.},
  year = {2017},
  month = jul,
  pages = {3348--3357},
  urldate = {2019-04-17},
  abstract = {Many applications require forecasts for a hierarchy comprising a set of time series along with aggregates of subsets of these series. Hierarchical forecasting require not only good prediction accur...},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\H8MD5Q38\\Taieb et al. - 2017 - Coherent Probabilistic Forecasts for Hierarchical .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\34EULX4L\\taieb17a.html}
}

@article{taieb_forecasting_2016,
  title = {Forecasting {{Uncertainty}} in {{Electricity Smart Meter Data}} by {{Boosting Additive Quantile Regression}}},
  author = {Taieb, S. Ben and Huser, R. and Hyndman, R. J. and Genton, M. G.},
  year = {2016},
  month = sep,
  journal = {IEEE Transactions on Smart Grid},
  volume = {7},
  number = {5},
  pages = {2448--2455},
  issn = {1949-3053},
  doi = {10.1109/TSG.2016.2527820},
  abstract = {Smart electricity meters are currently deployed in millions of households to collect detailed individual electricity consumption data. Compared with traditional electricity data based on aggregated consumption, smart meter data are much more volatile and less predictable. There is a need within the energy industry for probabilistic forecasts of household electricity consumption to quantify the uncertainty of future electricity demand in order to undertake appropriate planning of generation and distribution. We propose to estimate an additive quantile regression model for a set of quantiles of the future distribution using a boosting procedure. By doing so, we can benefit from flexible and interpretable models, which include an automatic variable selection. We compare our approach with three benchmark methods on both aggregated and disaggregated scales using a smart meter data set collected from 3639 households in Ireland at 30-min intervals over a period of 1.5 years. The empirical results demonstrate that our approach based on quantile regression provides better forecast accuracy for disaggregated demand, while the traditional approach based on a normality assumption (possibly after an appropriate Box-Cox transformation) is a better approximation for aggregated demand. These results are particularly useful since more energy data will become available at the disaggregated level in the future.},
  keywords = {additive quantile regression,Box-Cox transformation,disaggregated demand,electricity smart meter data,energy industry,Forecasting,forecasting uncertainty,gradient boosting,Ireland,load forecasting,Load forecasting,Load modeling,Predictive models,probabilistic forecasts,Probabilistic load forecasting,Probabilistic logic,quantile regression,regression analysis,smart electricity meters,smart meter data,smart meters,Smart meters,time 30 min,Uncertainty},
  annotation = {00072},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\29I3LV5N\7423794.html}
}

@article{taieb_hierarchical_2021,
  title = {Hierarchical {{Probabilistic Forecasting}} of {{Electricity Demand With Smart Meter Data}}},
  author = {Taieb, Souhaib Ben and Taylor, James W. and Hyndman, Rob J.},
  year = {2021},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {116},
  number = {533},
  pages = {27--43},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1736081},
  urldate = {2021-10-18},
  abstract = {Decisions regarding the supply of electricity across a power grid must take into consideration the inherent uncertainty in demand. Optimal decision-making requires probabilistic forecasts for demand in a hierarchy with various levels of aggregation, such as substations, cities, and regions. The forecasts should be coherent in the sense that the forecast of the aggregated series should equal the sum of the forecasts of the corresponding disaggregated series. Coherency is essential, since the allocation of electricity at one level of the hierarchy relies on the appropriate amount being provided from the previous level. We introduce a new probabilistic forecasting method for a large hierarchy based on UK residential smart meter data. We find our method provides coherent and accurate probabilistic forecasts, as a result of an effective forecast combination. Furthermore, by avoiding distributional assumptions, we find that our method captures the variety of distributions in the smart meter hierarchy. Finally, the results confirm that, to ensure coherency in our large-scale hierarchy, it is sufficient to model a set of lower-dimension dependencies, rather than modeling the entire joint distribution of all series in the hierarchy. In achieving coherent and accurate hierarchical probabilistic forecasts, this work contributes to improved decision-making for smart grids. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  keywords = {Aggregates of time series,Empirical copulas,Forecast combinations,Predictive distributions,Smart meters},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TWZ5LIDF\\Taieb et al. - 2021 - Hierarchical Probabilistic Forecasting of Electric.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IX2G9CYH\\01621459.2020.html}
}

@article{taieb_probabilistic_2015,
  title = {Probabilistic Time Series Forecasting with Boosted Additive Models: An Application to Smart Meter Data},
  author = {Taieb, Souhaib Ben and Huser, Raphael and Hyndman, Rob J and Genton, Marc G},
  year = {2015},
  pages = {30},
  abstract = {A large body of the forecasting literature so far has been focused on forecasting the conditional mean of future observations. However, there is an increasing need for generating the entire conditional distribution of future observations in order to effectively quantify the uncertainty in time series data. We present two different methods for probabilistic time series forecasting that allow the inclusion of a possibly large set of exogenous variables. One method is based on forecasting both the conditional mean and variance of the future distribution using a traditional regression approach. The other directly computes multiple quantiles of the future distribution using quantile regression. We propose an implementation for the two methods based on boosted additive models, which enjoy many useful properties including accuracy, flexibility, interpretability and automatic variable selection. We conduct extensive experiments using electricity smart meter data, on both aggregated and disaggregated scales, to compare the two forecasting methods for the challenging problem of forecasting the distribution of future electricity consumption. The empirical results demonstrate that the mean and variance forecasting provides better forecasts for aggregated demand, while the flexibility of the quantile regression approach is more suitable for disaggregated demand. These results are particularly useful since more energy data will become available at the disaggregated level in the future.},
  langid = {english},
  annotation = {00006},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZEVUMYEU\Taieb et al. - 2015 - Probabilistic time series forecasting with boosted.pdf}
}

@article{taieb_recursive_2012,
  title = {Recursive and Direct Multi-Step Forecasting: The Best of Both Worlds},
  author = {Taieb, Souhaib Ben and Hyndman, Rob J},
  year = {2012},
  pages = {36},
  abstract = {We propose a new forecasting strategy, called rectify, that seeks to combine the best properties of both the recursive and direct forecasting strategies. The rationale behind the rectify strategy is to begin with biased recursive forecasts and adjust them so they are unbiased and have smaller error. We use linear and nonlinear simulated time series to investigate the performance of the rectify strategy and compare the results with those from the recursive and the direct strategies. We also carry out some experiments using real world time series from the M3 and the NN5 forecasting competitions. We find that the rectify strategy is always better than, or at least has comparable performance to, the best of the recursive and the direct strategies. This finding makes the rectify strategy very attractive as it avoids making a choice between the recursive and the direct strategies which can be a difficult task in real-world applications.},
  langid = {english},
  annotation = {00016},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JL78L2IF\Taieb and Hyndman - 2012 - Recursive and direct multi-step forecasting the b.pdf}
}

@article{taieb_regularization_2017,
  title = {Regularization in {{Hierarchical Time Series Forecasting}} with {{Application}} to {{Electricity Smart Meter Data}}},
  author = {Taieb, Souhaib Ben and Yu, Jiafan and Barreto, Mateus and Rajagopal, Ram},
  year = {2017},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {31},
  number = {1},
  issn = {2374-3468},
  urldate = {2021-02-04},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {regularization,time series},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\3CJLRLXN\\Taieb et al. - 2017 - Regularization in Hierarchical Time Series Forecas.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IXC67VQL\\11167.html}
}

@inproceedings{taieb_sparse_2017,
  title = {Sparse and {{Smooth Adjustments}} for {{Coherent Forecasts}} in {{Temporal Aggregation}} of {{Time Series}}},
  booktitle = {Proceedings of the {{Time Series Workshop}} at {{NIPS}} 2016},
  author = {Taieb, Souhaib Ben},
  year = {2017},
  month = feb,
  pages = {16--26},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2022-05-25},
  abstract = {Independent forecasts obtained from different temporal aggregates of a given time series may not be mutually consistent. State-of the art forecasting methods usually apply adjustments on the individual level forecasts to satisfy the aggregation constraints. These adjustments require the estimation of the covariance between the individual forecast errors at all aggregation levels. In order to keep a maximum number of individual forecasts unaffected by estimation errors, we propose a new forecasting algorithm that provides sparse and smooth adjustments while still preserving the aggregation constraints. The algorithm computes the revised forecasts by solving a generalized lasso problem. It is shown that it not only provides accurate forecasts, but also applies a significantly smaller number of adjustments to the base forecasts in a large-scale smart meter dataset.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\4ZMRLXPW\Taieb - 2017 - Sparse and Smooth Adjustments for Coherent Forecas.pdf}
}

@article{takahashi_modeling_2019,
  title = {Modeling Financial Time-Series with Generative Adversarial Networks},
  author = {Takahashi, Shuntaro and Chen, Yu and {Tanaka-Ishii}, Kumiko},
  year = {2019},
  month = aug,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {527},
  pages = {121261},
  issn = {0378-4371},
  doi = {10.1016/j.physa.2019.121261},
  urldate = {2020-05-13},
  abstract = {Financial time-series modeling is a challenging problem as it retains various complex statistical properties and the mechanism behind the process is unrevealed to a large extent. In this paper, a deep neural networks based approach, generative adversarial networks (GANs) for financial time-series modeling is presented. GANs learn the properties of data and generate realistic data in a data-driven manner. The GAN model produces a time-series that recovers the statistical properties of financial time-series such as the linear unpredictability, the heavy-tailed price return distribution, volatility clustering, leverage effects, the coarse-fine volatility correlation, and the gain/loss asymmetry.},
  langid = {english},
  keywords = {Deep learning,Financial market,Generative adversarial networks,Stylized facts},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\G85XT887\\Takahashi et al. - 2019 - Modeling financial time-series with generative adv.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CACU9VY8\\S0378437119307277.html}
}

@article{takeuchi_nonparametric_,
  title = {Nonparametric {{Quantile Estimation}}},
  author = {Takeuchi, Ichiro and Le, Quoc V and Sears, Timothy D and Smola, Alexander J},
  pages = {34},
  abstract = {In regression, the desired estimate of y|x is not always given by a conditional mean, although this is most common. Sometimes one wants to obtain a good estimate that satisfies the property that a proportion, {$\tau$}, of y|x, will be below the estimate. For {$\tau$} = 0.5 this is an estimate of the median. What might be called median regression, is subsumed under the term quantile regression. We present a nonparametric version of a quantile estimator, which can be obtained by solving a simple quadratic programming problem and provide uniform convergence statements and bounds on the quantile property of our estimator. Experimental results show the feasibility of the approach and competitiveness of our method with existing ones. We discuss several types of extensions including an approach to solve the quantile crossing problems, as well as a method to incorporate prior qualitative knowledge such as monotonicity constraints.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7GY8XPH8\takeuchi06a.pdf}
}

@article{tan_learning_2020,
  title = {Learning to {{Hash}} with {{Graph Neural Networks}} for {{Recommender Systems}}},
  author = {Tan, Qiaoyu and Liu, Ninghao and Zhao, Xing and Yang, Hongxia and Zhou, Jingren and Hu, Xia},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.01917 [cs]},
  eprint = {2003.01917},
  primaryclass = {cs},
  urldate = {2021-12-15},
  abstract = {Recommender systems in industry generally include two stages: recall and ranking. Recall refers to efficiently identify hundreds of candidate items that user may interest in from a large volume of item corpus, while the latter aims to output a precise ranking list using complex ranking models. Recently, graph representation learning has attracted much attention in supporting high quality candidate search at scale. Despite its effectiveness in learning embedding vectors for objects in the user-item interaction network, the computational costs to infer users' preferences in continuous embedding space are tremendous. In this work, we investigate the problem of hashing with graph neural networks (GNNs) for high quality retrieval, and propose a simple yet effective discrete representation learning framework to jointly learn continuous and discrete codes. Specifically, a deep hashing with GNNs (HashGNN) is presented, which consists of two components, a GNN encoder for learning node representations, and a hash layer for encoding representations to hash codes. The whole architecture is trained end-to-end by jointly optimizing two losses, i.e., reconstruction loss from reconstructing observed links, and ranking loss from preserving the relative ordering of hash codes. A novel discrete optimization strategy based on straight through estimator (STE) with guidance is proposed. The principal idea is to avoid gradient magnification in back-propagation of STE with continuous embedding guidance, in which we begin from learning an easier network that mimic the continuous embedding and let it evolve during the training until it finally goes back to STE. Comprehensive experiments over three publicly available and one real-world Alibaba company datasets demonstrate that our model not only can achieve comparable performance compared with its continuous counterpart but also runs multiple times faster during inference.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\UFUWGRVI\Tan et al. - 2020 - Learning to Hash with Graph Neural Networks for Re.pdf}
}

@article{tang_joint_2019,
  title = {Joint {{Modeling}} of {{Local}} and {{Global Temporal Dynamics}} for {{Multivariate Time Series Forecasting}} with {{Missing Values}}},
  author = {Tang, Xianfeng and Yao, Huaxiu and Sun, Yiwei and Aggarwal, Charu and Mitra, Prasenjit and Wang, Suhang},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.10273 [cs, stat]},
  eprint = {1911.10273},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {Multivariate time series (MTS) forecasting is widely used in various domains, such as meteorology and traffic. Due to limitations on data collection, transmission, and storage, real-world MTS data usually contains missing values, making it infeasible to apply existing MTS forecasting models such as linear regression and recurrent neural networks. Though many efforts have been devoted to this problem, most of them solely rely on local dependencies for imputing missing values, which ignores global temporal dynamics. Local dependencies/patterns would become less useful when the missing ratio is high, or the data have consecutive missing values; while exploring global patterns can alleviate such problems. Thus, jointly modeling local and global temporal dynamics is very promising for MTS forecasting with missing values. However, work in this direction is rather limited. Therefore, we study a novel problem of MTS forecasting with missing values by jointly exploring local and global temporal dynamics. We propose a new framework LGnet, which leverages memory network to explore global patterns given estimations from local perspectives. We further introduce adversarial training to enhance the modeling of global temporal distribution. Experimental results on real-world datasets show the effectiveness of LGnet for MTS forecasting with missing values and its robustness under various missing ratios.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FRIVF3BE\\Tang et al. - 2019 - Joint Modeling of Local and Global Temporal Dynami.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8WMLZUQS\\1911.html}
}

@inproceedings{tang_probabilistic_2021,
  title = {Probabilistic {{Transformer For Time Series Analysis}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tang, Binh and Matteson, David S},
  year = {2021},
  volume = {34},
  pages = {23592--23608},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-06},
  abstract = {Generative modeling of multivariate time series has remained challenging partly due to the complex, non-deterministic dynamics across long-distance timesteps. In this paper, we propose deep probabilistic methods that combine state-space models (SSMs) with transformer architectures. In contrast to previously proposed SSMs, our approaches use attention mechanism to model non-Markovian dynamics in the latent space and avoid recurrent neural networks entirely. We also extend our models to include several layers of stochastic variables organized in a hierarchy for further expressiveness. Compared to transformer models, ours are probabilistic, non-autoregressive, and capable of generating diverse long-term forecasts with uncertainty estimates. Extensive experiments show that our models consistently outperform competitive baselines on various tasks and datasets, including time series forecasting and human motion prediction.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZUXUEVZL\Tang and Matteson - 2021 - Probabilistic Transformer For Time Series Analysis.pdf}
}

@article{tang_why_2018,
  title = {Why {{Self-Attention}}? {{A Targeted Evaluation}} of {{Neural Machine Translation Architectures}}},
  shorttitle = {Why {{Self-Attention}}?},
  author = {Tang, Gongbo and M{\"u}ller, Mathias and Rios, Annette and Sennrich, Rico},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.08946 [cs]},
  eprint = {1808.08946},
  primaryclass = {cs},
  urldate = {2019-04-03},
  abstract = {Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\SESMTXSS\\Tang et al. - 2018 - Why Self-Attention A Targeted Evaluation of Neura.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IDVRAYXB\\1808.html}
}

@article{tank_neural_2018,
  title = {Neural {{Granger Causality}} for {{Nonlinear Time Series}}},
  author = {Tank, Alex and Covert, Ian and Foti, Nicholas and Shojaie, Ali and Fox, Emily},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.05842 [stat]},
  eprint = {1802.05842},
  primaryclass = {stat},
  urldate = {2019-04-15},
  abstract = {While most classical approaches to Granger causality detection assume linear dynamics, many interactions in applied domains, like neuroscience and genomics, are inherently nonlinear. In these cases, using linear models may lead to inconsistent estimation of Granger causal interactions. We propose a class of nonlinear methods by applying structured multilayer perceptrons (MLPs) or recurrent neural networks (RNNs) combined with sparsity-inducing penalties on the weights. By encouraging specific sets of weights to be zero\textemdash in particular through the use of convex group-lasso penalties\textemdash we can extract the Granger causal structure. To further contrast with traditional approaches, our framework naturally enables us to efficiently capture long-range dependencies between series either via our RNNs or through an automatic lag selection in the MLP. We show that our neural Granger causality methods outperform state-of-the-art nonlinear Granger causality methods on the DREAM3 challenge data. This data consists of nonlinear gene expression and regulation time courses with only a limited number of time points. The successes we show in this challenging dataset provide a powerful example of how deep learning can be useful in cases that go beyond prediction on large datasets. We likewise demonstrate our methods in detecting nonlinear interactions in a human motion capture dataset.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {read - in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\X8KX7AZI\Tank et al. - 2018 - Neural Granger Causality for Nonlinear Time Series.pdf}
}

@article{tanno_adaptive_2018,
  title = {Adaptive {{Neural Trees}}},
  author = {Tanno, Ryutaro and Arulkumaran, Kai and Alexander, Daniel C. and Criminisi, Antonio and Nori, Aditya},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.06699 [cs, stat]},
  eprint = {1807.06699},
  primaryclass = {cs, stat},
  urldate = {2019-08-30},
  abstract = {Deep neural networks and decision trees operate on largely separate paradigms; typically, the former performs representation learning with prespecified architectures, while the latter is characterised by learning hierarchies over pre-specified features with data-driven architectures. We unite the two via adaptive neural trees (ANTs) that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving competitive performance on classification and regression datasets, ANTs benefit from (i) lightweight inference via conditional computation, (ii) hierarchical separation of features useful to the task e.g. learning meaningful class associations, such as separating natural vs. man-made objects, and (iii) a mechanism to adapt the architecture to the size and complexity of the training dataset.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VFEEYZVR\Tanno et al. - 2018 - Adaptive Neural Trees.pdf}
}

@article{tao_hierarchical_2018,
  title = {Hierarchical {{Attention-Based Recurrent Highway Networks}} for {{Time Series Prediction}}},
  author = {Tao, Yunzhe and Ma, Lin and Zhang, Weizhong and Liu, Jian and Liu, Wei and Du, Qiang},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.00685 [cs, stat]},
  eprint = {1806.00685},
  primaryclass = {cs, stat},
  urldate = {2019-04-03},
  abstract = {Time series prediction has been studied in a variety of domains. However, it is still challenging to predict future series given historical observations and past exogenous data. Existing methods either fail to consider the interactions among different components of exogenous variables which may affect the prediction accuracy, or cannot model the correlations between exogenous data and target data. Besides, the inherent temporal dynamics of exogenous data are also related to the target series prediction, and thus should be considered as well. To address these issues, we propose an end-to-end deep learning model, i.e., Hierarchical attention-based Recurrent Highway Network (HRHN), which incorporates spatio-temporal feature extraction of exogenous variables and temporal dynamics modeling of target variables into a single framework. Moreover, by introducing the hierarchical attention mechanism, HRHN can adaptively select the relevant exogenous features in different semantic levels. We carry out comprehensive empirical evaluations with various methods over several datasets, and show that HRHN outperforms the state of the arts in time series prediction, especially in capturing sudden changes and sudden oscillations of time series.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read - in related work,Statistics - Machine Learning},
  annotation = {00001},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\LDXEX4JP\Tao et al. - 2018 - Hierarchical Attention-Based Recurrent Highway Net.pdf}
}

@article{taylor_forecasting_2007,
  title = {Forecasting Daily Supermarket Sales Using Exponentially Weighted Quantile Regression},
  author = {Taylor, James W.},
  year = {2007},
  month = apr,
  journal = {European Journal of Operational Research},
  volume = {178},
  number = {1},
  pages = {154--167},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2006.02.006},
  urldate = {2019-11-21},
  abstract = {Inventory control systems typically require the frequent updating of forecasts for many different products. In addition to point predictions, interval forecasts are needed to set appropriate levels of safety stock. The series considered in this paper are characterised by high volatility and skewness, which are both time-varying. These features motivate the consideration of forecasting methods that are robust with regard to distributional assumptions. The widespread use of exponential smoothing for point forecasting in inventory control motivates the development of the approach for interval forecasting. In this paper, we construct interval forecasts from quantile predictions generated using exponentially weighted quantile regression. The approach amounts to exponential smoothing of the cumulative distribution function, and can be viewed as an extension of generalised exponential smoothing to quantile forecasting. Empirical results are encouraging, with improvements over traditional methods being particularly apparent when the approach is used as the basis for robust point forecasting.},
  langid = {english},
  keywords = {Exponential smoothing,Forecasting,Interval forecasting,Quantile regression,Robust point forecasting},
  annotation = {00120},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\CJNKW22M\\Taylor - 2007 - Forecasting daily supermarket sales using exponent.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\S27PMSUJ\\S0377221706000737.html}
}

@article{taylor_forecasting_2018,
  title = {Forecasting at {{Scale}}},
  author = {Taylor, Sean J. and Letham, Benjamin},
  year = {2018},
  month = jan,
  journal = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {37--45},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1380080},
  urldate = {2019-04-17},
  abstract = {Forecasting is a common data science task that helps organizations with capacity planning, goal setting, and anomaly detection. Despite its importance, there are serious challenges associated with producing reliable and high-quality forecasts\textemdash especially when there are a variety of time series and analysts with expertise in time series modeling are relatively rare. To address these challenges, we describe a practical approach to forecasting ``at scale'' that combines configurable models with analyst-in-the-loop performance analysis. We propose a modular regression model with interpretable parameters that can be intuitively adjusted by analysts with domain knowledge about the time series. We describe performance analyses to compare and evaluate forecasting procedures, and automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable, practical forecasting of business time series.},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\YZVXCQAF\\Taylor and Letham - 2018 - Forecasting at Scale.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WBFM2WEU\\00031305.2017.html}
}

@article{teflioudi_exact_,
  title = {Exact and {{Approximate Maximum Inner Product Search}} with {{LEMP}} (Draft / Author Version)},
  author = {Teflioudi, Christina and Gemulla, Rainer},
  journal = {ACM Transactions on Database Systems},
  pages = {51},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PT4PNN65\Teï¬‚ioudi and Gemulla - Exact and Approximate Maximum Inner Product Search.pdf}
}

@misc{teng_predictive_2022,
  title = {Predictive {{Inference}} with {{Feature Conformal Prediction}}},
  author = {Teng, Jiaye and Wen, Chuan and Zhang, Dinghuai and Bengio, Yoshua and Gao, Yang and Yuan, Yang},
  year = {2022},
  month = sep,
  number = {arXiv:2210.00173},
  eprint = {2210.00173},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.00173},
  urldate = {2023-03-07},
  abstract = {Conformal prediction is a distribution-free technique for establishing valid prediction intervals. Although conventionally people conduct conformal prediction in the output space, this is not the only possibility. In this paper, we propose feature conformal prediction, which extends the scope of conformal prediction to semantic feature spaces by leveraging the inductive bias of deep representation learning. From a theoretical perspective, we demonstrate that feature conformal prediction provably outperforms regular conformal prediction under mild assumptions. Our approach could be combined with not only vanilla conformal prediction, but also other adaptive conformal prediction methods. Experiments on various predictive inference tasks corroborate the efficacy of our method.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\447S2EX7\\Teng et al. - 2022 - Predictive Inference with Feature Conformal Predic.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\H4GI6FF6\\2210.html}
}

@misc{tetsuyasakai_sigir2018tutorial_08:01:42UTC,
  type = {Technology},
  title = {Sigir2018tutorial},
  author = {Tetsuya Sakai},
  year = {08:01:42 UTC},
  urldate = {2019-04-03},
  abstract = {Conducting Laboratory Experiments Properlywith Statistical Tools: An Easy}
}

@misc{theodorou_exploring_2021,
  title = {Exploring the Representativeness of the {{M5}} Competition Data},
  author = {Theodorou, Evangelos and Wang, Shengjie and Kang, Yanfei and Spiliotis, Evangelos and Makridakis, Spyros and Assimakopoulos, Vassilios},
  year = {2021},
  month = jul,
  number = {arXiv:2103.02941},
  eprint = {2103.02941},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2103.02941},
  urldate = {2022-05-25},
  abstract = {The main objective of the M5 competition, which focused on forecasting the hierarchical unit sales of Walmart, was to evaluate the accuracy and uncertainty of forecasting methods in the field in order to identify best practices and highlight their practical implications. However, whether the findings of the M5 competition can be generalized and exploited by retail firms to better support their decisions and operation depends on the extent to which the M5 data is sufficiently similar to unit sales data of retailers that operate in different regions, sell different types of products, and consider different marketing strategies. To answer this question, we analyze the characteristics of the M5 time series and compare them with those of two grocery retailers, namely Corporaci\textbackslash 'on Favorita and a major Greek supermarket chain, using feature spaces. Our results suggest that there are only small discrepancies between the examined data sets, supporting the representativeness of the M5 data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Applications},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\B2AK32AS\\Theodorou et al. - 2021 - Exploring the representativeness of the M5 competi.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GFXCBYFN\\2103.html}
}

@misc{theodosiou_forecasting_2021,
  type = {{{SSRN Scholarly Paper}}},
  title = {Forecasting with {{Deep Temporal Hierarchies}}},
  author = {Theodosiou, Filotas and Kourentzes, Nikolaos},
  year = {2021},
  month = sep,
  number = {3918315},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.3918315},
  urldate = {2023-06-01},
  abstract = {In time series analysis and forecasting, the identification of an appropriate model remains a challenging task. Model misspecification can lead to erroneous forecasts and insights. The use of multiple views of the same time series by constructing temporally aggregate levels has been proposed as a way to overcome the model specification and selection uncertainty, with ample empirical evidence of forecast accuracy gains. Temporal Hierarchies is the most popular approach to achieve this, which itself is based on research in hierarchical forecasting. Although there has been substantial progress in this literature, the vast majority of methods rely on a restricted linear combination of different model outputs across the hierarchy. We investigate the use of deep learning to augment temporal hierarchies, relaxing the classical restrictions. Specifically, we look at deep learning for the generation of all the base forecasts, the hierarchical reconciliation, and an end-to-end method that embeds all steps in a single neural network. We inspect the performance of the proposed methods when applied to individual time series, or with global training across complete sets of series. We further investigate the requirements in terms of series set size, illustrating the conditions where deep learning temporal hierarchies outperform conventional temporal hierarchies.},
  langid = {english},
  keywords = {Artificial intelligence,Deep Learning,Forecasting,Temporal Hierarchies},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PADBFR7H\Theodosiou and Kourentzes - 2021 - Forecasting with Deep Temporal Hierarchies.pdf}
}

@article{thompson_classification_2020,
  title = {Classification with the Matrix-Variate-\$t\$ Distribution},
  author = {Thompson, Geoffrey Z. and Maitra, Ranjan and Meeker, William Q. and Bastawros, Ashraf},
  year = {2020},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  eprint = {1907.09565},
  pages = {1--7},
  issn = {1061-8600, 1537-2715},
  doi = {10.1080/10618600.2019.1696208},
  urldate = {2020-02-19},
  abstract = {Matrix-variate distributions can intuitively model the dependence structure of matrix-valued observations that arise in applications with multivariate time series, spatio-temporal or repeated measures. This paper develops an Expectation-Maximization algorithm for discriminant analysis and classification with matrix-variate \$t\$-distributions. The methodology shows promise on simulated datasets or when applied to the forensic matching of fractured surfaces or the classification of functional Magnetic Resonance, satellite or hand gestures images.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EZ3B9GLQ\\Thompson et al. - 2020 - Classification with the matrix-variate-$t$ distrib.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\XH7BDXAD\\1907.html}
}

@incollection{tobar_learning_2015,
  title = {Learning {{Stationary Time Series}} Using {{Gaussian Processes}} with {{Nonparametric Kernels}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Tobar, Felipe and Bui, Thang D and Turner, Richard E},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {3501--3509},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  keywords = {read - not in related work},
  annotation = {00028},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\L27ICCEG\\Tobar et al. - 2015 - Learning Stationary Time Series using Gaussian Pro.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LJVALR2Z\\5772-learning-stationary-time-series-using-gaussian-processes-with-nonparametric-kernels.html}
}

@misc{tomasetti_updating_2019,
  title = {Updating {{Variational Bayes}}: {{Fast}} Sequential Posterior Inference},
  shorttitle = {Updating {{Variational Bayes}}},
  author = {Tomasetti, Nathaniel and Forbes, Catherine S. and Panagiotelis, Anastasios},
  year = {2019},
  month = aug,
  number = {arXiv:1908.00225},
  eprint = {1908.00225},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-05-24},
  abstract = {Variational Bayesian (VB) methods produce posterior inference in a time frame considerably smaller than traditional Markov Chain Monte Carlo approaches. Although the VB posterior is an approximation, it has been shown to produce good parameter estimates and predicted values when a rich classes of approximating distributions are considered. In this paper we propose Updating VB (UVB), a recursive algorithm used to update a sequence of VB posterior approximations in an online setting, with the computation of each posterior update requiring only the data observed since the previous update. An extension to the proposed algorithm, named UVB-IS, allows the user to trade accuracy for a substantial increase in computational speed through the use of importance sampling. The two methods and their properties are detailed in two separate simulation studies. Two empirical illustrations of the proposed UVB methods are provided, including one where a Dirichlet Process Mixture model with a novel posterior dependence structure is repeatedly updated in the context of predicting the future behaviour of vehicles on a stretch of the US Highway 101.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {62-04,Computer Science - Machine Learning,Statistics - Computation},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HE78LUT6\Tomasetti et al. - 2019 - Updating Variational Bayes Fast sequential poster.pdf}
}

@article{torgo_functional_,
  title = {Functional {{Models}} for {{Regression Tree Leaves}}},
  author = {Torgo, Lu{\'i}s},
  pages = {9},
  abstract = {This paper presents a study about functional models for regression tree leaves. We evaluate experimentally several alternatives to the averages commonly used in regression trees. We have implemented a regression tree learner (HTL) that is able to use several alternative models in the tree leaves. We study the effect on accuracy and the computational cost of these alternatives. The experiments carried out on 11 data sets revealed that it is possible to significantly outperform the ``naive'' averages of regression trees. Among the four alternative models that we evaluated, kernel regressors were usually the best in terms of accuracy. Our study also indicates that by integrating regression trees with other regression approaches we are able to overcome the limitations of individual methods both in terms of accuracy as well as in computational efficiency.},
  langid = {english},
  annotation = {00138},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\N4U2B3FX\Torgo - Functional Models for Regression Tree Leaves.pdf}
}

@article{touloumis_nonparametric_2015,
  title = {Nonparametric {{Stein-type Shrinkage Covariance Matrix Estimators}} in {{High-Dimensional Settings}}},
  author = {Touloumis, Anestis},
  year = {2015},
  month = mar,
  journal = {Computational Statistics \& Data Analysis},
  volume = {83},
  eprint = {1410.4726},
  primaryclass = {stat},
  pages = {251--261},
  issn = {01679473},
  doi = {10.1016/j.csda.2014.10.018},
  urldate = {2022-08-09},
  abstract = {Estimating a covariance matrix is an important task in applications where the number of variables is larger than the number of observations. In the literature, shrinkage approaches for estimating a high-dimensional covariance matrix are employed to circumvent the limitations of the sample covariance matrix. A new family of nonparametric Stein-type shrinkage covariance estimators is proposed whose members are written as a convex linear combination of the sample covariance matrix and of a predefined invertible target matrix. Under the Frobenius norm criterion, the optimal shrinkage intensity that defines the best convex linear combination depends on the unobserved covariance matrix and it must be estimated from the data. A simple but effective estimation process that produces nonparametric and consistent estimators of the optimal shrinkage intensity for three popular target matrices is introduced. In simulations, the proposed Stein-type shrinkage covariance matrix estimator based on a scaled identity matrix appeared to be up to 80\% more efficient than existing ones in extreme high-dimensional settings. A colon cancer dataset was analyzed to demonstrate the utility of the proposed estimators. A rule of thumb for adhoc selection among the three commonly used target matrices is recommended.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VSDD3AW2\1410.4726.pdf}
}

@article{triebe_arnet_2019,
  title = {{{AR-Net}}: {{A}} Simple {{Auto-Regressive Neural Network}} for Time-Series},
  shorttitle = {{{AR-Net}}},
  author = {Triebe, Oskar and Laptev, Nikolay and Rajagopal, Ram},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.12436 [cs, stat]},
  eprint = {1911.12436},
  primaryclass = {cs, stat},
  urldate = {2019-12-14},
  abstract = {In this paper we present a new framework for time-series modeling that combines the best of traditional statistical models and neural networks. We focus on time-series with long-range dependencies, needed for monitoring fine granularity data (e.g. minutes, seconds, milliseconds), prevalent in operational use-cases.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JYFXBZKG\Triebe et al. - 2019 - AR-Net A simple Auto-Regressive Neural Network fo.pdf}
}

@article{truong_selective_2020,
  title = {Selective Review of Offline Change Point Detection Methods},
  author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
  year = {2020},
  month = feb,
  journal = {Signal Processing},
  volume = {167},
  eprint = {1801.00718},
  pages = {107299},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2019.107299},
  urldate = {2020-05-12},
  abstract = {This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {{Computer Science - Computational Engineering, Finance, and Science},Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\9BBDLRDK\Truong et al. - 2020 - Selective review of offline change point detection.pdf}
}

@article{tuncel_autoregressive_2018,
  title = {Autoregressive Forests for Multivariate Time Series Modeling},
  author = {Tuncel, Kerem Sinan and Baydogan, Mustafa Gokce},
  year = {2018},
  month = jan,
  journal = {Pattern Recognition},
  volume = {73},
  pages = {202--215},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2017.08.016},
  urldate = {2019-09-05},
  abstract = {Multivariate Time Series (MTS) modeling has received significant attention in the last decade because of the complex nature of the data. Efficient representations are required to deal with the high dimensionality due to the increase in the number of variables and duration of the time series in different applications. For example, model-based approaches such as Hidden Markov Models (HMM) or autoregressive (AR) models focus on finding a model to represent the series with the model parameters to handle this problem. Both HMM and AR models are known to be very successful in the representation of the time series however most of the HMM approaches assume independence and traditional AR models consider linear dependence between the variables of MTS. As most of the real systems exhibit nonlinear relations, traditional approaches fail to represent the time series. To handle these problems, we propose an autoregressive tree-based ensemble approach that can model the nonlinear behavior embedded in the time series with the help of tree-based learning. Multivariate autoregressive forest, namely mv-ARF, is a nonparametric vector autoregression approach which provides an easy and efficient representation that scales well with large datasets. An error-based representation based on the learned models is the basis of the proposed approach. This is very similar to time series kernels used for multivariate time series classification problems. We test mv-ARF on MTS classification problems and show that mv-ARF provides fast and competitive results on benchmark datasets from several domains. Furthermore, mv-ARF provides a research direction for vector autoregressive models that breaks from the linear dependency models to potentially foster other promising nonlinear approaches.},
  keywords = {Classification,Ensemble learning,Multivariate time series,Time series representation,Vector autoregression},
  annotation = {00014},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WFL72YWR\\Tuncel and Baydogan - 2018 - Autoregressive forests for multivariate time serie.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\C7FQNUFB\\S0031320317303291.html}
}

@article{turiel_wisdom_2020,
  title = {Wisdom of the Crowds in Forecasting {{COVID-19}} Spreading Severity},
  author = {Turiel, Jeremy and Aste, Tomaso},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.04125 [physics]},
  eprint = {2004.04125},
  primaryclass = {physics},
  urldate = {2020-05-12},
  abstract = {In this work we report that the public reacted on social media at an early stage of the COVID-19 pandemic in a surprisingly accurate way, with activity levels reflecting the severity of the contagion figures registered almost a month later. Specifically, the intensity of COVID-related social media activity from different Italian regions at the beginning of the epidemic (21-24/2/2020), predicts well the total number of deaths reached almost a month later (7/4/2020) in each region. It should be noted that at the time of the initial twitter reaction no tabled regional data on the epidemic was readily available. By the 24th February 2020 only two regions reported death cases and only three reported infected subjects.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Social and Information Networks,Physics - Physics and Society},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\BNJTBKUC\Turiel and Aste - 2020 - Wisdom of the crowds in forecasting COVID-19 sprea.pdf}
}

@misc{tyralis_review_2022,
  title = {A Review of Probabilistic Forecasting and Prediction with Machine Learning},
  author = {Tyralis, Hristos and Papacharalampous, Georgia},
  year = {2022},
  month = sep,
  number = {arXiv:2209.08307},
  eprint = {2209.08307},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.08307},
  urldate = {2023-03-07},
  abstract = {Predictions and forecasts of machine learning models should take the form of probability distributions, aiming to increase the quantity of information communicated to end users. Although applications of probabilistic prediction and forecasting with machine learning models in academia and industry are becoming more frequent, related concepts and methods have not been formalized and structured under a holistic view of the entire field. Here, we review the topic of predictive uncertainty estimation with machine learning algorithms, as well as the related metrics (consistent scoring functions and proper scoring rules) for assessing probabilistic predictions. The review covers a time period spanning from the introduction of early statistical (linear regression and time series models, based on Bayesian statistics or quantile regression) to recent machine learning algorithms (including generalized additive models for location, scale and shape, random forests, boosting and deep learning algorithms) that are more flexible by nature. The review of the progress in the field, expedites our understanding on how to develop new algorithms tailored to users' needs, since the latest advancements are based on some fundamental concepts applied to more complex algorithms. We conclude by classifying the material and discussing challenges that are becoming a hot topic of research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EAKJ326D\\Tyralis and Papacharalampous - 2022 - A review of probabilistic forecasting and predicti.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\L2A6R7XN\\2209.html}
}

@article{vancalster_profitoriented_2020,
  title = {Profit-Oriented Sales Forecasting: A Comparison of Forecasting Techniques from a Business Perspective},
  shorttitle = {Profit-Oriented Sales Forecasting},
  author = {Van Calster, Tine and den Bossche, Filip Van and Baesens, Bart and Lemahieu, Wilfried},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.00949 [cs, econ, stat]},
  eprint = {2002.00949},
  primaryclass = {cs, econ, stat},
  urldate = {2020-02-17},
  abstract = {Choosing the technique that is the best at forecasting your data, is a problem that arises in any forecasting application. Decades of research have resulted into an enormous amount of forecasting methods that stem from statistics, econometrics and machine learning (ML), which leads to a very difficult and elaborate choice to make in any forecasting exercise. This paper aims to facilitate this process for high-level tactical sales forecasts by comparing a large array of techniques for 35 times series that consist of both industry data from the Coca-Cola Company and publicly available datasets. However, instead of solely focusing on the accuracy of the resulting forecasts, this paper introduces a novel and completely automated profit-driven approach that takes into account the expected profit that a technique can create during both the model building and evaluation process. The expected profit function that is used for this purpose, is easy to understand and adaptable to any situation by combining forecasting accuracy with business expertise. Furthermore, we examine the added value of ML techniques, the inclusion of external factors and the use of seasonal models in order to ascertain which type of model works best in tactical sales forecasting. Our findings show that simple seasonal time series models consistently outperform other methodologies and that the profit-driven approach can lead to selecting a different forecasting model.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VAWH6Z6U\Van Calster et al. - 2020 - Profit-oriented sales forecasting a comparison of.pdf}
}

@inproceedings{vanerven_gametheoretically_2015,
  title = {Game-{{Theoretically Optimal Reconciliation}} of {{Contemporaneous Hierarchical Time Series Forecasts}}},
  booktitle = {Modeling and {{Stochastic Learning}} for {{Forecasting}} in {{High Dimensions}}},
  author = {{van Erven}, Tim and Cugliari, Jairo},
  editor = {Antoniadis, Anestis and Poggi, Jean-Michel and Brossat, Xavier},
  year = {2015},
  series = {Lecture {{Notes}} in {{Statistics}}},
  pages = {297--317},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-18732-7_15},
  abstract = {In hierarchical time series (HTS) forecasting, the hierarchical relation between multiple time series is exploited to make better forecasts. This hierarchical relation implies one or more aggregate consistency constraints that the series are known to satisfy. Many existing approaches, like for example bottom-up or top-down forecasting, therefore attempt to achieve this goal in a way that guarantees that the forecasts will also be aggregate consistent. We propose to split the problem of HTS into two independent steps: first one comes up with the best possible forecasts for the time series without worrying about aggregate consistency; and then a reconciliation procedure is used to make the forecasts aggregate consistent. We introduce a Game-Theoretically OPtimal (GTOP) reconciliation method, which is guaranteed to only improve any given set of forecasts. This opens up new possibilities for constructing the forecasts. For example, it is not necessary to assume that bottom-level forecasts are unbiased, and aggregate forecasts may be constructed by regressing both on bottom-level forecasts and on other covariates that may only be available at the aggregate level. We illustrate the benefits of our approach both on simulated data and on real electricity consumption data.},
  isbn = {978-3-319-18732-7},
  langid = {english},
  keywords = {Bregman Divergence,Bregman Projection,Initial Forecast,Prediction Interval,Regional Forecast},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\4D4GDGXK\van Erven and Cugliari - 2015 - Game-Theoretically Optimal Reconciliation of Conte.pdf}
}

@article{varol_wasserstein_2019,
  title = {Wasserstein Total Variation Filtering},
  author = {Varol, Erdem and Nejatbakhsh, Amin},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.10822 [cs, eess, stat]},
  eprint = {1910.10822},
  primaryclass = {cs, eess, stat},
  urldate = {2019-10-28},
  abstract = {In this paper, we expand upon the theory of trend filtering by introducing the use of the Wasserstein metric as a means to control the amount of spatiotemporal variation in filtered time series data. While trend filtering utilizes regularization to produce signal estimates that are piecewise linear, in the case of \$\textbackslash ell\_1\$ regularization, or temporally smooth, in the case of \$\textbackslash ell\_2\$ regularization, it ignores the topology of the spatial distribution of signal. By incorporating the information about the underlying metric space of the pixel layout, the Wasserstein metric is an attractive choice as a regularizer to undercover spatiotemporal trends in time series data. We introduce a globally optimal algorithm for efficiently estimating the filtered signal under a Wasserstein finite differences operator. The efficacy of the proposed algorithm in preserving spatiotemporal trends in time series video is demonstrated in both simulated and fluorescent microscopy videos of the nematode caenorhabditis elegans and compared against standard trend filtering algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing,Statistics - Applications},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZMBKEZJD\\Varol and Nejatbakhsh - 2019 - Wasserstein total variation filtering.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\BMR5FXI5\\1910.html}
}

@incollection{vaswani_attention_2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  pages = {5998--6008},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-01-17},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NKQIKJGX\\Vaswani et al. - 2017 - Attention is All you Need.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FY36J6JB\\7181-attention-is-all-you-need.html}
}

@article{vecoven_master_,
  title = {Master Thesis : {{Feature}} Selection with Deep Neural Networks},
  author = {Vecoven, N},
  pages = {77},
  langid = {english},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\TCTK4GAK\Vecoven - Master thesis  Feature selection with deep neural.pdf}
}

@article{villegas_supply_2018,
  title = {Supply Chain Decision Support Systems Based on a Novel Hierarchical Forecasting Approach},
  author = {Villegas, Marco A. and Pedregal, Diego J.},
  year = {2018},
  month = oct,
  journal = {Decision Support Systems},
  volume = {114},
  pages = {29--36},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2018.08.003},
  urldate = {2021-02-04},
  abstract = {Time series forecasting plays an important role in many decision support systems, also in those related to the management of supply chains. Forecast accuracy is, therefore, essential to optimise the efficiency of any supply chain. One aspect that is often overlooked is the fact that sales of many products within an organization are assembled as complex hierarchies with different levels of aggregation. Very often forecasts are produced regardless of such structure, though forecasting accuracy may be improved by taking it into account. In this paper, an approach for hierarchical time series forecasting based on State Space modelling is proposed. Previous developments provide solutions to the hierarchical forecasting problem by algebra manipulations based on forecasts produced by independent models for each time series involved in the hierarchy. The solutions produce optimal reconciled forecasts for each individual forecast horizon, but the link along time that is implied by the dynamics of the models is completely ignored. Therefore, the novel approach in this paper improves upon past research at least in two key points. Firstly, the algebra is already encoded in the State Space system and the Kalman Filter algorithm, giving an elegant and clean solution to the problem. Secondly, the State Space approach is optimal both across the hierarchy, as expected, but also along time, something missing in past developments. The approach is assessed by comparing its forecasting performance to the existing methods, through simulations and using real data of a Spanish grocery retailer.},
  langid = {english},
  keywords = {Decision support system,Forecasting,Hierarchical forecasting,Reconciliation,State Space},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3X9F6AUL\Villegas and Pedregal - 2018 - Supply chain decision support systems based on a n.pdf}
}

@article{villegasgarcia_unified_2018,
  title = {A Unified Approach for Hierarchical Time Series Forecasting},
  author = {Villegas Garc{\'i}a, Marco Antonio and Pedregal Tercero, Diego Jos{\'e}},
  year = {2018},
  publisher = {{Elsevier}},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2018.08.003},
  urldate = {2021-02-04},
  abstract = {In this paper an approach for hierarchical time series forecasting based on State Space modelling is proposed. Previous developments provide solutions to the hierarchical forecasting problem by algebra manipulations based on forecasts produced by independent models for each time series involved in the hierarchy. The solutions produce optimal reconciled forecasts for each individual forecast horizon, but the link along time that is implied by the dynamics of the models is completely ignored. Therefore, the novel approach in this paper improves upon past research at least in two key points. Firstly, the algebra is already encoded in the State Space system and the Kalman Filter algorithm, giving an elegant and clean solution to the problem. Secondly, the State Space approach is optimal both across the hierarchy, as expected, but also along time, something missing in past developments. In addition, the present approach provides an unified treatment of top-down, bottom-up, middle-out and reconciled approaches reported in the literature; it generalizes the optimization of hierarchies by proposing combined hierarchies which integrate the previous categories at different segments of the hierarchy; and it allows for multiple hierarchies to be simultaneously adjusted. The approach is assessed by comparing its forecasting performance to the existing methods, through simulations and using real data of a Spanish grocery retailer.},
  copyright = {info:eu-repo/semantics/openAccess},
  langid = {english},
  annotation = {Accepted: 2019-04-10T07:56:50Z},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\22LIKKI8\Villegas GarcÃ­a and Pedregal Tercero - 2018 - A unified approach for hierarchical time series fo.pdf}
}

@article{vinayavekhin_focusing_2018,
  title = {Focusing on {{What}} Is {{Relevant}}: {{Time-Series Learning}} and {{Understanding}} Using {{Attention}}},
  shorttitle = {Focusing on {{What}} Is {{Relevant}}},
  author = {Vinayavekhin, Phongtharin and Chaudhury, Subhajit and Munawar, Asim and Agravante, Don Joven and De Magistris, Giovanni and Kimura, Daiki and Tachibana, Ryuki},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.08523 [cs]},
  eprint = {1806.08523},
  primaryclass = {cs},
  urldate = {2019-04-11},
  abstract = {This paper is a contribution towards interpretability of the deep learning models in different applications of timeseries. We propose a temporal attention layer that is capable of selecting the relevant information to perform various tasks, including data completion, key-frame detection and classification. The method uses the whole input sequence to calculate an attention value for each time step. This results in more focused attention values and more plausible visualisation than previous methods. We apply the proposed method to three different tasks. Experimental results show that the proposed network produces comparable results to a state of the art. In addition, the network provides better interpretability of the decision, that is, it generates more significant attention weight to related frames compared to similar techniques attempted in the past.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read - not in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\SG98L7C3\Vinayavekhin et al. - 2018 - Focusing on What is Relevant Time-Series Learning.pdf}
}

@article{vinyals_matching_2016,
  title = {Matching {{Networks}} for {{One Shot Learning}}},
  author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.04080 [cs, stat]},
  eprint = {1606.04080},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6\% to 93.2\% and from 88.0\% to 93.8\% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RYV49GUN\Vinyals et al. - 2016 - Matching Networks for One Shot Learning.pdf}
}

@article{vinyals_pointer_,
  title = {Pointer {{Networks}}},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  pages = {9},
  abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence [1] and Neural Turing Machines [2], because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems \textendash{} finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem \textendash{} using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YY5NPIGG\Vinyals et al. - Pointer Networks.pdf}
}

@article{virtanen_scipy_2020,
  title = {{{SciPy}} 1.0: Fundamental Algorithms for Scientific Computing in {{Python}}},
  shorttitle = {{{SciPy}} 1.0},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and Van Der Walt, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and Van Mulbregt, Paul and {SciPy 1.0 Contributors} and Vijaykumar, Aditya and Bardelli, Alessandro Pietro and Rothberg, Alex and Hilboll, Andreas and Kloeckner, Andreas and Scopatz, Anthony and Lee, Antony and Rokem, Ariel and Woods, C. Nathan and Fulton, Chad and Masson, Charles and H{\"a}ggstr{\"o}m, Christian and Fitzgerald, Clark and Nicholson, David A. and Hagen, David R. and Pasechnik, Dmitrii V. and Olivetti, Emanuele and Martin, Eric and Wieser, Eric and Silva, Fabrice and Lenders, Felix and Wilhelm, Florian and Young, G. and Price, Gavin A. and Ingold, Gert-Ludwig and Allen, Gregory E. and Lee, Gregory R. and Audren, Herv{\'e} and Probst, Irvin and Dietrich, J{\"o}rg P. and Silterra, Jacob and Webber, James T and Slavi{\v c}, Janko and Nothman, Joel and Buchner, Johannes and Kulick, Johannes and Sch{\"o}nberger, Johannes L. and De Miranda Cardoso, Jos{\'e} Vin{\'i}cius and Reimer, Joscha and Harrington, Joseph and Rodr{\'i}guez, Juan Luis Cano and {Nunez-Iglesias}, Juan and Kuczynski, Justin and Tritz, Kevin and Thoma, Martin and Newville, Matthew and K{\"u}mmerer, Matthias and Bolingbroke, Maximilian and Tartre, Michael and Pak, Mikhail and Smith, Nathaniel J. and Nowaczyk, Nikolai and Shebanov, Nikolay and Pavlyk, Oleksandr and Brodtkorb, Per A. and Lee, Perry and McGibbon, Robert T. and Feldbauer, Roman and Lewis, Sam and Tygier, Sam and Sievert, Scott and Vigna, Sebastiano and Peterson, Stefan and More, Surhud and Pudlik, Tadeusz and Oshima, Takuya and Pingel, Thomas J. and Robitaille, Thomas P. and Spura, Thomas and Jones, Thouis R. and Cera, Tim and Leslie, Tim and Zito, Tiziano and Krauss, Tom and Upadhyay, Utkarsh and Halchenko, Yaroslav O. and {V{\'a}zquez-Baeza}, Yoshiki},
  year = {2020},
  month = mar,
  journal = {Nature Methods},
  volume = {17},
  number = {3},
  pages = {261--272},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-019-0686-2},
  urldate = {2023-06-06},
  abstract = {Abstract             SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\9JBQQIJG\Virtanen et al. - 2020 - SciPy 1.0 fundamental algorithms for scientific c.pdf}
}

@article{volkovs_dropoutnet_,
  title = {{{DropoutNet}}: {{Addressing Cold Start}} in {{Recommender Systems}}},
  author = {Volkovs, Maksims and Yu, Guangwei and Poutanen, Tomi},
  pages = {10},
  abstract = {Latent models have become the default choice for recommender systems due to their performance and scalability. However, research in this area has primarily focused on modeling user-item interactions, and few latent models have been developed for cold start. Deep learning has recently achieved remarkable success showing excellent results for diverse input types. Inspired by these results we propose a neural network based latent model called DropoutNet to address the cold start problem in recommender systems. Unlike existing approaches that incorporate additional content-based objective terms, we instead focus on the optimization and show that neural network models can be explicitly trained for cold start through dropout. Our model can be applied on top of any existing latent model effectively providing cold start capabilities, and full power of deep architectures. Empirically we demonstrate state-of-the-art accuracy on publicly available benchmarks. Code is available at https://github.com/layer6ai-labs/DropoutNet.},
  langid = {english},
  annotation = {00027},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YBFSIWUK\Volkovs et al. - DropoutNet Addressing Cold Start in Recommender S.pdf}
}

@article{volodina_importance_,
  title = {The Importance of Uncertainty Quantification in Model Reproducibility},
  author = {Volodina, Victoria and Challenor, Peter},
  journal = {Philosophical transactions. Series A, Mathematical, physical, and engineering sciences},
  volume = {379},
  number = {2197},
  pages = {20200071},
  issn = {1364-503X},
  doi = {10.1098/rsta.2020.0071},
  urldate = {2023-03-06},
  abstract = {Many computer models possess high-dimensional input spaces and substantial computational time to produce a single model evaluation. Although such models are often `deterministic', these models suffer from a wide range of uncertainties. We argue that uncertainty quantification is crucial for computer model validation and reproducibility. We present a statistical framework, termed history matching, for performing global parameter search by comparing model output to the observed data. We employ Gaussian process (GP) emulators to produce fast predictions about model behaviour at the arbitrary input parameter settings allowing output uncertainty distributions to be calculated. History matching identifies sets of input parameters that give rise to acceptable matches between observed data and model output given our representation of uncertainties. Modellers could proceed by simulating computer models' outputs of interest at these identified parameter settings and producing a range of predictions. The variability in model results is crucial for inter-model comparison as well as model development. We illustrate the performance of emulation and history matching on a simple one-dimensional toy model and in application to a climate model., This article is part of the theme issue `Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico'.},
  pmcid = {PMC8059558},
  pmid = {33775141},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\X26RRGBG\Volodina and Challenor - The importance of uncertainty quantification in mo.pdf}
}

@article{waldmann_bayesian_2013,
  title = {Bayesian {{Geoadditive Expectile Regression}}},
  author = {Waldmann, Elisabeth and Sobotka, Fabian and Kneib, Thomas},
  year = {2013},
  month = dec,
  journal = {arXiv:1312.5054 [stat]},
  eprint = {1312.5054},
  primaryclass = {stat},
  urldate = {2021-10-13},
  abstract = {Regression classes modeling more than the mean of the response have found a lot of attention in the last years. Expectile regression is a special and computationally convenient case of this family of models. Expectiles offer a quantile-like characterisation of a complete distribution and include the mean as a special case. In the frequentist framework the impact of a lot of covariates with very different structures have been made possible. We propose Bayesian expectile regression based on the asymmetric normal distribution. This renders possible incorporating for example linear, nonlinear, spatial and random effects in one model. Furthermore a detailed inference on the estimated parameters can be conducted. Proposal densities based on iterativly weighted least squares updates for the resulting Markov chain Monte Carlo (MCMC) simulation algorithm are proposed and the potential of the approach for extending the flexibility of expectile regression towards complex semiparametric regression specifications is discussed.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\M37VMVE5\Waldmann et al. - 2013 - Bayesian Geoadditive Expectile Regression.pdf}
}

@inproceedings{wan_dual_1997,
  title = {Dual {{Kalman Filtering Methods}} for {{Nonlinear Prediction}}, {{Smoothing}}, and {{Estimation}}},
  booktitle = {In {{Advances}} in {{Neural Information Processing Systems}} 9},
  author = {Wan, Eric A. and Nelson, Alex T.},
  year = {1997},
  abstract = {Prediction, estimation, and smoothing are fundamental to signal processing. To perform these interrelated tasks given noisy data, we form a time series model of the process that generates the data. Taking noise in the system explicitly into account, maximumlikelihood and Kalman frameworks are discussed which involve the dual process of estimating both the model parameters and the underlying state of the system. We review several established methods in the linear case, and propose several extensions utilizing dual Kalman filters (DKF) and forward-backward (FB) filters that are applicable to neural networks. Methods are compared on several simulations of noisy time series. We also include an example of nonlinear noise reduction in speech. 1 INTRODUCTION  Consider the general autoregressive model of a noisy time series with both process and additive observation noise:  x(k) = f(x(k \textbackslash Gamma 1); :::x(k \textbackslash Gamma M ); w) + v(k \textbackslash Gamma 1) (1)  y(k) = x(k) + r(k); (2) where x(k) corresponds to the ...},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6IQIN574\\Wan and Nelson - 1997 - Dual Kalman Filtering Methods for Nonlinear Predic.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5Q5WYES5\\summary.html}
}

@article{wang_attentional_,
  title = {Attentional {{Neural Network}}: {{Feature Selection Using Cognitive Feedback}}},
  author = {Wang, Qian and Zhang, Jiaxing and Song, Sen and Zhang, Zheng},
  pages = {9},
  abstract = {Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PRJLEQM2\Wang et al. - Attentional Neural Network Feature Selection Usin.pdf}
}

@article{wang_copulasbased_2012,
  title = {A {{Copulas-Based Approach}} to {{Modeling Dependence}} in {{Decision Trees}}},
  author = {Wang, Tianyang and Dyer, James S.},
  year = {2012},
  month = feb,
  journal = {Operations Research},
  volume = {60},
  number = {1},
  pages = {225--242},
  issn = {0030-364X},
  doi = {10.1287/opre.1110.1004},
  urldate = {2020-02-24},
  abstract = {This paper presents a general framework based on copulas for modeling dependent multivariate uncertainties through the use of a decision tree. The proposed dependent decision tree model allows multiple dependent uncertainties with arbitrary marginal distributions to be represented in a decision tree with a sequence of conditional probability distributions. This general framework could be naturally applied in decision analysis and real options valuations, as well as in more general applications of dependent probability trees. While this approach to modeling dependencies can be based on several popular copula families as we illustrate, we focus on the use of the normal copula and present an efficient computational method for multivariate decision and risk analysis that can be standardized for convenient application.},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ATS6FXLV\opre.1110.html}
}

@article{wang_deep_2019,
  title = {Deep {{Factors}} for {{Forecasting}}},
  author = {Wang, Yuyang and Smola, Alex and Maddix, Danielle C. and Gasthaus, Jan and Foster, Dean and Januschowski, Tim},
  year = {2019},
  month = may,
  journal = {arXiv:1905.12417 [cs, stat]},
  eprint = {1905.12417},
  primaryclass = {cs, stat},
  urldate = {2020-01-27},
  abstract = {Producing probabilistic forecasts for large collections of similar and/or dependent time series is a practically relevant and challenging task. Classical time series models fail to capture complex patterns in the data, and multivariate techniques struggle to scale to large problem sizes. Their reliance on strong structural assumptions makes them data-efficient, and allows them to provide uncertainty estimates. The converse is true for models based on deep neural networks, which can learn complex patterns and dependencies given enough data. In this paper, we propose a hybrid model that incorporates the benefits of both approaches. Our new method is data-driven and scalable via a latent, global, deep component. It also handles uncertainty through a local classical model. We provide both theoretical and empirical evidence for the soundness of our approach through a necessary and sufficient decomposition of exchangeable time series into a global and a local part. Our experiments demonstrate the advantages of our model both in term of data efficiency, accuracy and computational complexity.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6SGBSD6F\Wang et al. - 2019 - Deep Factors for Forecasting.pdf}
}

@article{wang_dueling_2015,
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and {van Hasselt}, Hado and Lanctot, Marc and {de Freitas}, Nando},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.06581 [cs]},
  eprint = {1511.06581},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VEU253AU\Wang et al. - 2015 - Dueling Network Architectures for Deep Reinforceme.pdf}
}

@article{wang_experimental_2013,
  title = {Experimental Comparison of Representation Methods and Distance Measures for Time Series Data},
  author = {Wang, Xiaoyue and Mueen, Abdullah and Ding, Hui and Trajcevski, Goce and Scheuermann, Peter and Keogh, Eamonn},
  year = {2013},
  month = mar,
  journal = {Data Mining and Knowledge Discovery},
  volume = {26},
  number = {2},
  pages = {275--309},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-012-0250-5},
  urldate = {2019-04-24},
  langid = {english},
  annotation = {00521},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\CYFNGKFI\Wang et al. - 2013 - Experimental comparison of representation methods .pdf}
}

@misc{wang_forecast_2022,
  title = {Forecast Combinations: An over 50-Year Review},
  shorttitle = {Forecast Combinations},
  author = {Wang, Xiaoqian and Hyndman, Rob J. and Li, Feng and Kang, Yanfei},
  year = {2022},
  month = may,
  number = {arXiv:2205.04216},
  eprint = {2205.04216},
  primaryclass = {stat},
  publisher = {{arXiv}},
  urldate = {2022-05-24},
  abstract = {Forecast combinations have flourished remarkably in the forecasting community and, in recent years, have become part of the mainstream of forecasting research and activities. Combining multiple forecasts produced from the single (target) series is now widely used to improve accuracy through the integration of information gleaned from different sources, thereby mitigating the risk of identifying a single ``best'' forecast. Combination schemes have evolved from simple combination methods without estimation, to sophisticated methods involving time-varying weights, nonlinear combinations, correlations among components, and cross-learning. They include combining point forecasts, and combining probabilistic forecasts. This paper provides an up-to-date review of the extensive literature on forecast combinations, together with reference to available open-source software implementations. We discuss the potential and limitations of various methods and highlight how these ideas have developed over time. Some important issues concerning the utility of forecast combinations are also surveyed. Finally, we conclude with current research gaps and potential insights for future research.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5KKJFIIZ\Wang et al. - 2022 - Forecast combinations an over 50-year review.pdf}
}

@inproceedings{wang_market_2018,
  title = {Market {{Abnormality Period Detection}} via {{Co-movement Attention Model}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Wang, Y. and Zhang, C. and Wang, S. and Yu, P. S. and Bai, L. and Cui, L.},
  year = {2018},
  month = dec,
  pages = {1514--1523},
  doi = {10.1109/BigData.2018.8621877},
  abstract = {The financial contagion describes a widespread phenomenon of the interdependency for pairs of stock time series during the market abnormality periods. Since the interdependency rule between stocks varies in different periods, it is difficult to capture the interdependency rule for stocks related to the market status effectively. We define this interdependency rule as, the co-movement pattern, a high-dimensional co-varying relationship between pairs of stock time series and propose a Co-movement Attention Model (CAM) to discover the co-movement patterns for the stocks related to the market status. With the discovered patterns, CAM focuses on the stock-level abnormality periods by the co-movement attention automatically. CAM is trained under the supervision of the stock sector label information. CAM has the ability to model financial contagion and detect global market abnormality periods, by modeling co-movement patterns on every pair-wise stocks. We verify our methods on the real-world stock data and compare it with state of the art methods. The experimental result shows that our method not only captures the co-movement attentions with better quantitative metric values but also covers more real market abnormalities than the other alternatives.},
  keywords = {Big Data,CAM,co-movement attention,co-movement attention model,co-movement pattern,Computational modeling,Computer science,Data models,Economics,financial contagion,Financial data mining,global market abnormality periods,high-dimensional co-varying relationship,interdependency rule,market abnormality period detection,market opportunities,market status,market-level abnormality,Microsoft Windows,pattern recognition,read - not in related work,real-world stock data,stock markets,stock sector label information,stock time series,stock-level abnormality,stock-level abnormality periods,time series,Time series analysis},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FHHGPJL2\8621877.html}
}

@inproceedings{wang_multilevel_2018,
  title = {Multilevel {{Wavelet Decomposition Network}} for {{Interpretable Time Series Analysis}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Wang, Jingyuan and Wang, Ze and Li, Jianfeng and Wu, Junjie},
  year = {2018},
  series = {{{KDD}} '18},
  pages = {2437--2446},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3219819.3220060},
  urldate = {2019-04-16},
  abstract = {Recent years have witnessed the unprecedented rising of time series from almost all kindes of academic and industrial fields. Various types of deep neural network models have been introduced to time series analysis, but the important frequency information is yet lack of effective modeling. In light of this, in this paper we propose a wavelet-based neural network structure called multilevel Wavelet Decomposition Network (mWDN) for building frequency-aware deep learning models for time series analysis. mWDN preserves the advantage of multilevel discrete wavelet decomposition in frequency learning while enables the fine-tuning of all parameters under a deep neural network framework. Based on mWDN, we further propose two deep learning models called Residual Classification Flow (RCF) and multi-frequecy Long Short-Term Memory (mLSTM) for time series classification and forecasting, respectively. The two models take all or partial mWDN decomposed sub-series in different frequencies as input, and resort to the back propagation algorithm to learn all the parameters globally, which enables seamless embedding of wavelet-based frequency analysis into deep learning frameworks. Extensive experiments on 40 UCR datasets and a real-world user volume dataset demonstrate the excellent performance of our time series models based on mWDN. In particular, we propose an importance analysis method to mWDN based models, which successfully identifies those time-series elements and mWDN layers that are crucially important to time series analysis. This indeed indicates the interpretability advantage of mWDN, and can be viewed as an indepth exploration to interpretable deep learning.},
  isbn = {978-1-4503-5552-0},
  keywords = {epidemic propagation,intracity epidemic control and prevention,metapopulation,network inference,read - in related work},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\IS5WLSLE\Wang et al. - 2018 - Multilevel Wavelet Decomposition Network for Inter.pdf}
}

@article{wang_que_2020,
  title = {Que Ser\textbackslash 'a Ser\textbackslash 'a? {{The}} Uncertainty Estimation of Feature-Based Time Series Forecasts},
  shorttitle = {Que Ser\textbackslash 'a Ser\textbackslash 'a?},
  author = {Wang, Xiaoqian and Kang, Yanfei and Petropoulos, Fotios and Li, Feng},
  year = {2020},
  month = jan,
  journal = {arXiv:1908.02891 [stat]},
  eprint = {1908.02891},
  primaryclass = {stat},
  urldate = {2020-02-19},
  abstract = {Interval forecasts have significant advantages in accounting for the uncertainty estimation of point forecasts, highlighting the importance of providing prediction intervals (PIs) as well as point forecasts in forecasting activities. In this paper, a general feature-based framework is outlined to examine the relationship between time series features and the interval forecasting accuracy and to provide reliable forecasts as well as their uncertainty estimation. Specifically, the framework is divided into training and testing phases. In the training part, we use a collection of time series to train a model to explore how time series features affect the interval forecasting accuracy of different forecasting methods, which makes our proposed framework interpretable in terms of the contribution of each feature to the models' uncertainty prediction. The effect analysis is further applied to assign weights to various benchmark methods with the purpose of reducing the uncertainty of the forecasts. In the testing part, we calculate the point forecasts and PIs of new series using the trained model and weight determination process obtained in the training phase. We illustrate that, whether in point or interval forecasts, our feature-based forecasting framework outperforms all individual benchmark methods and their simple equally weighted combination for different confidence levels on the M3 competition data with an improved computational efficiency.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\5PP57G3B\\Wang et al. - 2020 - Que ser'a ser'a The uncertainty estimation of f.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Q2UV5CAQ\\1908.html}
}

@inproceedings{wang_sparse_2018,
  title = {Sparse {{Gaussian Conditional Random Fields}} on {{Top}} of {{Recurrent Neural Networks}}},
  booktitle = {{{AAAI}}},
  author = {Wang, Xishun and Zhang, Minjie and Ren, Fenghui},
  year = {2018},
  abstract = {Predictions of time-series are widely used in different disciplines. We propose CoR, Sparse Gaussian Conditional Random Fields (SGCRF) on top of Recurrent Neural Networks (RNN), for problems of this kind. CoR gains advantages from both RNN and SGCRF. It can not only effectively represent the temporal correlations in observed data, but can also learn the structured information of the output. CoR is challenging to train because it is a hybrid of deep neural networks and densely-connected graphical models. Alternative training can be a tractable way to train CoR, and furthermore, an end-toend training method is proposed to train CoR more efficiently. CoR is evaluated by both synthetic data and real-world data, and it shows a significant improvement in performance over state-of-the-art methods.},
  keywords = {Artificial neural network,Cobham's thesis,Conditional random field,Deep learning,End-to-end principle,Graphical model,Graphical user interface,Neural Networks,Nonlinear system,Random neural network,read - in related work,Recurrent neural network,Sparse,Synthetic data,Teaching method,Time series},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5AD2P8PE\Wang et al. - 2018 - Sparse Gaussian Conditional Random Fields on Top o.pdf}
}

@article{wee_adaptive_2019,
  title = {Adaptive Load Forecasting Using Reinforcement Learning with Database Technology},
  author = {Wee, Chee Keong and Nayak, Richi},
  year = {2019},
  month = jul,
  journal = {Journal of Information and Telecommunication},
  volume = {3},
  number = {3},
  pages = {381--399},
  issn = {2475-1839},
  doi = {10.1080/24751839.2019.1596470},
  urldate = {2020-02-17},
  abstract = {Load forecasting is an essential operation in the power utility industry. However, a common challenge is faced for adjusting forecasting models to fit the need for substations' load prediction as well as minimizing expenditure in IT resources for repurposing these forecasting models to bigger datasets. The goal of this paper is to propose a novel solution that is responsive to these demands through the integration of reinforcement learning with load forecasting on existing database technology. To deal with the varying accuracy of the forecasting models on different substations' loads, the proposed solution compares and uses the best models and recalibrate them iteratively by comparing the model's prediction against the actual load data. As shown in empirical analysis, the solution interacts with the environment and performs the optimum forecasting routine intuitively.},
  keywords = {database technology,electricity utility,Forecasting,reinforcement learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\C3FEAVMZ\\Wee and Nayak - 2019 - Adaptive load forecasting using reinforcement lear.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\29YWNXMJ\\24751839.2019.html}
}

@article{wehenkel_unconstrained_,
  title = {Unconstrained {{Monotonic Neural Networks}}},
  author = {Wehenkel, Antoine and Louppe, Gilles},
  pages = {3},
  abstract = {Monotonic neural networks have recently been proposed as a way to define invertible transformations. These transformations can be combined into powerful autoregressive flows that have been shown to be universal approximators of continuous probability distributions. Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations. In this work, we propose the Unconstrained Monotonic Neural Network (UMNN) architecture based on the insight that a function is monotonic as long as its derivative is strictly positive. In particular, this latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output. We evaluate our new invertible building block within a new autoregressive flow (UMNN-MAF) and demonstrate its e{$\carriagereturn$}ectiveness on density estimation experiments. We also illustrate the ability of UMNNs to improve variational inference.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\NAESFSVM\Wehenkel and Louppe - Unconstrained Monotonic Neural Networks.pdf}
}

@article{wei_network_2016,
  title = {Network {{Morphism}}},
  author = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
  year = {2016},
  month = mar,
  journal = {arXiv:1603.01670 [cs]},
  eprint = {1603.01670},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with nonlinearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6J62XQDF\Wei et al. - 2016 - Network Morphism.pdf}
}

@article{wei_rpc_2019,
  title = {{{RPC}}: {{A Large-Scale Retail Product Checkout Dataset}}},
  shorttitle = {{{RPC}}},
  author = {Wei, Xiu-Shen and Cui, Quan and Yang, Lei and Wang, Peng and Liu, Lingqiao},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.07249 [cs]},
  eprint = {1901.07249},
  primaryclass = {cs},
  urldate = {2020-05-25},
  abstract = {Over recent years, emerging interest has occurred in integrating computer vision technology into the retail industry. Automatic checkout (ACO) is one of the critical problems in this area which aims to automatically generate the shopping list from the images of the products to purchase. The main challenge of this problem comes from the large scale and the fine-grained nature of the product categories as well as the difficulty for collecting training images that reflect the realistic checkout scenarios due to continuous update of the products. Despite its significant practical and research value, this problem is not extensively studied in the computer vision community, largely due to the lack of a high-quality dataset. To fill this gap, in this work we propose a new dataset to facilitate relevant research. Our dataset enjoys the following characteristics: (1) It is by far the largest dataset in terms of both product image quantity and product categories. (2) It includes single-product images taken in a controlled environment and multi-product images taken by the checkout system. (3) It provides different levels of annotations for the check-out images. Comparing with the existing datasets, ours is closer to the realistic setting and can derive a variety of research problems. Besides the dataset, we also benchmark the performance on this dataset with various approaches. The dataset and related resources can be found at https://rpc-dataset.github.io/.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\5CXEJFHG\Wei et al. - 2019 - RPC A Large-Scale Retail Product Checkout Dataset.pdf}
}

@phdthesis{weiss_essays_2018,
  type = {Thesis},
  title = {Essays in {{Hierarchical Time Series Forecasting}} and {{Forecast Combination}}},
  author = {Weiss, Christoph},
  year = {2018},
  month = may,
  doi = {10.17863/CAM.21895},
  urldate = {2021-02-04},
  abstract = {This dissertation comprises of three original contributions to empirical forecasting research. Chapter 1 introduces the dissertation.  Chapter 2 contributes to the literature on hierarchical time series (HTS) modelling by proposing a disaggregated forecasting system for both inflation rate and its volatility. Using monthly data that underlies the Retail Prices Index for the UK, we analyse the dynamics of the inflation process. We examine patterns in the time-varying covariation among product-level inflation rates that aggregate up to industry-level inflation rates that in turn aggregate up to the overall inflation rate. The aggregate inflation volatility closely tracks the time path of this covariation, which is seen to be driven primarily by the variances of common shocks shared by all products, and by the covariances between idiosyncratic product-level shocks. We formulate a forecasting system that comprises of models for mean inflation rate and its variance, and exploit the index structure of the aggregate inflation rate using the HTS framework. Using a dynamic model selection approach to forecasting, we obtain forecasts that are between 9 and 155 \% more accurate than a SARIMA-GARCH(1,1) for the aggregate inflation volatility. Chapter 3 is on improving forecasts using forecast combinations. The paper documents the software implementation of the open source R package for forecast combination that we coded and published on the official R package depository, CRAN. The GeomComb package is the only R package that covers a wide range of different popular forecast combination methods. We implement techniques from 3 broad categories: (a) simple non-parametric methods, (b) regression-based methods, and (c) geometric (eigenvector) methods, allowing for static or dynamic estimation of each approach. Using S3 classes/methods in R, the package provides a user-friendly environment for applied forecasting, implementing solutions for typical issues related to forecast combination (multicollinearity, missing values, etc.), criterion-based optimisation for several parametric methods, and post-fit functions to rationalise and visualise estimation results. The package has been listed in the official R Task Views for Time Series Analysis and for Official Statistics. The brief empirical application in the paper illustrates the package's functionality by estimating forecast combination techniques for monthly UK electricity supply. Chapter 4 introduces HTS forecasting and forecast combination to a healthcare staffing context. A slowdown of healthcare budget growth in the UK that does not keep pace with growth of demand for hospital services made efficient cost planning increasingly crucial for hospitals, in particular for staff which accounts for more than half of hospitals' expenses. This is facilitated by accurate forecasts of patient census and churn. Using a dataset of more than 3 million observations from a large UK hospital, we show how HTS forecasting can improve forecast accuracy by using information at different levels of the hospital hierarchy (aggregate, emergency/electives, divisions, specialties), compared to the na\"ive benchmark: the seasonal random walk model applied to the aggregate. We show that forecast combination can improve accuracy even more in some cases, and leads to lower forecast error variance (decreasing forecasting risk). We propose a comprehensive parametric approach to use forecasts in a nurse staffing model that has the aim of minimising cost while satisfying that the care requirements (e.g. nurse hours per patient day thresholds) are met},
  copyright = {All rights reserved},
  langid = {english},
  school = {University of Cambridge},
  annotation = {Accepted: 2018-04-11T10:14:43Z},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\IKTYV85S\\Weiss - 2018 - Essays in Hierarchical Time Series Forecasting and.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9R5C746L\\274757.html}
}

@article{wen_deep_2019,
  title = {Deep {{Generative Quantile-Copula Models}} for {{Probabilistic Forecasting}}},
  author = {Wen, Ruofeng and Torkkola, Kari},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.10697 [cs, stat]},
  eprint = {1907.10697},
  primaryclass = {cs, stat},
  urldate = {2019-12-14},
  abstract = {We introduce a new category of multivariate conditional generative models and demonstrate its performance and versatility in probabilistic time series forecasting and simulation. Specifically, the output of quantile regression networks is expanded from a set of fixed quantiles to the whole Quantile Function by a univariate mapping from a latent uniform distribution to the target distribution. Then the multivariate case is solved by learning such quantile functions for each dimension's marginal distribution, followed by estimating a conditional Copula to associate these latent uniform random variables. The quantile functions and copula, together defining the joint predictive distribution, can be parameterized by a single implicit generative Deep Neural Network.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FTR4UYHD\Wen and Torkkola - 2019 - Deep Generative Quantile-Copula Models for Probabi.pdf}
}

@article{wen_multihorizon_2018,
  title = {A {{Multi-Horizon Quantile Recurrent Forecaster}}},
  author = {Wen, Ruofeng and Torkkola, Kari and Narayanaswamy, Balakrishnan and Madeka, Dhruv},
  year = {2018},
  month = jun,
  journal = {31st Conference on Neural Information Processing Systems (NIPS2017), Time Series Workshop. Long Beach, CA, USA.},
  urldate = {2020-01-10},
  abstract = {We propose a framework for general probabilistic multi-step time series regression. Specifically, we exploit the expressiveness and temporal nature of Sequence-to-Sequence Neural Networks (e.g. recurrent and convolutional structures), the nonparametric nature of Quantile Regression and the efficiency of Direct Multi-Horizon Forecasting. A new training scheme, forking-sequences, is designed for sequential nets to boost stability and performance. We show that the approach accommodates both temporal and static covariates, learning across multiple related series, shifting seasonality, future planned event spikes and coldstarts in real life large-scale forecasting. The performance of the framework is demonstrated in an application to predict the future demand of items sold on Amazon.com, and in a public probabilistic forecasting competition to predict electricity price and load.},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\KYIRDMAN\Wen et al. - 2018 - A Multi-Horizon Quantile Recurrent Forecaster.pdf}
}

@article{weron_electricity_2014,
  title = {Electricity Price Forecasting: {{A}} Review of the State-of-the-Art with a Look into the Future},
  shorttitle = {Electricity Price Forecasting},
  author = {Weron, Rafa{\l}},
  year = {2014},
  month = oct,
  journal = {International Journal of Forecasting},
  volume = {30},
  number = {4},
  pages = {1030--1081},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2014.08.008},
  urldate = {2019-04-03},
  abstract = {A variety of methods and ideas have been tried for electricity price forecasting (EPF) over the last 15 years, with varying degrees of success. This review article aims to explain the complexity of available solutions, their strengths and weaknesses, and the opportunities and threats that the forecasting tools offer or that may be encountered. The paper also looks ahead and speculates on the directions EPF will or should take in the next decade or so. In particular, it postulates the need for objective comparative EPF studies involving (i) the same datasets, (ii) the same robust error evaluation procedures, and (iii) statistical testing of the significance of one model's outperformance of another.},
  keywords = {Autoregression,Day-ahead market,Electricity price forecasting,Factor model,Forecast combination,Neural network,Probabilistic forecast,Seasonality},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FS49P76G\\Weron - 2014 - Electricity price forecasting A review of the sta.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\U6VEHIUY\\S0169207014001083.html}
}

@article{west_bayesian_2020,
  title = {Bayesian Forecasting of Multivariate Time Series: {{Scalability}}, Structure Uncertainty and Decisions},
  shorttitle = {Bayesian Forecasting of Multivariate Time Series},
  author = {West, Mike},
  year = {2020},
  month = feb,
  journal = {Annals of the Institute of Statistical Mathematics},
  volume = {72},
  number = {1},
  eprint = {1911.09656},
  pages = {1--31},
  issn = {0020-3157, 1572-9052},
  doi = {10.1007/s10463-019-00741-3},
  urldate = {2020-02-17},
  abstract = {I overview recent research advances in Bayesian state-space modeling of multivariate time series. A main focus is on the ``decouple/recouple'' concept that enables application of state-space models to increasingly large-scale data, applying to continuous or discrete time series outcomes. The scope includes large-scale dynamic graphical models for forecasting and multivariate volatility analysis in areas such as economics and finance, multi-scale approaches for forecasting discrete/count time series in areas such as commercial sales and demand forecasting, and dynamic network flow models for areas including internet traffic monitoring. In applications, explicit forecasting, monitoring and decision goals are paramount and should factor into model assessment and comparison, a perspective that is highlighted.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Electrical Engineering and Systems Science - Signal Processing,Mathematics - Dynamical Systems,Statistics - Applications,Statistics - Computation,Statistics - Methodology},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HA8HCARS\West - 2020 - Bayesian forecasting of multivariate time series .pdf}
}

@article{weston_memory_2014,
  title = {Memory {{Networks}}},
  author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  year = {2014},
  month = oct,
  journal = {arXiv:1410.3916 [cs, stat]},
  eprint = {1410.3916},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HDHISFPV\Weston et al. - 2014 - Memory Networks.pdf}
}

@techreport{wickramasuriya_forecasting_2015,
  type = {Monash {{Econometrics}} and {{Business Statistics Working Paper}}},
  title = {Forecasting Hierarchical and Grouped Time Series through Trace Minimization},
  author = {Wickramasuriya, Shanika L. and Athanasopoulos, George and Hyndman, Rob},
  year = {2015},
  number = {15/15},
  institution = {{Monash University, Department of Econometrics and Business Statistics}},
  urldate = {2022-04-11},
  abstract = {Large collections of time series often have aggregation constraints due to product or geographical hierarchies. The forecasts for the disaggregated series are usually required to add up exactly to the forecasts of the aggregated series, a constraint known as \^a\texteuro\oe aggregate consistency\^a\texteuro{} . The combination forecasts proposed by Hyndman et al. (2011) are based on a Generalized Least Squares (GLS) estimator and require an estimate of the covariance matrix of the reconciliation errors (i.e., the errors that arise due to aggregate inconsistency). We show that this is impossible to estimate in practice due to identifiability conditions.},
  keywords = {contemporaneous error correlation,forecasting,Hierarchical time series,reconciliation,trace minimization},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8L4QNM24\\Wickramasuriya et al. - 2015 - Forecasting hierarchical and grouped time series t.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ZCTCPW6T\\2015-15.html}
}

@article{wickramasuriya_optimal_2019,
  title = {Optimal {{Forecast Reconciliation}} for {{Hierarchical}} and {{Grouped Time Series Through Trace Minimization}}},
  author = {Wickramasuriya, Shanika L. and Athanasopoulos, George and Hyndman, Rob J.},
  year = {2019},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {114},
  number = {526},
  pages = {804--819},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2018.1448825},
  urldate = {2020-03-15},
  abstract = {Large collections of time series often have aggregation constraints due to product or geographical groupings. The forecasts for the most disaggregated series are usually required to add-up exactly to the forecasts of the aggregated series, a constraint we refer to as ``coherence''. Forecast reconciliation is the process of adjusting forecasts to make them coherent.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\D3238978\Wickramasuriya et al. - 2019 - Optimal Forecast Reconciliation for Hierarchical a.pdf}
}

@techreport{wickramasuriya_optimal_2019a,
  title = {Optimal {{Non-negative Forecast Reconciliation}}},
  author = {Wickramasuriya, Shanika L. and Turlach, Berwin A. and Hyndman, Rob J.},
  year = {2019},
  journal = {Monash Econometrics and Business Statistics Working Papers},
  number = {15/19},
  institution = {{Monash University, Department of Econometrics and Business Statistics}},
  urldate = {2021-10-06},
  abstract = {The sum of forecasts of a disaggregated time series are often required to equal the forecast of the aggregate. The least squares solution for finding coherent forecasts uses a reconciliation approach known as MinT, proposed by Wickramasuriya, Athanasopoulos and Hyndman (2019). The MinT approach and its variants do not guarantee that the coherent forecasts are nonnegative, even when all of the original forecasts are non-negative in nature. This has become a serious issue in applications that are inherently non-negative such as with sales data or tourism numbers. While overcoming this difficulty, we consider the analytical solution of MinT as a least squares minimization problem. The non-negativity constraints are then imposed on the minimization problem to ensure that the coherent forecasts are strictly non-negative. Considering the dimension and sparsity of the matrices involved, and the alternative representation of MinT, this constrained quadratic programming problem is solved using three algorithms. They are the block principal pivoting algorithm, projected conjugate gradient algorithm, and scaled gradient projection algorithm. A Monte Carlo simulation is performed to evaluate the computational performances of these algorithms. The results demonstrate that the block principal pivoting algorithm clearly outperforms the rest, and projected conjugate gradient is the second best. The superior performance of the block principal pivoting algorithm can be partially attributed to the alternative representation of the weight matrix in the MinT approach. An empirical investigation is carried out to assess the impact of imposing non-negativity constraints on forecast reconciliation. It is observed that slight gains in forecast accuracy have occurred at the most disaggregated level. At the aggregated level slight losses are also observed. Although the gains or losses are negligible, the procedure plays an important role in decision and policy implementation processes.},
  langid = {english},
  keywords = {aggregation,Australian tourism,coherent forecasts,contemporaneous error correlation,forecast combinations,least squares,non-negative,reconciliation.,spatial correlations},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\FYYFHVIM\\Wickramasuriya et al. - 2019 - Optimal Non-negative Forecast Reconciliation.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\SHBGI6N5\\2019-15.html}
}

@misc{wijaya_forecasting_2015,
  title = {Forecasting {{Uncertainty}} in {{Electricity Demand}}},
  author = {Wijaya, Tri Kurniawan and Sinn, Mathieu and Chen, Bei},
  year = {2015},
  journal = {Infoscience},
  number = {CONF},
  urldate = {2020-11-18},
  abstract = {Generalized Additive Models (GAM) are a widely popular class of regression models to forecast electricity demand, due to their high accuracy, flexibility and interpretability. However, the residuals of the fitted GAM are typically heteroscedastic and leptokurtic caused by the nature of energy data. In this paper we propose a novel approach to estimate the time-varying conditional variance of the GAM residuals, which we call the GAM\^2 algorithm. It allows utility companies and network operators to assess the uncertainty of future electricity demand and incorporate it into their planning processes. The basic idea of our algorithm is to apply another GAM to the squared residuals to explain the dependence of uncertainty on exogenous variables. Empirical evidence shows that the residuals rescaled by the estimated conditional variance are approximately normal. We combine our modeling approach with online learning algorithms that adjust for dynamic changes in the distributions of demand. We illustrate our method by a case study on data from RTE, the operator of the French transmission grid.},
  howpublished = {http://infoscience.epfl.ch/record/203769},
  langid = {english},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\Y3A2CVND\\Wijaya et al. - 2015 - Forecasting Uncertainty in Electricity Demand.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9CV552EA\\203769.html}
}

@article{willi_recurrent_2019,
  title = {Recurrent {{Neural Processes}}},
  author = {Willi, Timon and Masci, Jonathan and Schmidhuber, J{\"u}rgen and Osendorfer, Christian},
  year = {2019},
  month = nov,
  journal = {arXiv:1906.05915 [cs, stat]},
  eprint = {1906.05915},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {We extend Neural Processes (NPs) to sequential data through Recurrent NPs or RNPs, a family of conditional state space models. RNPs model the state space with Neural Processes. Given time series observed on fast real-world time scales but containing slow long-term variabilities, RNPs may derive appropriate slow latent time scales. They do so in an efficient manner by establishing conditional independence among subsequences of the time series. Our theoretically grounded framework for stochastic processes expands the applicability of NPs while retaining their benefits of flexibility, uncertainty estimation, and favorable runtime with respect to Gaussian Processes (GPs). We demonstrate that state spaces learned by RNPs benefit predictive performance on real-world time-series data and nonlinear system identification, even in the case of limited data availability.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\4BKKT6GC\Willi et al. - 2019 - Recurrent Neural Processes.pdf}
}

@misc{williams_causal_,
  title = {Causal {{Inference}} on {{Time Series}} Using {{Restricted Structural Equation Models}} | {{Perceiving Systems}} - {{Max Planck Institute}} for {{Intelligent Systems}}},
  author = {Williams, Jon},
  urldate = {2019-04-17},
  abstract = {Using computer vision, computer graphics, and machine learning, we teach computers to see people and understand their behavior in complex 3D scenes.},
  howpublished = {https://is.tuebingen.mpg.de/},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\QAKXAUXV\petersjslmzk2013.html}
}

@inproceedings{wisniewski_application_2020,
  title = {Application of Conformal Prediction Interval Estimations to Market Makers' Net Positions},
  booktitle = {Proceedings of the {{Ninth Symposium}} on {{Conformal}} and {{Probabilistic Prediction}} and {{Applications}}},
  author = {Wisniewski, Wojciech and Lindsay, David and Lindsay, Sian},
  year = {2020},
  month = aug,
  pages = {285--301},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-03-07},
  abstract = {In this study we focus on the application of Conformal Prediction (CP) interval estimations to provide financial Market Makers (MMs) with some ``meaningful'' forecasts relating to their future short-term position in a given financial market. The idea is that using these market position forecasts, MMs can deploy proactive risk management strategies with a given degree of confidence. We make use of a novel financial time series dataset that comprises the net positions of a given MM over a three year period for trades pertaining to the top-traded Foreign Exchange (FX) symbols. This dataset \textendash{} \textbackslash nolinebreak\{NetPositionTimeSeries\} \textendash{} is noisy and complex. The net positions within it are generated from the trades of tens of thousands of clients trading in different directions (buy or sell) and over many different time horizons. We approached the problem of predicting future net position not as one that required an accurate point estimate as this is impossible. Rather we sought to gain a meaningful range of possible position bounds which would nonetheless be invaluable. In this study we tested a range of predictive Machine Learning (ML) techniques. We compared the CP framework to benchmark methods like moving average (MA) and quantile regression (QR). We demonstrate how application of the CP framework gives well calibrated region bounds on the MM net position forecasts.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\MBGXW5JB\Wisniewski et al. - 2020 - Application of conformal prediction interval estim.pdf}
}

@article{wooldridge_selection_1995,
  title = {Selection Corrections for Panel Data Models under Conditional Mean Independence Assumptions},
  author = {Wooldridge, Jeffrey M.},
  year = {1995},
  month = jul,
  journal = {Journal of Econometrics},
  volume = {68},
  number = {1},
  pages = {115--132},
  issn = {0304-4076},
  doi = {10.1016/0304-4076(94)01645-G},
  urldate = {2020-03-13},
  abstract = {Some new methods for testing and correcting for sample selection bias in panel data models are proposed. The assumptions allow the unobserved effects in both the regression and selection equations to be correlated with the observed variables; the error distribution in the regression equation is unspecified; arbitrary serial dependence in the idiosyncratic errors of both equations is allowed; and all idiosyncratic errors can be heterogeneously distributed. Compared with maximum likelihood and other estimators derived under fully parametric assumptions, the new estimators are much more robust and have significant computational advantages.},
  langid = {english},
  keywords = {Conditional mean independence,Fixed effects,Panel data,Sample selection,Two-step estimation},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\AKNGMFV5\\Wooldridge - 1995 - Selection corrections for panel data models under .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\U28ZI7NZ\\030440769401645G.html}
}

@article{wu_conditional_2020,
  title = {Conditional {{Mutual}} Information-Based {{Contrastive Loss}} for {{Financial Time Series Forecasting}}},
  author = {Wu, Hanwei and Gattami, Ather and Flierl, Markus},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.07638 [cs, stat]},
  eprint = {2002.07638},
  primaryclass = {cs, stat},
  urldate = {2020-02-20},
  abstract = {We present a method for financial time series forecasting using representation learning techniques. Recent progress on deep autoregressive models has shown their ability to capture long-term dependencies of the sequence data. However, the shortage of available financial data for training will make the deep models susceptible to the overfitting problem. In this paper, we propose a neural-network-powered conditional mutual information (CMI) estimator for learning representations for the forecasting task. Specifically, we first train an encoder to maximize the mutual information between the latent variables and the label information conditioned on the encoded observed variables. Then the features extracted from the trained encoder are used to learn a subsequent logistic regression model for predicting time series movements. Our proposed estimator transforms the CMI maximization problem to a classification problem whether two encoded representations are sampled from the same class or not. This is equivalent to perform pairwise comparisons of the training datapoints, and thus, improves the generalization ability of the deep autoregressive model. Empirical experiments indicate that our proposed method has the potential to advance the state-of-the-art performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\7P654HDV\\Wu et al. - 2020 - Conditional Mutual information-based Contrastive L.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\EA3XKYY3\\2002.html}
}

@article{wu_deep_2020,
  title = {Deep {{Transformer Models}} for {{Time Series Forecasting}}: {{The Influenza Prevalence Case}}},
  shorttitle = {Deep {{Transformer Models}} for {{Time Series Forecasting}}},
  author = {Wu, Neo and Green, Bradley and Ben, Xue and O'Banion, Shawn},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.08317 [cs, stat]},
  eprint = {2001.08317},
  primaryclass = {cs, stat},
  urldate = {2020-02-19},
  abstract = {In this paper, we present a new approach to time series forecasting. Time series data are prevalent in many scientific and engineering disciplines. Time series forecasting is a crucial task in modeling time series data, and is an important area of machine learning. In this work we developed a novel method that employs Transformer-based machine learning models to forecast time series data. This approach works by leveraging self-attention mechanisms to learn complex patterns and dynamics from time series data. Moreover, it is a generic framework and can be applied to univariate and multivariate time series data, as well as time series embeddings. Using influenza-like illness (ILI) forecasting as a case study, we show that the forecasting results produced by our approach are favorably comparable to the state-of-the-art.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\L3PRINK4\\Wu et al. - 2020 - Deep Transformer Models for Time Series Forecastin.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MXF9IAR6\\2001.html}
}

@incollection{wu_gaussian_2014,
  title = {Gaussian {{Process Volatility Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Wu, Yue and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Ghahramani, Zoubin},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {1044--1052},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6K4LIXWG\\Wu et al. - 2014 - Gaussian Process Volatility Model.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\86JNJHVP\\5376-gaussian-process-volatility-model.html}
}

@inproceedings{wu_hybrid_2018,
  title = {Hybrid {{Deep Sequential Modeling}} for {{Social Text-Driven Stock Prediction}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}  - {{CIKM}} '18},
  author = {Wu, Huizhe and Zhang, Wei and Shen, Weiwei and Wang, Jun},
  year = {2018},
  pages = {1627--1630},
  publisher = {{ACM Press}},
  address = {{Torino, Italy}},
  doi = {10.1145/3269206.3269290},
  urldate = {2019-04-25},
  abstract = {In addition to only considering stocks' price series, utilizing short and instant texts from social medias like Twitter has potential to yield better stock market prediction. While some previous approaches have explored this direction, their results are still far from satisfactory due to their reliance on performance of sentiment analysis and limited capabilities of learning direct relations between target stock trends and their daily social texts. To bridge this gap, we propose a novel Cross-modal attention based Hybrid Recurrent Neural Network (CH-RNN), which is inspired by the recent proposed DA-RNN model. Specifically, CH-RNN consists of two essential modules. One adopts DA-RNN to gain stock trend representations for different stocks. The other utilizes recurrent neural network to model daily aggregated social texts. These two modules interact seamlessly by the following two manners: 1) daily representations of target stock trends from the first module are leveraged to select trend-related social texts through a cross-modal attention mechanism, and 2) representations of text sequences and trend series are further integrated. The comprehensive experiments on the real dataset we build demonstrate the effectiveness of CH-RNN and benefit of considering social texts.},
  isbn = {978-1-4503-6014-2},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\RTHSYK4Q\Wu et al. - 2018 - Hybrid Deep Sequential Modeling for Social Text-Dr.pdf}
}

@inproceedings{wu_restful_2018,
  title = {{{RESTFul}}: {{Resolution-Aware Forecasting}} of {{Behavioral Time Series Data}}},
  shorttitle = {{{RESTFul}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Wu, Xian and Shi, Baoxu and Dong, Yuxiao and Huang, Chao and Faust, Louis and Chawla, Nitesh V.},
  year = {2018},
  series = {{{CIKM}} '18},
  pages = {1073--1082},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3269206.3271794},
  urldate = {2019-04-29},
  abstract = {Leveraging historical behavioral data (e.g., sales volume and email communication) for future prediction is of fundamental importance for practical domains ranging from sales to temporal link prediction. Current forecasting approaches often use only a single time resolution (e.g., daily or weekly), which truncates the range of observable temporal patterns. However, real-world behavioral time series typically exhibit patterns across multi-dimensional temporal patterns, yielding dependencies at each level. To fully exploit these underlying dynamics, this paper studies the forecasting problem for behavioral time series data with the consideration of multiple time resolutions and proposes a multi-resolution time series forecasting framework, RESolution-aware Time series Forecasting (RESTFul). In particular, we first develop a recurrent framework to encode the temporal patterns at each resolution. In the fusion process, a convolutional fusion framework is proposed, which is capable of learning conclusive temporal patterns for modeling behavioral time series data to predict future time steps. Our extensive experiments demonstrate that the RESTFul model significantly outperforms the state-of-the-art time series prediction techniques on both numerical and categorical behavioral time series data.},
  isbn = {978-1-4503-6014-2},
  keywords = {deep learning,multiple resolutions,read - in related work,time series forecasting},
  annotation = {00003},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\C2UQBA6S\Wu et al. - 2018 - RESTFul.pdf}
}

@article{wu_topological_2019,
  title = {Topological {{Machine Learning}} for {{Multivariate Time Series}}},
  author = {Wu, Chengyuan and Hargreaves, Carol Anne},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.12082 [cs, eess, math]},
  eprint = {1911.12082},
  primaryclass = {cs, eess, math},
  urldate = {2020-02-17},
  abstract = {We develop a framework for analyzing multivariate time series using topological data analysis (TDA) methods. The proposed methodology involves converting the multivariate time series to point cloud data, calculating Wasserstein distances between the persistence diagrams and using the \$k\$-nearest neighbors algorithm (\$k\$-NN) for supervised machine learning. Two methods (symmetry-breaking and anchor points) are also introduced to enable TDA to better analyze data with heterogeneous features that are sensitive to translation, rotation, or choice of coordinates. We apply our methods to room occupancy detection based on 5 time-dependent variables (temperature, humidity, light, CO2 and humidity ratio). Experimental results show that topological methods are effective in predicting room occupancy during a time window.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Mathematics - Algebraic Topology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WKGLWABH\\Wu and Hargreaves - 2019 - Topological Machine Learning for Multivariate Time.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RXTL9DHR\\1911.html}
}

@inproceedings{xiang_gaips_2021,
  title = {{{GAIPS}}: {{Accelerating Maximum Inner Product Search}} with {{GPU}}},
  shorttitle = {{{GAIPS}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Xiang, Long and Yan, Xiao and Lu, Lan and Tang, Bo},
  year = {2021},
  month = jul,
  pages = {1920--1924},
  publisher = {{ACM}},
  address = {{Virtual Event Canada}},
  doi = {10.1145/3404835.3462997},
  urldate = {2022-02-04},
  isbn = {978-1-4503-8037-9},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\C74ZI8LH\Xiang et al. - 2021 - GAIPS Accelerating Maximum Inner Product Search w.pdf}
}

@article{xiao_ndt_2017,
  title = {{{NDT}}: {{Neural Decision Tree Towards Fully Functioned Neural Graph}}},
  shorttitle = {{{NDT}}},
  author = {Xiao, Han},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.05934 [cs]},
  eprint = {1712.05934},
  primaryclass = {cs},
  urldate = {2019-09-10},
  abstract = {Though traditional algorithms could be embedded into neural architectures with the proposed principle of (Xiao, 2017), the variables that only occur in the condition of branch could not be updated as a special case. To tackle this issue, we multiply the conditioned branches with Dirac symbol (i.e. 1x{$>$}0), then approximate Dirac symbol with the continuous functions (e.g. 1 - e-{$\alpha$}|x|). In this way, the gradients of condition-specific variables could be worked out in the back-propagation process, approximately, making a fully functioned neural graph. Within our novel principle, we propose the neural decision tree (NDT), which takes simplified neural networks as decision function in each branch and employs complex neural networks to generate the output in each leaf. Extensive experiments verify our theoretical analysis and demonstrate the effectiveness of our model.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {00005},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\B4JRQZL4\Xiao - 2017 - NDT Neual Decision Tree Towards Fully Functioned .pdf}
}

@article{xie_unified_2018,
  title = {A {{Unified Framework}} for {{Long Range}} and {{Cold Start Forecasting}} of {{Seasonal Profiles}} in {{Time Series}}},
  author = {Xie, Christopher and Tank, Alex and {Greaves-Tunnell}, Alec and Fox, Emily},
  year = {2018},
  month = aug,
  journal = {arXiv:1710.08473 [cs, stat]},
  eprint = {1710.08473},
  primaryclass = {cs, stat},
  urldate = {2019-11-21},
  abstract = {Providing long-range forecasts is a fundamental challenge in time series modeling, which is only compounded by the challenge of having to form such forecasts when a time series has never previously been observed. The latter challenge is the time series version of the cold-start problem seen in recommender systems which, to our knowledge, has not been addressed in previous work. A similar problem occurs when a long range forecast is required after only observing a small number of time points \textemdash{} a warm start forecast. With these aims in mind, we focus on forecasting seasonal profiles\textemdash or baseline demand \textemdash for periods on the order of a year in three cases: the long range case with multiple previously observed seasonal profiles, the cold start case with no previous observed seasonal profiles, and the warm start case with only a single partially observed profile. Classical time series approaches that perform iterated step-ahead forecasts based on previous observations struggle to provide accurate long range predictions; in settings with little to no observed data, such approaches are simply not applicable. Instead, we present a straightforward framework which combines ideas from high-dimensional regression and matrix factorization on a carefully constructed data matrix. Key to our formulation and resulting performance is leveraging (1) repeated patterns over fixed periods of time and across series, and (2) metadata associated with the individual series; without this additional data, the cold-start/warm-start problems are nearly impossible to solve. We demonstrate that our framework can accurately forecast an array of seasonal profiles on multiple large scale datasets.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\EI8BT2WJ\Xie et al. - 2018 - A Unified Framework for Long Range and Cold Start .pdf}
}

@article{xiong_learning_,
  title = {Learning {{Decision Trees}} with {{Reinforcement Learning}}},
  author = {Xiong, Zheng and Zhang, Wenpeng and Zhu, Wenwu},
  pages = {5},
  abstract = {Decision trees are usually learned by heuristic methods like greedy search, which only considers immediate information gain at the current splitting node and often results in sub-optimal solutions in a constrained search space. In this paper, to overcome this problem, we propose a reinforcement learning approach to automatically search for splitting strategies in the global search space based on the evaluation of long-term payoff. Empirically, decision trees generated by our method outperform those generated by commonly used greedy search methods under the same hyper-parameter setting.},
  langid = {english},
  annotation = {00003},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\M85H2K6C\Xiong et al. - Learning Decision Trees with Reinforcement Learnin.pdf}
}

@inproceedings{xu_conformal_2021,
  title = {Conformal Prediction Interval for Dynamic Time-Series},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Xu, Chen and Xie, Yao},
  year = {2021},
  month = jul,
  pages = {11559--11569},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-03-06},
  abstract = {We develop a method to construct distribution-free prediction intervals for dynamic time-series, called \textbackslash Verb|EnbPI| that wraps around any bootstrap ensemble estimator to construct sequential prediction intervals. \textbackslash Verb|EnbPI| is closely related to the conformal prediction (CP) framework but does not require data exchangeability. Theoretically, these intervals attain finite-sample, \textbackslash textit\{approximately valid\} marginal coverage for broad classes of regression functions and time-series with strongly mixing stochastic errors. Computationally, \textbackslash Verb|EnbPI| avoids overfitting and requires neither data-splitting nor training multiple ensemble estimators; it efficiently aggregates bootstrap estimators that have been trained. In general, \textbackslash Verb|EnbPI| is easy to implement, scalable to producing arbitrarily many prediction intervals sequentially, and well-suited to a wide range of regression functions. We perform extensive real-data analyses to demonstrate its effectiveness.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\VTFK4722\Xu and Xie - 2021 - Conformal prediction interval for dynamic time-ser.pdf}
}

@article{xu_gradient_2019,
  title = {Gradient {{Boosted Feature Selection}}},
  author = {Xu, Zhixiang Eddie and Huang, Gao and Weinberger, Kilian Q. and Zheng, Alice X.},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.04055 [cs, stat]},
  eprint = {1901.04055},
  primaryclass = {cs, stat},
  urldate = {2019-07-30},
  abstract = {A feature selection algorithm should ideally satisfy four conditions: reliably extract relevant features; be able to identify non-linear feature interactions; scale linearly with the number of features and dimensions; allow the incorporation of known sparsity structure. In this work we propose a novel feature selection algorithm, Gradient Boosted Feature Selection (GBFS), which satisfies all four of these requirements. The algorithm is flexible, scalable, and surprisingly straight-forward to implement as it is based on a modification of Gradient Boosted Trees. We evaluate GBFS on several real world data sets and show that it matches or outperforms other state of the art feature selection algorithms. Yet it scales to larger data set sizes and naturally allows for domain-specific side information.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00057},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\AFLYQ6F8\Xu et al. - 2019 - Gradient Boosted Feature Selection.pdf}
}

@misc{xu_sequential_2023,
  title = {Sequential {{Predictive Conformal Inference}} for {{Time Series}}},
  author = {Xu, Chen and Xie, Yao},
  year = {2023},
  month = feb,
  number = {arXiv:2212.03463},
  eprint = {2212.03463},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.03463},
  urldate = {2023-03-07},
  abstract = {We present a new distribution-free conformal prediction algorithm for sequential data (e.g., time series), called the \textbackslash textit\{sequential predictive conformal inference\} (\textbackslash texttt\{SPCI\}). We specifically account for the nature that time series data are non-exchangeable, and thus many existing conformal prediction algorithms are not applicable. The main idea is to exploit the temporal dependence of non-conformity scores (e.g., prediction residuals); thus, the past residuals contain information about future ones. Then we cast the problem of conformal prediction interval as predicting the quantile of a future residual, given a user-specified point prediction algorithm. Theoretically, we establish asymptotic valid conditional coverage upon extending consistency analyses in quantile regression. Using simulation and real-data experiments, we demonstrate a significant reduction in interval width of \textbackslash texttt\{SPCI\} compared to other existing methods under the desired empirical coverage.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RLFPC2J4\\Xu and Xie - 2023 - Sequential Predictive Conformal Inference for Time.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RYPRMJ4Z\\2212.html}
}

@article{yagli_reconciling_2020,
  title = {Reconciling Solar Forecasts: {{Probabilistic}} Forecasting with Homoscedastic {{Gaussian}} Errors on a Geographical Hierarchy},
  shorttitle = {Reconciling Solar Forecasts},
  author = {Yagli, Gokhan Mert and Yang, Dazhi and Srinivasan, Dipti},
  year = {2020},
  month = nov,
  journal = {Solar Energy},
  series = {Special {{Issue}} on {{Grid Integration}}},
  volume = {210},
  pages = {59--67},
  issn = {0038-092X},
  doi = {10.1016/j.solener.2020.06.005},
  urldate = {2021-02-04},
  abstract = {Hierarchical forecasting and reconciliation are new to the field of solar engineering. Previous papers in this series, namely, Yang et al. (2017a, 2017b), and Yagli et al. (2019b), discussed various reconciliation techniques for deterministic solar forecasts obtained across spatio-temporal hierarchies. This paper extends the discussion into probability space, and studies how reconciliation can affect the performance of probabilistic forecasting. More specifically, qualities of the parametric predictive distributions before and after reconciliation are compared. Four minimum-trace-based reconciliation techniques are used to reconcile day-ahead and hour-ahead forecasts generated using two datasets: (1) distributed solar power generation for 318 simulated PV systems in California, and (2) satellite-derived irradiance over Arizona. The empirical result shows that reconciliation not only improves the accuracy of point forecasts, but also leads to high-quality predictive distributions in terms of sharpness, calibration, and skill score. Moreover, such improvement is quite general, and does not seem to depend on data, hierarchy structure, nor the underlying forecasting model.},
  langid = {english},
  keywords = {Machine learning,Numerical weather prediction,Probabilistic forecasting,Reconciliation,Satellite-derived irradiance,Solar forecasting},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7FL7AU6C\Yagli et al. - 2020 - Reconciling solar forecasts Probabilistic forecas.pdf}
}

@article{yang_deep_2018,
  title = {Deep {{Neural Decision Trees}}},
  author = {Yang, Yongxin and Morillo, Irene Garcia and Hospedales, Timothy M.},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.06988 [cs, stat]},
  eprint = {1806.06988},
  primaryclass = {cs, stat},
  urldate = {2019-08-06},
  abstract = {Deep neural networks have been proven powerful at processing perceptual data, such as images and audio. However for tabular data, tree-based models are more popular. A nice property of tree-based models is their natural interpretability. In this work, we present Deep Neural Decision Trees (DNDT) \textendash{} tree models realised by neural networks. A DNDT is intrinsically interpretable, as it is a tree. Yet as it is also a neural network (NN), it can be easily implemented in NN toolkits, and trained with gradient descent rather than greedy splitting. We evaluate DNDT on several tabular datasets, verify its efficacy, and investigate similarities and differences between DNDT and vanilla decision trees. Interestingly, DNDT self-prunes at both split and feature-level.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00007},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\K87PRFRK\Yang et al. - 2018 - Deep Neural Decision Trees.pdf}
}

@misc{yang_diffusion_2022,
  title = {Diffusion {{Models}}: {{A Comprehensive Survey}} of {{Methods}} and {{Applications}}},
  shorttitle = {Diffusion {{Models}}},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  year = {2022},
  month = oct,
  number = {arXiv:2209.00796},
  eprint = {2209.00796},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.00796},
  urldate = {2022-11-03},
  abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LHPXN8KQ\\Yang et al. - 2022 - Diffusion Models A Comprehensive Survey of Method.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\E3RZY6AK\\2209.html}
}

@inproceedings{yang_explainable_2018,
  title = {Explainable {{Text-Driven Neural Network}} for {{Stock Prediction}}},
  booktitle = {2018 5th {{IEEE International Conference}} on {{Cloud Computing}} and {{Intelligence Systems}} ({{CCIS}})},
  author = {Yang, Linyi and Zhang, Zheng and Xiong, Su and Wei, Lirui and Ng, James and Xu, Lina and Dong, Ruihai},
  year = {2018},
  month = nov,
  pages = {441--445},
  publisher = {{IEEE}},
  address = {{Nanjing, China}},
  doi = {10.1109/CCIS.2018.8691233},
  urldate = {2019-04-17},
  abstract = {It has been shown that financial news leads to news text using natural language processing toolkits, the fluctuation of stock prices. However, previous work such as OpenIE (Fader, 2011) and dependency parsing. on news-driven financial market prediction focused only Obviously, the errors generated by natural language on predicting stock price movement without providing processing (NLP) tools will propagate in these methods. an explanation. In this paper, we propose a dual-layer Furthermore, it will lose a large amount of useful attention-based neural network to address this issue. In information from those news which cannot be dealt the initial stage, we introduce a knowledge-based with, since the limitation of existing NLP tools.},
  isbn = {978-1-5386-6005-8},
  langid = {english},
  keywords = {read - not in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\ZQSUSSD4\Yang et al. - 2018 - Explainable Text-Driven Neural Network for Stock P.pdf}
}

@misc{yang_finitesample_2021,
  title = {Finite-Sample {{Efficient Conformal Prediction}}},
  author = {Yang, Yachong and Kuchibhotla, Arun Kumar},
  year = {2021},
  month = may,
  number = {arXiv:2104.13871},
  eprint = {2104.13871},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.13871},
  urldate = {2023-03-07},
  abstract = {Conformal prediction is a generic methodology for finite-sample valid distribution-free prediction. This technique has garnered a lot of attention in the literature partly because it can be applied with any machine learning algorithm that provides point predictions to yield valid prediction regions. Of course, the efficiency (width/volume) of the resulting prediction region depends on the performance of the machine learning algorithm. In this paper, we consider the problem of obtaining the smallest conformal prediction region given a family of machine learning algorithms. We provide two general-purpose selection algorithms and consider coverage as well as width properties of the final prediction region. The first selection method yields the smallest width prediction region among the family of conformal prediction regions for all sample sizes, but only has an approximate coverage guarantee. The second selection method has a finite sample coverage guarantee but only attains close to the smallest width. The approximate optimal width property of the second method is quantified via an oracle inequality. Asymptotic oracle inequalities are also considered when the family of algorithms is given by ridge regression with different penalty parameters.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\63S88MLF\\Yang and Kuchibhotla - 2021 - Finite-sample Efficient Conformal Prediction.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\TDIHCSWR\\2104.html}
}

@article{yang_free_2021,
  title = {Free {{Lunch}} for {{Few-shot Learning}}: {{Distribution Calibration}}},
  shorttitle = {Free {{Lunch}} for {{Few-shot Learning}}},
  author = {Yang, Shuo and Liu, Lu and Xu, Min},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.06395 [cs]},
  eprint = {2101.06395},
  primaryclass = {cs},
  urldate = {2021-01-20},
  abstract = {Learning from a limited number of samples is challenging since the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples, then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-theart accuracy on two datasets (5\% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HQHX3APQ\Yang et al. - 2021 - Free Lunch for Few-shot Learning Distribution Cal.pdf}
}

@incollection{yang_fully_2019,
  title = {Fully {{Parameterized Quantile Function}} for {{Distributional Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Yang, Derek and Zhao, Li and Lin, Zichuan and Qin, Tao and Bian, Jiang and Liu, Tie-Yan},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {6190--6199},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-01-29},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\B93KTG3H\\Yang et al. - 2019 - Fully Parameterized Quantile Function for Distribu.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\F7DI4E3B\\8850-fully-parameterized-quantile-function-for-distributional-reinforcement-learning.html}
}

@inproceedings{yang_multitask_2019,
  title = {Multi-Task {{Learning Method}} for {{Hierarchical Time Series Forecasting}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2019: {{Text}} and {{Time Series}}},
  author = {Yang, Maoxin and Hu, Qinghua and Wang, Yun},
  editor = {Tetko, Igor V. and K{\r{u}}rkov{\'a}, V{\v e}ra and Karpov, Pavel and Theis, Fabian},
  year = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {474--485},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-30490-4_38},
  abstract = {Hierarchical time series is a set of time series organized by aggregation constraints and it is widely used in many real-world applications. Usually, hierarchical time series forecasting can be realized with a two-step method, in which all time series are forecasted independently and then the forecasting results are reconciled to satisfy aggregation consistency. However, these two-step methods have a high computational complexity and are unable to ensure optimal forecasts for all time series. In this paper, we propose a novel hierarchical forecasting approach to solve the above problems. Based on multi-task learning, we construct an integrated model that combines features of the bottom level series and the hierarchical structure. Then forecasts of all time series are output simultaneously and they are aggregated consistently. The model has the advantage of utilizing the correlation between time series. And the forecasting results are overall optimal by optimizing a global loss function. In order to avoid the curse of dimensionality as the number of time series grows larger, we further learn a sparse model with group sparsity and element-wise sparsity constraints according to data characteristics. The experimental results on simulation data and tourism data demonstrate that our method has a better overall performance while simplifying forecasting process.},
  isbn = {978-3-030-30490-4},
  langid = {english},
  keywords = {Hierarchical time series forecasting,Multi-task learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\AW2U87CL\Yang et al. - 2019 - Multi-task Learning Method for Hierarchical Time S.pdf}
}

@inproceedings{yang_recurrent_2018,
  title = {Recurrent {{Deep Multiagent Q-Learning}} for {{Autonomous Brokers}} in {{Smart Grid}}},
  booktitle = {Proceedings of the {{Twenty-Seventh International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Yang, Yaodong and Hao, Jianye and Sun, Mingyang and Wang, Zan and Fan, Changjie and Strbac, Goran},
  year = {2018},
  month = jul,
  pages = {569--575},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Stockholm, Sweden}},
  doi = {10.24963/ijcai.2018/79},
  urldate = {2020-05-25},
  abstract = {The broker mechanism is widely applied to serve for interested parties to derive long-term policies in order to reduce costs or gain profits in smart grid. However, a broker is faced with a number of challenging problems such as balancing demand and supply from customers and competing with other coexisting brokers to maximize its profit. In this paper, we develop an efficient pricing strategy for brokers in local electricity retail market based on recurrent deep multiagent reinforcement learning and sequential clustering. We use real household electricity consumption data to simulate the retail market for evaluating our strategy. The experiments demonstrate the superior performance of the proposed pricing strategy and highlight the effectiveness of our reward shaping mechanism.},
  isbn = {978-0-9992411-2-7},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\C3Z6PMRX\Yang et al. - 2018 - Recurrent Deep Multiagent Q-Learning for Autonomou.pdf}
}

@article{yang_traffic_2019,
  title = {Traffic Flow Prediction Using {{LSTM}} with Feature Enhancement},
  author = {Yang, Bailin and Sun, Shulin and Li, Jianyuan and Lin, Xianxuan and Tian, Yan},
  year = {2019},
  month = mar,
  journal = {Neurocomputing},
  volume = {332},
  pages = {320--327},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2018.12.016},
  urldate = {2019-04-03},
  abstract = {Long short-term memory (LSTM) is widely used to process and predict events with time series, but it is difficult to solve exceedingly long-term dependencies, possibly because the LSTM errors increase as the sequence length increases. Recently, researchers have noted that adding features on multiple time scales can help improve the long-term dependency of the RNN, which is inspired by the attention mechanism, considering the need for historical data in traffic flow prediction. We propose an improved approach that connects the high-impact value of remarkably long sequence time steps to the current time step, and these high-impact traffic flow values are captured using the attention mechanism. At the same time, we smooth out some data beyond the normal range to obtain better prediction results. The experimental results show that the proposed prediction model has certain competitiveness in short-term traffic flow predictions.},
  keywords = {Attention mechanism,LSTM feature enhancement,Noise processing,read - in related work,Short-term traffic flow prediction},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\VNXXXN6W\\Yang et al. - 2019 - Traffic flow prediction using LSTM with feature en.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LP2I27JD\\S092523121831470X.html}
}

@inproceedings{yeo_dernn_2018,
  title = {{{DE-RNN}}: {{Forecasting}} the {{Probability Density Function}} of {{Nonlinear Time Series}}},
  shorttitle = {{{DE-RNN}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Yeo, K. and Melnyk, I. and Nguyen, N. and Lee, E. K.},
  year = {2018},
  month = nov,
  pages = {697--706},
  doi = {10.1109/ICDM.2018.00085},
  abstract = {Model-free identification of a nonlinear dynamical system from the noisy observations is of current interest due to its direct relevance to many applications in Industry 4.0. Making a prediction of such noisy time series constitutes a problem of learning the nonlinear time evolution of a probability distribution. Capability of most of the conventional time series models is limited when the underlying dynamics is nonlinear, multi-scale or when there is no prior knowledge at all on the system dynamics. We propose DE-RNN (Density Estimation Recurrent Neural Network) to learn the probability density function (PDF) of a stochastic process with an underlying nonlinear dynamics and compute the time evolution of the PDF for a probabilistic forecast. A Recurrent Neural Network (RNN)-based model is employed to learn a nonlinear operator for the temporal evolution of the stochastic process. We use a softmax layer for a numerical discretization of a smooth PDF, which transforms a function approximation problem to a classification task. A regularized cross-entropy method is introduced to impose a smoothness condition on the estimated probability distribution. A Monte Carlo procedure to compute the temporal evolution of the distribution for a multiple-step forecast is presented. It is shown that the proposed algorithm can learn the nonlinear multi-scale dynamics from the noisy observations and provides an effective tool to forecast time evolution of the underlying probability distribution. Evaluation of the algorithm on three synthetic and two real data sets shows advantage over the compared baselines, and a potential value to a wide range of problems in physics and engineering.},
  keywords = {Computational modeling,cross-entropy method,DE-RNN,density estimation recurrent neural network,Dynamical System,entropy,Forecast,function approximation,function approximation problem,learning (artificial intelligence),Mathematical model,model-free identification,Monte Carlo methods,Monte Carlo procedure,noisy time series,nonlinear dynamical system,nonlinear dynamical systems,Nonlinear dynamical systems,nonlinear multiscale dynamics,nonlinear time evolution,nonlinear time series,Predictive models,probability,probability density function,Probability density function,probability distribution,Probability distribution,recurrent neural nets,Recurrent Neural Network,smooth PDF,stochastic process,stochastic processes,time series,Time Series,Time series analysis,Uncertainty Quantification},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\68EBVFUP\8594894.html}
}

@article{yildiz_ode_2019,
  title = {{{ODE}}\$\^2\${{VAE}}: {{Deep}} Generative Second Order {{ODEs}} with {{Bayesian}} Neural Networks},
  shorttitle = {{{ODE}}\$\^2\${{VAE}}},
  author = {Y{\i}ld{\i}z, {\c C}a{\u g}atay and Heinonen, Markus and L{\"a}hdesm{\"a}ki, Harri},
  year = {2019},
  month = oct,
  journal = {arXiv:1905.10994 [cs, stat]},
  eprint = {1905.10994},
  primaryclass = {cs, stat},
  urldate = {2020-02-19},
  abstract = {We present Ordinary Differential Equation Variational Auto-Encoder (ODE\$\^2\$VAE), a latent second order ODE model for high-dimensional sequential data. Leveraging the advances in deep generative models, ODE\$\^2\$VAE can simultaneously learn the embedding of high dimensional trajectories and infer arbitrarily complex continuous-time latent dynamics. Our model explicitly decomposes the latent space into momentum and position components and solves a second order ODE system, which is in contrast to recurrent neural network (RNN) based time series models and recently proposed black-box ODE techniques. In order to account for uncertainty, we propose probabilistic latent ODE dynamics parameterized by deep Bayesian neural networks. We demonstrate our approach on motion capture, image rotation and bouncing balls datasets. We achieve state-of-the-art performance in long term motion prediction and imputation tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\QGX7X8QX\\YÄ±ldÄ±z et al. - 2019 - ODE$^2$VAE Deep generative second order ODEs with.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WZQ4VZHX\\1905.html}
}

@incollection{yoon_timeseries_2019,
  title = {Time-Series {{Generative Adversarial Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Yoon, Jinsung and Jarrett, Daniel and {van der Schaar}, Mihaela},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {5508--5518},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2020-05-20},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\QKLL2DJA\\Yoon et al. - 2019 - Time-series Generative Adversarial Networks.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\W5UWING9\\8789-time-series-generative-adversarial-networks.html}
}

@article{yu_longterm_2019,
  title = {Long-Term {{Forecasting}} Using {{Higher Order Tensor RNNs}}},
  author = {Yu, Rose and Zheng, Stephan and Anandkumar, Anima and Yue, Yisong},
  year = {2019},
  month = aug,
  journal = {arXiv:1711.00073 [cs]},
  eprint = {1711.00073},
  primaryclass = {cs},
  urldate = {2020-02-17},
  abstract = {We present Higher-Order Tensor RNN (HOT-RNN), a novel family of neural sequence architectures for multivariate forecasting in environments with nonlinear dynamics. Long-term forecasting in such systems is highly challenging, since there exist long-term temporal dependencies, higher-order correlations and sensitivity to error propagation. Our proposed recurrent architecture addresses these issues by learning the nonlinear dynamics directly using higher-order moments and higher-order state transition functions. Furthermore, we decompose the higher-order structure using the tensor-train decomposition to reduce the number of parameters while preserving the model performance. We theoretically establish the approximation guarantees and the variance bound for HOT-RNN for general sequence inputs. We also demonstrate 5\% \textasciitilde{} 12\% improvements for long-term prediction over general RNN and LSTM architectures on a range of simulated environments with nonlinear dynamics, as well on real-world time series data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\NBQ9SPQT\\Yu et al. - 2019 - Long-term Forecasting using Higher Order Tensor RN.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LUZ73HRH\\1711.html}
}

@article{yu_online_2019,
  title = {Online Big Data-Driven Oil Consumption Forecasting with {{Google}} Trends},
  author = {Yu, Lean and Zhao, Yaqing and Tang, Ling and Yang, Zebin},
  year = {2019},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {Special {{Section}}: {{Supply Chain Forecasting}}},
  volume = {35},
  number = {1},
  pages = {213--223},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2017.11.005},
  urldate = {2019-03-19},
  abstract = {The rapid development of big data technologies and the Internet provides a rich mine of online big data (e.g., trend spotting) that can be helpful in predicting oil consumption \textemdash{} an essential but uncertain factor in the oil supply chain. An online big data-driven oil consumption forecasting model is proposed that uses Google trends, which finely reflect various related factors based on a myriad of search results. This model involves two main steps, relationship investigation and prediction improvement. First, cointegration tests and a Granger causality analysis are conducted in order to statistically test the predictive power of Google trends, in terms of having a significant relationship with oil consumption. Second, the effective Google trends are introduced into popular forecasting methods for predicting both oil consumption trends and values. The experimental study of global oil consumption prediction confirms that the proposed online big-data-driven forecasting work with Google trends improves on the traditional techniques without Google trends significantly, for both directional and level predictions.},
  keywords = {Artificial intelligence,Google trends,Oil consumption forecasting,Online big data,Supply chain},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LRXZ8W5G\\Yu et al. - 2019 - Online big data-driven oil consumption forecasting.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\6K77PS3H\\S016920701730136X.html}
}

@article{yu_sample_,
  title = {Towards {{Sample Efficient Reinforcement Learning}}},
  author = {Yu, Yang},
  pages = {5},
  abstract = {Reinforcement learning is a major tool to realize intelligent agents that can be autonomously adaptive to the environment. With deep models, reinforcement learning has shown great potential in complex tasks such as playing games from pixels. However, current reinforcement learning techniques are still suffer from requiring a huge amount of interaction data, which could result in unbearable cost in realworld applications. In this article, we share our understanding of the problem, and discuss possible ways to alleviate the sample cost of reinforcement learning, from the aspects of exploration, optimization, environment modeling, experience transfer, and abstraction. We also discuss some challenges in real-world applications, with the hope of inspiring future researches.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\UXKPPJP9\Yu - Towards Sample Efficient Reinforcement Learning.pdf}
}

@incollection{yu_temporal_2016,
  title = {Temporal {{Regularized Matrix Factorization}} for {{High-dimensional Time Series Prediction}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Yu, Hsiang-Fu and Rao, Nikhil and Dhillon, Inderjit S},
  year = {2016},
  pages = {847--855},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  keywords = {read - in related work},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\HSG6CW9R\\Yu et al. - 2016 - Temporal Regularized Matrix Factorization for High.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\KZXSLU7N\\6160-temporal-regularized-matrix-factorization-for-high-dimensional-time-series-prediction.html}
}

@misc{yuan_diverse_2019,
  title = {Diverse {{Trajectory Forecasting}} with {{Determinantal Point Processes}}},
  author = {Yuan, Ye and Kitani, Kris},
  year = {2019},
  month = dec,
  number = {arXiv:1907.04967},
  eprint = {1907.04967},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.04967},
  urldate = {2023-03-20},
  abstract = {The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a single outcome. While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode that has most data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse and likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as a parameter estimation of the DSF. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn move the latent codes of the sample set to find an optimal diverse and likely set of trajectories. Our method is a novel application of DPPs to optimize a set of items (trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\H7MQ5S3S\\Yuan and Kitani - 2019 - Diverse Trajectory Forecasting with Determinantal .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\LGYLNG4Q\\1907.html}
}

@article{yukseltan_hourly_2020,
  title = {Hourly Electricity Demand Forecasting Using {{Fourier}} Analysis with Feedback},
  author = {Yukseltan, Ergun and Yucekaya, Ahmet and Bilge, Ayse Humeyra},
  year = {2020},
  month = sep,
  journal = {Energy Strategy Reviews},
  volume = {31},
  pages = {100524},
  issn = {2211-467X},
  doi = {10.1016/j.esr.2020.100524},
  urldate = {2021-01-21},
  abstract = {Whether it be long-term, like year-ahead, or short-term, such as hour-ahead or day-ahead, forecasting of electricity demand is crucial for the success of deregulated electricity markets. The stochastic nature of the demand for electricity, along with parameters such as temperature, humidity, and work habits, eventually causes deviations from expected demand. In this paper, we propose a feedback-based forecasting methodology in which the hourly prediction by a Fourier series expansion is updated by using the error at the current hour for the forecast at the next hour. The proposed methodology is applied to the Turkish power market for the period 2012\textendash 2017 and provides a powerful tool to forecasts the demand in hourly, daily and yearly horizons using only the past demand data. The hourly forecasting errors in the demand, in the Mean Absolute Percentage Error (MAPE) norm, are 0.87\% in hour-ahead, 2.90\% in day-ahead, and 3.54\% in year-ahead horizons, respectively. An autoregressive (AR) model is also applied to the predictions by the Fourier series expansion to obtain slightly better results. As predictions are updated on an hourly basis using the already realized data for the current hour, the model can be considered as reliable and practical in circumstances needed to make bidding and dispatching decisions.},
  langid = {english},
  keywords = {Feedback,Forecast,Fourier series,Modulation,Prediction,Time series analysis},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WK9LRN49\\Yukseltan et al. - 2020 - Hourly electricity demand forecasting using Fourie.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\U7WLP65I\\S2211467X20300778.html}
}

@misc{zador_nextgeneration_2022,
  title = {Toward {{Next-Generation Artificial Intelligence}}: {{Catalyzing}} the {{NeuroAI Revolution}}},
  shorttitle = {Toward {{Next-Generation Artificial Intelligence}}},
  author = {Zador, Anthony and Richards, Blake and {\"O}lveczky, Bence and Escola, Sean and Bengio, Yoshua and Boahen, Kwabena and Botvinick, Matthew and Chklovskii, Dmitri and Churchland, Anne and Clopath, Claudia and DiCarlo, James and Ganguli, Surya and Hawkins, Jeff and Koerding, Konrad and Koulakov, Alexei and LeCun, Yann and Lillicrap, Timothy and Marblestone, Adam and Olshausen, Bruno and Pouget, Alexandre and Savin, Cristina and Sejnowski, Terrence and Simoncelli, Eero and Solla, Sara and Sussillo, David and Tolias, Andreas S. and Tsao, Doris},
  year = {2022},
  month = oct,
  number = {arXiv:2210.08340},
  eprint = {2210.08340},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.08340},
  urldate = {2022-11-03},
  abstract = {Neuroscience has long been an important driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\MSRNX2EB\\Zador et al. - 2022 - Toward Next-Generation Artificial Intelligence Ca.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\V3ZZ4LTN\\2210.html}
}

@inproceedings{zaffran_adaptive_2022,
  title = {Adaptive {{Conformal Predictions}} for {{Time Series}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Zaffran, Margaux and Feron, Olivier and Goude, Yannig and Josse, Julie and Dieuleveut, Aymeric},
  year = {2022},
  month = jun,
  pages = {25834--25866},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-03-07},
  abstract = {Uncertainty quantification of predictive models is crucial in decision-making problems. Conformal prediction is a general and theoretically sound answer. However, it requires exchangeable data, excluding time series. While recent works tackled this issue, we argue that Adaptive Conformal Inference (ACI, Gibbs \& Cand\{\`e\}s, 2021), developed for distribution-shift time series, is a good procedure for time series with general dependency. We theoretically analyse the impact of the learning rate on its efficiency in the exchangeable and auto-regressive case. We propose a parameter-free method, AgACI, that adaptively builds upon ACI based on online expert aggregation. We lead extensive fair simulations against competing methods that advocate for ACI's use in time series. We conduct a real case study: electricity price forecasting. The proposed aggregation algorithm provides efficient prediction intervals for day-ahead forecasting. All the code and data to reproduce the experiments are made available on GitHub.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JGZQWKWH\Zaffran et al. - 2022 - Adaptive Conformal Predictions for Time Series.pdf}
}

@article{zamo_estimation_2018,
  title = {Estimation of the {{Continuous Ranked Probability Score}} with {{Limited Information}} and {{Applications}} to {{Ensemble Weather Forecasts}}},
  author = {Zamo, Micha{\"e}l and Naveau, Philippe},
  year = {2018},
  month = feb,
  journal = {Mathematical Geosciences},
  volume = {50},
  number = {2},
  pages = {209--234},
  issn = {1874-8953},
  doi = {10.1007/s11004-017-9709-7},
  urldate = {2020-12-16},
  abstract = {The continuous ranked probability score (CRPS) is a much used measure of performance for probabilistic forecasts of a scalar observation. It is a quadratic measure of the difference between the forecast cumulative distribution function (CDF) and the empirical CDF of the observation. Analytic formulations of the CRPS can be derived for most classical parametric distributions, and be used to assess the efficiency of different CRPS estimators. When the true forecast CDF is not fully known, but represented as an ensemble of values, the CRPS is estimated with some error. Thus, using the CRPS to compare parametric probabilistic forecasts with ensemble forecasts may be misleading due to the unknown error of the estimated CRPS for the ensemble. With simulated data, the impact of the type of the verified ensemble (a random sample or a set of quantiles) on the CRPS estimation is studied. Based on these simulations, recommendations are issued to choose the most accurate CRPS estimator according to the type of ensemble. The interest of these recommendations is illustrated with real ensemble weather forecasts. Also, relationships between several estimators of the CRPS are demonstrated and used to explain the differences of accuracy between the estimators.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\6269HE43\Zamo and Naveau - 2018 - Estimation of the Continuous Ranked Probability Sc.pdf}
}

@article{zamora-martinez_online_2014,
  title = {On-Line Learning of Indoor Temperature Forecasting Models towards Energy Efficiency},
  author = {{Zamora-Mart{\'i}nez}, F. and Romeu, P. and {Botella-Rocamora}, P. and Pardo, J.},
  year = {2014},
  month = nov,
  journal = {Energy and Buildings},
  series = {{{SCIENCE BEHIND AND BEYOND THE SOLAR DECATHLON EUROPE}} 2012},
  volume = {83},
  pages = {162--172},
  issn = {0378-7788},
  doi = {10.1016/j.enbuild.2014.04.034},
  urldate = {2019-04-03},
  abstract = {The SMLsystem is a house built at the Universidad CEU Cardenal Herrera (CEU-UCH) to participate in the Solar Decathlon 2012 competition. Several technologies have been integrated to reduce power consumption. A predictive module, based on artificial neural networks (ANNs), has been developed using data acquired in Valencia. The module produces short-term forecast of indoor temperature, using as input data captured by a complex monitoring system. The system expects to reduce the power consumption related to Heating, Ventilation and Air Conditioning (HVAC) system, due to the following assumptions: the high power consumption for which HVAC is responsible (53.9\% of the overall consumption); and the energy needed to maintain temperature is less than the energy required to lower/increase it. This paper studies the development viability of predictive systems for a totally unknown environment applying on-line learning techniques. The model parameters are estimated starting from a totally random model or from an unbiased a priori knowledge. These forecasting measures could allow the house to adapt itself to future temperature conditions by using home automation in an energy-efficient manner. Experimental results show reasonable forecasting accuracy with simple models, and in relatively short training time (4\textendash 5 days).},
  keywords = {Artificial neural networks,Bayesian models,Energy efficiency,Gradient descent,Time series forecasting},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\KCJUBR8J\\Zamora-MartÃ­nez et al. - 2014 - On-line learning of indoor temperature forecasting.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\ULYTZT8I\\S0378778814003569.html}
}

@article{zaremba_reinforcement_2015,
  title = {Reinforcement {{Learning Neural Turing Machines}} - {{Revised}}},
  author = {Zaremba, Wojciech and Sutskever, Ilya},
  year = {2015},
  month = may,
  journal = {arXiv:1505.00521 [cs]},
  eprint = {1505.00521},
  primaryclass = {cs},
  urldate = {2019-03-19},
  abstract = {The expressive power of a machine learning model is closely related to the number of sequential computational steps it can learn. For example, Deep Neural Networks have been more successful than shallow networks because they can perform a greater number of sequential computational steps (each highly parallel). The Neural Turing Machine (NTM) [8] is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiable; thus the NTM can be trained with backpropagation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\FHV7W3CV\Zaremba and Sutskever - 2015 - Reinforcement Learning Neural Turing Machines - Re.pdf}
}

@inproceedings{zeevi_time_1996,
  title = {Time {{Series Prediction Using Mixtures}} of {{Experts}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Zeevi, Assaf J. and Meir, Ron and Adler, Robert J.},
  year = {1996},
  series = {{{NIPS}}'96},
  pages = {309--315},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  urldate = {2019-04-17},
  abstract = {We consider the problem of prediction of stationary time series, using the architecture known as mixtures of experts (MEM). Here we suggest a mixture which blends several autoregressive models. This study focuses on some theoretical foundations of the prediction problem in this context. More precisely, it is demonstrated that this model is a universal approximator, with respect to learning the unknown prediction function. This statement is strengthened as upper bounds on the mean squared error are established. Based on these results it is possible to compare the MEM to other families of models (e.g., neural networks and state dependent models). It is shown that a degenerate version of the MEM is in fact equivalent to a neural network, and the number of experts in the architecture plays a similar role to the number of hidden units in the latter model.},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\JU8UFUHN\Zeevi et al. - Time Series Prediction using Mixtures of Experts.pdf}
}

@inproceedings{zhang_bidaware_2016,
  title = {Bid-Aware {{Gradient Descent}} for {{Unbiased Learning}} with {{Censored Data}} in {{Display Advertising}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Zhang, Weinan and Zhou, Tianxiong and Wang, Jun and Xu, Jian},
  year = {2016},
  series = {{{KDD}} '16},
  pages = {665--674},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2939672.2939713},
  urldate = {2019-04-16},
  abstract = {In real-time display advertising, ad slots are sold per impression via an auction mechanism. For an advertiser, the campaign information is incomplete --- the user responses (e.g, clicks or conversions) and the market price of each ad impression are observed only if the advertiser's bid had won the corresponding ad auction. The predictions, such as bid landscape forecasting, click-through rate (CTR) estimation, and bid optimisation, are all operated in the pre-bid stage with full-volume bid request data. However, the training data is gathered in the post-bid stage with a strong bias towards the winning impressions. A common solution for learning over such censored data is to reweight data instances to correct the discrepancy between training and prediction. However, little study has been done on how to obtain the weights independent of previous bidding strategies and consequently integrate them into the final CTR prediction and bid generation steps. In this paper, we formulate CTR estimation and bid optimisation under such censored auction data. Derived from a survival model, we show that historic bid information is naturally incorporated to produce Bid-aware Gradient Descents (BGD) which controls both the importance and the direction of the gradient to achieve unbiased learning. The empirical study based on two large-scale real-world datasets demonstrates remarkable performance gains from our solution. The learning framework has been deployed on Yahoo!'s real-time bidding platform and provided 2.97\% AUC lift for CTR estimation and 9.30\% eCPC drop for bid optimisation in an online A/B test.},
  isbn = {978-1-4503-4232-2},
  keywords = {censored data,display advertising,real-time bidding,unbiased learning}
}

@article{zhang_deep_2018,
  title = {A {{Deep Neural Network}} for {{Unsupervised Anomaly Detection}} and {{Diagnosis}} in {{Multivariate Time Series Data}}},
  author = {Zhang, Chuxu and Song, Dongjin and Chen, Yuncong and Feng, Xinyang and Lumezanu, Cristian and Cheng, Wei and Ni, Jingchao and Zong, Bo and Chen, Haifeng and Chawla, Nitesh V.},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.08055 [cs, stat]},
  eprint = {1811.08055},
  primaryclass = {cs, stat},
  urldate = {2019-04-03},
  abstract = {Nowadays, multivariate time series data are increasingly collected in various real world systems, e.g., power plants, wearable devices, etc. Anomaly detection and diagnosis in multivariate time series refer to identifying abnormal status in certain time steps and pinpointing the root causes. Building such a system, however, is challenging since it not only requires to capture the temporal dependency in each time series, but also need encode the inter-correlations between different pairs of time series. In addition, the system should be robust to noise and provide operators with different levels of anomaly scores based upon the severity of different incidents. Despite the fact that a number of unsupervised anomaly detection algorithms have been developed, few of them can jointly address these challenges. In this paper, we propose a Multi-Scale Convolutional Recurrent Encoder-Decoder (MSCRED), to perform anomaly detection and diagnosis in multivariate time series data. Specifically, MSCRED first constructs multi-scale (resolution) signature matrices to characterize multiple levels of the system statuses in different time steps. Subsequently, given the signature matrices, a convolutional encoder is employed to encode the inter-sensor (time series) correlations and an attention based Convolutional Long-Short Term Memory (ConvLSTM) network is developed to capture the temporal patterns. Finally, based upon the feature maps which encode the inter-sensor correlations and temporal information, a convolutional decoder is used to reconstruct the input signature matrices and the residual signature matrices are further utilized to detect and diagnose anomalies. Extensive empirical studies based on a synthetic dataset and a real power plant dataset demonstrate that MSCRED can outperform state-ofthe-art baseline methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,read - not in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HC6PD78V\Zhang et al. - 2018 - A Deep Neural Network for Unsupervised Anomaly Det.pdf}
}

@article{zhang_deep_2019,
  title = {Deep {{Reinforcement Learning}} for {{Trading}}},
  author = {Zhang, Zihao and Zohren, Stefan and Roberts, Stephen},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.10107 [cs, q-fin]},
  eprint = {1911.10107},
  primaryclass = {cs, q-fin},
  urldate = {2020-02-17},
  abstract = {We adopt Deep Reinforcement Learning algorithms to design trading strategies for continuous futures contracts. Both discrete and continuous action spaces are considered and volatility scaling is incorporated to create reward functions which scale trade positions based on market volatility. We test our algorithms on the 50 most liquid futures contracts from 2011 to 2019, and investigate how performance varies across different asset classes including commodities, equity indices, fixed income and FX markets. We compare our algorithms against classical time series momentum strategies, and show that our method outperforms such baseline models, delivering positive profits despite heavy transaction costs. The experiments show that the proposed algorithms can follow large market trends without changing positions and can also scale down, or hold, through consolidation periods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Finance - Computational Finance,Quantitative Finance - Trading and Market Microstructure},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\8KGZWDI3\\Zhang et al. - 2019 - Deep Reinforcement Learning for Trading.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\UMXNJXI2\\1911.html}
}

@inproceedings{zhang_dnnbased_2016,
  title = {{{DNN-based Prediction Model}} for {{Spatio-temporal Data}}},
  booktitle = {Proceedings of the 24th {{ACM SIGSPATIAL International Conference}} on {{Advances}} in {{Geographic Information Systems}}},
  author = {Zhang, Junbo and Zheng, Yu and Qi, Dekang and Li, Ruiyuan and Yi, Xiuwen},
  year = {2016},
  series = {{{SIGSPACIAL}} '16},
  pages = {92:1--92:4},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2996913.2997016},
  urldate = {2019-04-15},
  abstract = {Advances in location-acquisition and wireless communication technologies have led to wider availability of spatio-temporal (ST) data, which has unique spatial properties (i.e. geographical hierarchy and distance) and temporal properties (i.e. closeness, period and trend). In this paper, we propose a {$<$}u{$>$}Deep{$<$}/u{$>$}-learning-based prediction model for {$<$}u{$>$}S{$<$}/u{$>$}patio-{$<$}u{$>$}T{$<$}/u{$>$}emporal data (DeepST). We leverage ST domain knowledge to design the architecture of DeepST, which is comprised of two components: spatio-temporal and global. The spatio-temporal component employs the framework of convolutional neural networks to simultaneously model spatial near and distant dependencies, and temporal closeness, period and trend. The global component is used to capture global factors, such as day of the week, weekday or weekend. Using DeepST, we build a real-time crowd flow forecasting system called UrbanFlow1. Experiment results on diverse ST datasets verify DeepST's ability to capture ST data's spatio-temporal properties, showing the advantages of DeepST beyond four baseline methods.},
  isbn = {978-1-4503-4589-7},
  keywords = {deep learning,prediction,read - in related work,spatio-temporal data},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\V9IGSEDZ\Zhang et al. - 2016 - DNN-based Prediction Model for Spatio-temporal Dat.pdf}
}

@inproceedings{zhang_endtoend_2019,
  title = {An {{End-to-End Automatic Cloud Database Tuning System Using Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Zhang, Ji and Liu, Yu and Zhou, Ke and Li, Guoliang and Xiao, Zhili and Cheng, Bin and Xing, Jiashu and Wang, Yangtao and Cheng, Tianheng and Liu, Li and Ran, Minwei and Li, Zekang},
  year = {2019},
  month = jun,
  pages = {415--432},
  publisher = {{ACM}},
  address = {{Amsterdam Netherlands}},
  doi = {10.1145/3299869.3300085},
  urldate = {2022-03-10},
  isbn = {978-1-4503-5643-5},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\7EHQPI8I\Zhang et al. - 2019 - An End-to-End Automatic Cloud Database Tuning Syst.pdf}
}

@article{zhang_errorfeedback_2020,
  title = {Error-Feedback {{Stochastic Configuration Strategy}} on {{Convolutional Neural Networks}} for {{Time Series Forecasting}}},
  author = {Zhang, Xinze and He, Kun and Bao, Yukun},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.00717 [cs, stat]},
  eprint = {2002.00717},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {Despite the superiority of convolutional neural networks demonstrated in time series modeling and forecasting, it has not been fully explored on the design of the neural network architecture as well as the tuning of the hyper-parameters. Inspired by the iterative construction strategy for building a random multilayer perceptron, we propose a novel Error-feedback Stochastic Configuration (ESC) strategy to construct a random Convolutional Neural Network (ESC-CNN) for time series forecasting task, which builds the network architecture adaptively. The ESC strategy suggests that random filters and neurons of the error-feedback fully connected layer are incrementally added in a manner that they can steadily compensate the prediction error during the construction process, and a filter selection strategy is introduced to secure that ESC-CNN holds the universal approximation property, providing helpful information at each iterative process for the prediction. The performance of ESC-CNN is justified on its prediction accuracy for one-step-ahead and multi-step-ahead forecasting tasks. Comprehensive experiments on a synthetic dataset and two real-world datasets show that the proposed ESC-CNN not only outperforms the state-of-art random neural networks, but also exhibits strong predictive power in comparison to trained Convolution Neural Networks and Long Short-Term Memory models, demonstrating the effectiveness of ESC-CNN in time series forecasting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\67KI6JH8\\Zhang et al. - 2020 - Error-feedback Stochastic Configuration Strategy o.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DAUA5HP3\\2002.html}
}

@article{zhang_forecasting_1998,
  title = {Forecasting with Artificial Neural Networks:: {{The}} State of the Art},
  shorttitle = {Forecasting with Artificial Neural Networks},
  author = {Zhang, Guoqiang and Eddy Patuwo, B. and Y. Hu, Michael},
  year = {1998},
  month = mar,
  journal = {International Journal of Forecasting},
  volume = {14},
  number = {1},
  pages = {35--62},
  issn = {0169-2070},
  doi = {10.1016/S0169-2070(97)00044-7},
  urldate = {2019-03-19},
  abstract = {Interest in using artificial neural networks (ANNs) for forecasting has led to a tremendous surge in research activities in the past decade. While ANNs provide a great deal of promise, they also embody much uncertainty. Researchers to date are still not certain about the effect of key factors on forecasting performance of ANNs. This paper presents a state-of-the-art survey of ANN applications in forecasting. Our purpose is to provide (1) a synthesis of published research in this area, (2) insights on ANN modeling issues, and (3) the future research directions.},
  keywords = {Forecasting,Neural networks},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HIZGRCMC\S0169207097000447.html}
}

@article{zhang_gradient_2015,
  title = {A Gradient Boosting Method to Improve Travel Time Prediction},
  author = {Zhang, Yanru and Haghani, Ali},
  year = {2015},
  month = sep,
  journal = {Transportation Research Part C: Emerging Technologies},
  series = {Big {{Data}} in {{Transportation}} and {{Traffic Engineering}}},
  volume = {58},
  pages = {308--324},
  issn = {0968-090X},
  doi = {10.1016/j.trc.2015.02.019},
  urldate = {2019-07-30},
  abstract = {Tree based ensemble methods have reached a celebrity status in prediction field. By combining simple regression trees with `poor' performance, they usually produce high prediction accuracy. In contrast to other machine learning methods that have been treated as black-boxes, tree based ensemble methods provide interpretable results, while requiring little data preprocessing, are able to handle different types of predictor variables, and can fit complex nonlinear relationship. These properties make the tree based ensemble methods good candidates for solving travel time prediction problems. However, applications of tree-based ensemble algorithms in traffic prediction area are limited. In this paper, we employ a gradient boosting regression tree method (GBM) to analyze and model freeway travel time to improve the prediction accuracy and model interpretability. The gradient boosting tree method strategically combines additional trees by correcting mistakes made by its previous base models, therefore, potentially improves prediction accuracy. Different parameters' effect on model performance and correlations of input\textendash output variables are discussed in details by using travel time data provided by INRIX along two freeway sections in Maryland. The proposed method is, then, compared with another popular ensemble method and a bench mark model. Study results indicate that the GBM model has its considerable advantages in freeway travel time prediction.},
  keywords = {Ensemble learning,Gradient boosting regression tree,Machine learning,Random forest,Short-term forecasting,Travel time},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\KSSYS4HH\\Zhang and Haghani - 2015 - A gradient boosting method to improve travel time .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\K93TTKXC\\S0968090X15000741.html}
}

@article{zhang_least_2018,
  title = {Least {{Squares-based Optimal Reconciliation Method}} for {{Hierarchical Forecasts}} of {{Wind Power Generation}}},
  author = {Zhang, Yao and Dong, Jiaojiao},
  year = {2018},
  journal = {IEEE Transactions on Power Systems},
  pages = {1--1},
  issn = {1558-0679},
  doi = {10.1109/TPWRS.2018.2868175},
  abstract = {Wind power generation is hierarchically organized and can be aggregated/disaggregated based on geography and electrical network structure. Forecasts are required at all levels of such a hierarchy for different operational requirements, which are also known as ``hierarchical forecasts''. In this paper, we propose a new method to produce hierarchical wind power forecasts which performs better than the conventional top-down or bottom-up method. At first, wind power generation at all levels of the hierarchy is independently forecasted using state-of-the-art techniques. Then, a least squares regression model is used to optimally reconcile these base forecasts. Adjusted forecasts are as close as possible to base forecasts but also satisfy the requirement of the aggregate consistency (i.e., the lower-level forecasts add up exactly to the higher-level forecasts). Case studies validate the effectiveness of the proposed approach for very short-term wind power forecasts. The accuracy improvement of reconciled forecasts over benchmarks is confirmed at all levels of the hierarchy, regardless of forecast horizons and hierarchical structures.},
  keywords = {Aggregates,Bottom-up forecasts,combining forecasts,Forecasting,hier-archical forecasting,Indexes,least squares regression,Time series analysis,top-down forecasts,very short-term forecasting,Wind farms,Wind forecasting,Wind power generation},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\HXSKN4DE\8453006.html}
}

@article{zhang_optimal_2023,
  title = {Optimal Reconciliation with Immutable Forecasts},
  author = {Zhang, Bohan and Kang, Yanfei and Panagiotelis, Anastasios and Li, Feng},
  year = {2023},
  month = jul,
  journal = {European Journal of Operational Research},
  volume = {308},
  number = {2},
  pages = {650--660},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2022.11.035},
  urldate = {2023-06-05},
  abstract = {The practical importance of coherent forecasts in hierarchical forecasting has inspired many studies on forecast reconciliation. Under this approach, base forecasts are produced for every series in the hierarchy and are subsequently adjusted to be coherent in a second reconciliation step. Reconciliation methods have been shown to improve forecast accuracy but will generally adjust the base forecast of every series. However, in an operational context, it is sometimes necessary or beneficial to keep forecasts of some variables unchanged after forecast reconciliation. In this paper, we formulate a reconciliation methodology that keeps forecasts of a pre-specified subset of variables unchanged or ``immutable''. In contrast to existing approaches, these immutable forecasts need not all come from the same level of a hierarchy, and our method can also be applied to grouped hierarchies. We prove that our approach preserves unbiasedness in base forecasts. Our method can also account for correlations between base forecasting errors and ensure the non-negativity of forecasts. We also perform empirical experiments, including an application to a large-scale online retailer's sales, to assess our proposed methodology's impacts.},
  langid = {english},
  keywords = {Constrained optimization,Forecasting,Hierarchical time series,Online retail,Unbiasedness},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\YQJJHU9I\Zhang et al. - 2023 - Optimal reconciliation with immutable forecasts.pdf}
}

@article{zhang_realtime_2018,
  title = {Real-Time {{Power System State Estimation}} and {{Forecasting}} via {{Deep Neural Networks}}},
  author = {Zhang, Liang and Wang, Gang and Giannakis, Georgios B.},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.06146 [cs, stat]},
  eprint = {1811.06146},
  primaryclass = {cs, stat},
  urldate = {2019-03-19},
  abstract = {Contemporary power grids are being challenged by rapid voltage fluctuations that are caused by large-scale deployment of renewable generation, electric vehicles, and demand response programs. In this context, monitoring the grid's operating conditions in real time becomes increasingly critical. With the emergent large scale and nonconvexity however, the existing power system state estimation (PSSE) schemes become computationally expensive or yield suboptimal performance. To bypass these hurdles, this paper advocates deep neural networks (DNNs) for real-time power system monitoring. By unrolling an iterative physics-based prox-linear solver, a novel model-specific DNN is developed for real-time PSSE with affordable training and minimal tuning effort. To further enable system awareness even ahead of the time horizon, as well as to endow the DNN-based estimator with resilience, deep recurrent neural networks (RNNs) are also pursued for power system state forecasting. Deep RNNs leverage the long-term nonlinear dependencies present in the historical voltage time series to enable forecasting, and they are easy to implement. Numerical tests showcase improved performance of the proposed DNN-based estimation and forecasting approaches compared with existing alternatives. In real load data experiments on the IEEE 118-bus benchmark system, the novel model-specific DNN-based PSSE scheme outperforms nearly by an order-of-magnitude the competing alternatives, including the widely adopted Gauss-Newton PSSE solver.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,read - not in related work,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\58CAT8QI\\Zhang et al. - 2018 - Real-time Power System State Estimation and Foreca.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RVJCMSRB\\1811.html}
}

@article{zhang_sequential_2019,
  title = {Sequential {{Gaussian Processes}} for {{Online Learning}} of {{Nonstationary Functions}}},
  author = {Zhang, Michael Minyi and Dumitrascu, Bianca and Williamson, Sinead A. and Engelhardt, Barbara E.},
  year = {2019},
  month = oct,
  journal = {arXiv:1905.10003 [cs, stat]},
  eprint = {1905.10003},
  primaryclass = {cs, stat},
  urldate = {2020-02-19},
  abstract = {Many machine learning problems can be framed in the context of estimating functions, and often these are time-dependent functions that are estimated in real-time as observations arrive. Gaussian processes (GPs) are an attractive choice for modeling real-valued nonlinear functions due to their flexibility and uncertainty quantification. However, the typical GP regression model suffers from several drawbacks: i) Conventional GP inference scales \$O(N\^\{3\})\$ with respect to the number of observations; ii) updating a GP model sequentially is not trivial; and iii) covariance kernels often enforce stationarity constraints on the function, while GPs with non-stationary covariance kernels are often intractable to use in practice. To overcome these issues, we propose an online sequential Monte Carlo algorithm to fit mixtures of GPs that capture non-stationary behavior while allowing for fast, distributed inference. By formulating hyperparameter optimization as a multi-armed bandit problem, we accelerate mixing for real time inference. Our approach empirically improves performance over state-of-the-art methods for online GP estimation in the context of prediction for simulated non-stationary data and hospital time series data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\RTETXT2S\\Zhang et al. - 2019 - Sequential Gaussian Processes for Online Learning .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GY59Z79U\\1905.html}
}

@inproceedings{zhang_stock_2017,
  title = {Stock {{Price Prediction}} via {{Discovering Multi-Frequency Trading Patterns}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}  - {{KDD}} '17},
  author = {Zhang, Liheng and Aggarwal, Charu and Qi, Guo-Jun},
  year = {2017},
  pages = {2141--2149},
  publisher = {{ACM Press}},
  address = {{Halifax, NS, Canada}},
  doi = {10.1145/3097983.3098117},
  urldate = {2019-04-17},
  abstract = {Stock prices are formed based on short and/or long-term commercial and trading activities that reflect different frequencies of trading patterns. However, these patterns are often elusive as they are affected by many uncertain politicaleconomic factors in the real world, such as corporate performances, government policies, and even breaking news circulated across markets. Moreover, time series of stock prices are non-stationary and non-linear, making the prediction of future price trends much challenging. To address them, we propose a novel State Frequency Memory (SFM) recurrent network to capture the multi-frequency trading patterns from past market data to make long and short term predictions over time. Inspired by Discrete Fourier Transform (DFT), the SFM decomposes the hidden states of memory cells into multiple frequency components, each of which models a particular frequency of latent trading pattern underlying the fluctuation of stock price. Then the future stock prices are predicted as a nonlinear mapping of the combination of these components in an Inverse Fourier Transform (IFT) fashion. Modeling multi-frequency trading patterns can enable more accurate predictions for various time ranges: while a shortterm prediction usually depends on high frequency trading patterns, a long-term prediction should focus more on the low frequency trading patterns targeting at long-term return. Unfortunately, no existing model explicitly distinguishes between various frequencies of trading patterns to make dynamic predictions in literature. The experiments on the real market data also demonstrate more competitive performance by the SFM as compared with the state-of-the-art methods.},
  isbn = {978-1-4503-4887-4},
  langid = {english},
  keywords = {read - in related work},
  annotation = {29},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\97MAIZJV\Zhang et al. - 2017 - Stock Price Prediction via Discovering Multi-Frequ.pdf}
}

@article{zhang_you_2019,
  title = {You {{May Not Need Order}} in {{Time Series Forecasting}}},
  author = {Zhang, Yunkai and Jiang, Qiao and Li, Shurui and Jin, Xiaoyong and Ma, Xueying and Yan, Xifeng},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.09620 [cs, stat]},
  eprint = {1910.09620},
  primaryclass = {cs, stat},
  urldate = {2019-10-28},
  abstract = {Time series forecasting with limited data is a challenging yet critical task. While transformers have achieved outstanding performances in time series forecasting, they often require many training samples due to the large number of trainable parameters. In this paper, we propose a training technique for transformers that prepares the training windows through random sampling. As input time steps need not be consecutive, the number of distinct samples increases from linearly to combinatorially many. By breaking the temporal order, this technique also helps transformers to capture dependencies among time steps in finer granularity. We achieve competitive results compared to the state-of-the-art on real-world datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\B4D6XLJL\\Zhang et al. - 2019 - You May Not Need Order in Time Series Forecasting.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\WRKTBGZX\\1910.html}
}

@article{zhao_blackbox_2019,
  title = {Blackbox {{Attacks}} on {{Reinforcement Learning Agents Using Approximated Temporal Information}}},
  author = {Zhao, Yiren and Shumailov, Ilia and Cui, Han and Gao, Xitong and Mullins, Robert and Anderson, Ross},
  year = {2019},
  month = nov,
  journal = {arXiv:1909.02918 [cs, stat]},
  eprint = {1909.02918},
  primaryclass = {cs, stat},
  urldate = {2020-02-17},
  abstract = {Recent research on reinforcement learning (RL) has suggested that trained agents are vulnerable to maliciously crafted adversarial samples. In this work, we show how such samples can be generalised from White-box and Grey-box attacks to a strong Black-box case, where the attacker has no knowledge of the agents, their training parameters and their training methods. We use sequence-to-sequence models to predict a single action or a sequence of future actions that a trained agent will make. First, we show our approximation model, based on time-series information from the agent, consistently predicts RL agents' future actions with high accuracy in a Black-box setup on a wide range of games and RL algorithms. Second, we find that although adversarial samples are transferable from the target model to our RL agents, they often outperform random Gaussian noise only marginally. This highlights a serious methodological deficiency in previous work on such agents; random jamming should have been taken as the baseline for evaluation. Third, we propose a novel use for adversarial samplesin Black-box attacks of RL agents: they can be used to trigger a trained agent to misbehave after a specific time delay. This appears to be a genuinely new type of attack. It potentially enables an attacker to use devices controlled by RL agents as time bombs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\747FUTPE\\Zhao et al. - 2019 - Blackbox Attacks on Reinforcement Learning Agents .pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\2GNHVEAJ\\1909.html}
}

@article{zhao_for2for_2020,
  title = {{{For2For}}: {{Learning}} to Forecast from Forecasts},
  shorttitle = {{{For2For}}},
  author = {Zhao, Shi and Feng, Ying},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.04601 [cs, stat]},
  eprint = {2001.04601},
  primaryclass = {cs, stat},
  urldate = {2020-02-27},
  abstract = {This paper presents a time series forecasting framework which combines standard forecasting methods and a machine learning model. The inputs to the machine learning model are not lagged values or regular time series features, but instead forecasts produced by standard methods. The machine learning model can be either a convolutional neural network model or a recurrent neural network model. The intuition behind this approach is that forecasts of a time series are themselves good features characterizing the series, especially when the modelling purpose is forecasting. It can also be viewed as a weighted ensemble method. Tested on the M4 competition dataset, this approach outperforms all submissions for quarterly series, and is more accurate than all but the winning algorithm for monthly series.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\DJHS2HLM\\Zhao and Feng - 2020 - For2For Learning to forecast from forecasts.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\KFP68X8C\\2001.html}
}

@inproceedings{zhao_forecasting_2018,
  title = {Forecasting {{Wavelet Transformed Time Series}} with {{Attentive Neural Networks}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Zhao, Y. and Shen, Y. and Zhu, Y. and Yao, J.},
  year = {2018},
  month = nov,
  pages = {1452--1457},
  doi = {10.1109/ICDM.2018.00201},
  abstract = {This paper studies the problem of time series forecasting. A time series is defined as a sequence of data points listed in time order. Many real-life time series data are driven by multiple latent components which occur at different frequencies. Existing solutions to time series forecasting fail to identify and discriminate these frequency-domain components. Inspired by the recent advent of signal processing and speech recognition techniques that decompose a time series signal into its time-frequency representation - a scalogram (or spectrogram), this paper proposes to explicitly disclose frequency-domain information from a univariate time series using wavelet transform, towards improving forecasting accuracy. Based on the transformed data, we leverage different neural networks to capture local time-frequency features and global long-term trend simultaneously. We further employ the attention mechanism to fuse local and global features in an effective manner. The experimental results on real time series show that our proposed approach achieves better performance than various baseline methods.},
  keywords = {attentive neural networks,Feature extraction,Forecasting,forecasting accuracy,frequency-domain components,frequency-domain information,local time-frequency features,Market research,neural nets,real-life time series data,signal processing,speech recognition techniques,time series,Time series analysis,time series forecasting,{Time series forecasting, wavelet transform, attentive neural networks},time series show,time series signal,time-frequency analysis,Time-frequency analysis,time-frequency representation,transformed data,univariate time series,wavelet transformed time series,wavelet transforms,Wavelet transforms},
  annotation = {00002},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\PDC9V8UQ\8595010.html}
}

@article{zhao_muse_2019,
  title = {{{MUSE}}: {{Parallel Multi-Scale Attention}} for {{Sequence}} to {{Sequence Learning}}},
  shorttitle = {{{MUSE}}},
  author = {Zhao, Guangxiang and Sun, Xu and Xu, Jingjing and Zhang, Zhiyuan and Luo, Liangchen},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.09483 [cs]},
  eprint = {1911.09483},
  primaryclass = {cs},
  urldate = {2019-11-25},
  abstract = {In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention, and pointwise transformation. MUSE builds on MUSE-simple and explores combining convolution and self-attention for learning sequence representations from more different scales. We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and outperforms all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism. Code will be available at https://github.com/lancopku/MUSE.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\A5WIWSMN\Zhao et al. - 2019 - MUSE Parallel Multi-Scale Attention for Sequence .pdf}
}

@article{zhao_neural_2018,
  title = {Neural {{Regression Tree}}},
  author = {Zhao, Wenbo and Memon, Shahan Ali and Raj, Bhiksha and Singh, Rita},
  year = {2018},
  month = sep,
  urldate = {2019-09-07},
  abstract = {Regression-via-Classification (RvC) is the process of converting a regression problem to a classification one. Current approaches for RvC use ad-hoc discretization strategies and are suboptimal. We...},
  annotation = {00000},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\9N4RQX6Z\\Zhao et al. - 2018 - Neural Regression Tree.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\GEHXXJ2Z\\forum.html}
}

@article{zhao_provably_2022,
  title = {Provably {{Calibrated Regression Under Distribution Drift}}},
  author = {Zhao, Shengjia and Tashiro, Yusuke and Tse, Danny and Ermon, Stefano},
  year = {2022},
  month = jan,
  urldate = {2023-03-07},
  abstract = {Accurate uncertainty quantification is a key building block of trustworthy machine learning systems. Uncertainty is typically represented by probability distributions over the possible outcomes, and these probabilities should be calibrated, \textbackslash textit\{e.g\}. the 90\textbackslash\% credible interval should contain the true outcome 90\textbackslash\% of the times. In the online prediction setup, existing conformal methods can provably achieve calibration assuming no distribution shift; however, the assumption is difficult to verify, and unlikely to hold in many applications such as time series prediction. Inspired by control theory, we propose a prediction algorithm that guarantees calibration even under distribution shift, and achieves strong performance on metrics such as sharpness and proper scores. We compare our method with baselines on 19 time-series and regression datasets, and our method achieves approximately 2x reduction in calibration error, comparable sharpness, and improved downstream decision utility.},
  langid = {english},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\R9WHBUCH\Zhao et al. - 2022 - Provably Calibrated Regression Under Distribution .pdf}
}

@inproceedings{zheng_efficient_2016,
  title = {Efficient {{Shift-Invariant Dictionary Learning}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '16},
  author = {Zheng, Guoqing and Yang, Yiming and Carbonell, Jaime},
  year = {2016},
  pages = {2095--2104},
  publisher = {{ACM Press}},
  address = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2939824},
  urldate = {2019-04-17},
  abstract = {Shift-invariant dictionary learning (SIDL) refers to the problem of discovering a latent basis (the dictionary) that captures informative local patterns at different locations of input sequences, and a sparse coding for each sequence as a linear combination of the latent basis elements. It differs from conventional dictionary learning and sparse coding where the latent basis has the same dimension as the input vectors, where the focus is on global patterns instead of shift-invariant local patterns. Unsupervised discovery of shift-invariant dictionary and the corresponding sparse coding has been an open challenge as the number of candidate local patterns is extremely large, and the number of possible linear combinations of such local patterns is even more so. In this paper we propose a new framework for unsupervised discovery of both the shift-invariant basis and the sparse coding of input data, with efficient algorithms for tractable computation. Empirical evaluations on multiple time series data sets demonstrate the effectiveness and efficiency of the proposed method.},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  keywords = {read - not in related work},
  annotation = {00006},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\3E6NBQN4\Zheng et al. - 2016 - Efficient Shift-Invariant Dictionary Learning.pdf}
}

@inproceedings{zheng_forecasting_2015,
  title = {Forecasting {{Fine-Grained Air Quality Based}} on {{Big Data}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '15},
  author = {Zheng, Yu and Yi, Xiuwen and Li, Ming and Li, Ruiyuan and Shan, Zhangqing and Chang, Eric and Li, Tianrui},
  year = {2015},
  pages = {2267--2276},
  publisher = {{ACM Press}},
  address = {{Sydney, NSW, Australia}},
  doi = {10.1145/2783258.2788573},
  urldate = {2019-04-15},
  abstract = {In this paper, we forecast the reading of an air quality monitoring station over the next 48 hours, using a data-driven method that considers current meteorological data, weather forecasts, and air quality data of the station and that of other stations within a few hundred kilometers. Our predictive model is comprised of four major components: 1) a linear regression-based temporal predictor to model the local factors of air quality, 2) a neural network-based spatial predictor to model global factors, 3) a dynamic aggregator combining the predictions of the spatial and temporal predictors according to meteorological data, and 4) an inflection predictor to capture sudden changes in air quality. We evaluate our model with data from 43 cities in China, surpassing the results of multiple baseline methods. We have deployed a system with the Chinese Ministry of Environmental Protection, providing 48-hour finegrained air quality forecasts for four major Chinese cities every hour. The forecast function is also enabled on Microsoft Bing Map and MS cloud platform Azure. Our technology is general and can be applied globally for other cities.},
  isbn = {978-1-4503-3664-2},
  langid = {english},
  keywords = {read - not in related work},
  annotation = {00126},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\2IGYHJMV\Zheng et al. - 2015 - Forecasting Fine-Grained Air Quality Based on Big .pdf}
}

@incollection{zhou_canonical_2009,
  title = {Canonical {{Time Warping}} for {{Alignment}} of {{Human Behavior}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 22},
  author = {Zhou, Feng and Torre, Fernando},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
  year = {2009},
  pages = {2286--2294},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-04-17},
  annotation = {00201},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\KGXMRH94\\Zhou and Torre - 2009 - Canonical Time Warping for Alignment of Human Beha.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\454L52AK\\3728-canonical-time-warping-for-alignment-of-human-behavior.html}
}

@inproceedings{zhou_deep_2017,
  title = {Deep {{Forest}}: {{Towards An Alternative}} to {{Deep Neural Networks}}},
  shorttitle = {Deep {{Forest}}},
  booktitle = {Proceedings of the {{Twenty-Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zhou, Zhi-Hua and Feng, Ji},
  year = {2017},
  month = aug,
  pages = {3553--3559},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Melbourne, Australia}},
  doi = {10.24963/ijcai.2017/497},
  urldate = {2019-09-04},
  abstract = {In this paper, we propose gcForest, a decision tree ensemble approach with performance highly competitive to deep neural networks in a broad range of tasks. In contrast to deep neural networks which require great effort in hyper-parameter tuning, gcForest is much easier to train; even when it is applied to different data across different domains in our experiments, excellent performance can be achieved by almost same settings of hyper-parameters. The training process of gcForest is efficient, and users can control training cost according to computational resource available. The efficiency may be further enhanced because gcForest is naturally apt to parallel implementation. Furthermore, in contrast to deep neural networks which require largescale training data, gcForest can work well even when there are only small-scale training data.},
  isbn = {978-0-9992411-0-3},
  langid = {english},
  annotation = {00008},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\UA3YA2MZ\Zhou and Feng - 2017 - Deep Forest Towards An Alternative to Deep Neural.pdf}
}

@article{zhu_deep_2017,
  title = {Deep and {{Confident Prediction}} for {{Time Series}} at {{Uber}}},
  author = {Zhu, Lingxue and Laptev, Nikolay},
  year = {2017},
  month = nov,
  journal = {2017 IEEE International Conference on Data Mining Workshops (ICDMW)},
  eprint = {1709.01907},
  pages = {103--110},
  doi = {10.1109/ICDMW.2017.19},
  urldate = {2019-03-19},
  abstract = {Reliable uncertainty estimation for time series prediction is critical in many fields, including physics, biology, and manufacturing. At Uber, probabilistic time series forecasting is used for robust prediction of number of trips during special events, driver incentive allocation, as well as real-time anomaly detection across millions of metrics. Classical time series models are often used in conjunction with a probabilistic formulation for uncertainty estimation. However, such models are hard to tune, scale, and add exogenous variables to. Motivated by the recent resurgence of Long Short Term Memory networks, we propose a novel end-to-end Bayesian deep model that provides time series prediction along with uncertainty estimation. We provide detailed experiments of the proposed solution on completed trips data, and successfully apply it to large-scale time series anomaly detection at Uber.},
  archiveprefix = {arxiv},
  keywords = {read - in related work,Statistics - Machine Learning},
  annotation = {00032},
  file = {C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\4XVB5FUQ\\Zhu and Laptev - 2017 - Deep and Confident Prediction for Time Series at U.pdf;C\:\\Users\\ospra\\OneDrive\\Phd\\References\\storage\\SKQ82DF7\\1709.html}
}

@article{zhu_hdiforest_2019,
  title = {{{HDI-Forest}}: {{Highest Density Interval Regression Forest}}},
  shorttitle = {{{HDI-Forest}}},
  author = {Zhu, Lin and Lu, Jiaxing and Chen, Yihong},
  year = {2019},
  month = may,
  journal = {arXiv:1905.10101 [cs, stat]},
  eprint = {1905.10101},
  primaryclass = {cs, stat},
  urldate = {2019-08-30},
  abstract = {By seeking the narrowest prediction intervals (PIs) that satisfy the specified coverage probability requirements, the recently proposed quality-based PI learning principle can extract high-quality PIs that better summarize the predictive certainty in regression tasks, and has been widely applied to solve many practical problems. Currently, the stateof-the-art quality-based PI estimation methods are based on deep neural networks or linear models. In this paper, we propose Highest Density Interval Regression Forest (HDI-Forest), a novel quality-based PI estimation method that is instead based on Random Forest. HDI-Forest does not require additional model training, and directly reuses the trees learned in a standard Random Forest model. By utilizing the special properties of Random Forest, HDIForest could efficiently and more directly optimize the PI quality metrics. Extensive experiments on benchmark datasets show that HDI-Forest significantly outperforms previous approaches, reducing the average PI width by over 20\% while achieving the same or better coverage probability.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\8GJQZQBK\Zhu et al. - 2019 - HDI-Forest Highest Density Interval Regression Fo.pdf}
}

@article{ziel_multivariate_2019,
  title = {Multivariate {{Forecasting Evaluation}}: {{On Sensitive}} and {{Strictly Proper Scoring Rules}}},
  shorttitle = {Multivariate {{Forecasting Evaluation}}},
  author = {Ziel, Florian and Berk, Kevin},
  year = {2019},
  month = oct,
  journal = {arXiv:1910.07325 [econ, stat]},
  eprint = {1910.07325},
  primaryclass = {econ, stat},
  urldate = {2021-02-09},
  abstract = {In recent years, probabilistic forecasting is an emerging topic, which is why there is a growing need of suitable methods for the evaluation of multivariate predictions. We analyze the sensitivity of the most common scoring rules, especially regarding quality of the forecasted dependency structures. Additionally, we propose scoring rules based on the copula, which uniquely describes the dependency structure for every probability distribution with continuous marginal distributions. Efficient estimation of the considered scoring rules and evaluation methods such as the DieboldMariano test are discussed. In detailed simulation studies, we compare the performance of the renowned scoring rules and the ones we propose. Besides extended synthetic studies based on recently published results we also consider a real data example. We find that the energy score, which is probably the most widely used multivariate scoring rule, performs comparably well in detecting forecast errors, also regarding dependencies. This contradicts other studies. The results also show that a proposed copula score provides very strong distinction between models with correct and incorrect dependency structure. We close with a comprehensive discussion on the proposed methodology.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {{62H05, 62H20, 62M10, 91B84},Economics - Econometrics,G.3,I.6.4,Statistics - Machine Learning,Statistics - Methodology,Statistics - Other Statistics},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\EKEGVQ5Y\Ziel and Berk - 2019 - Multivariate Forecasting Evaluation On Sensitive .pdf}
}

@incollection{zivot_vector_2006,
  title = {Vector {{Autoregressive Models}} for {{Multivariate Time Series}}},
  booktitle = {Modeling {{Financial Time Series}} with {{S-PLUS}}\textregistered},
  editor = {Zivot, Eric and Wang, Jiahui},
  year = {2006},
  pages = {385--429},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-32348-0_11},
  urldate = {2019-04-15},
  abstract = {The vector autoregression (VAR) model is one of the most successful, flexible, and easy to use models for the analysis of multivariate time series. It is a natural extension of the univariate autoregressive model to dynamic multivariate time series. The VAR model has proven to be especially useful for describing the dynamic behavior of economic and financial time series and for forecasting. It often provides superior forecasts to those from univariate time series models and elaborate theory-based simultaneous equations models. Forecasts from VAR models are quite flexible because they can be made conditional on the potential future paths of specified variables in the model.},
  isbn = {978-0-387-32348-0},
  langid = {english},
  keywords = {Forward Premium,Granger Causality,Impulse Response Function,Multivariate Time Series,read - in related work,Vector Autoregressive Model},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\799Q43P7\varModels.pdf}
}

@article{zou_regularization_2005,
  title = {Regularization and Variable Selection via the Elastic Net},
  author = {Zou, Hui and Hastie, Trevor},
  year = {2005},
  month = apr,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {67},
  number = {2},
  pages = {301--320},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2005.00503.x},
  urldate = {2019-04-15},
  abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
  langid = {english},
  keywords = {read - in related work},
  annotation = {00000},
  file = {C:\Users\ospra\OneDrive\Phd\References\storage\79ACZNXL\Zou and Hastie - 2005 - Regularization and variable selection via the elas.pdf}
}
